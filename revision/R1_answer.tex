\documentclass[11pt]{article}

\usepackage{amsmath,amsfonts,amssymb,amsthm,verbatim,multirow,hyperref,subfig,footnote,graphicx,array,xr,booktabs}
\usepackage{times}
\usepackage[textheight=600pt]{geometry}
\usepackage[usenames,dvipsnames]{color}
%%%%%%%%%%%%%%
%\usepackage[spanish]{babel}
%\usepackage[utf8]{inputenc}
%%%%%%%%%%%%%



% \oddsidemargin 0pt
% \topmargin 0pt
% \headheight 0pt
% \headsep 0pt
% \textwidth 15cm  
% \textheight  24cm
% \footskip 1cm

\newcommand{\lb}[1]{\color{ForestGreen}\textbf{[Luis B.: #1]}\normalcolor}
\newcommand{\bl}[1]{\color{red}\textbf{[Bo: #1]}\normalcolor}
\newcommand{\jeg}[1]{\color{blue}\textbf{Julien: #1]}\normalcolor}

\begin{document}
\begin{center}
  {\Large \textbf{Response to the comments of the Referee \#1.}}
\end{center}

We are grateful for the reviewer's careful commentary on the paper, which has improved
our presentation of the article. In what follows, we state the reviewer's
comments in italics, and describe our response in roman.

\begin{itemize}
\item \textit{The paper is interesting and important, but could use some more
focus. For example, it is unclear what the primary thesis of the manuscript is.
Is it the dimension reduction? INLA? or the forcings? Throughout the manuscript,
it seems like each of the above is the primary focus and then later the topic is
dropped. I think a revision that provides more focus and clarity about these
points is important.
}

  \lb{Once we finished the revision we can address this remark, because we have
    to adjust the writing and the length of the article to 25 pages.}
  \bl{I suggest to shorten the description of the five data reduction methods. I can do that if you agree. The sampling with INLA may also has room to be shortened. Also some papers in references have very long list of names that take up a lot space. For those papers, we might just simplify the authorship to leading author et al. }
  \jeg{I agree with the reviewer that the objective of the article is not as clear as it could be, and we can clarify that better from the outset.}
  
\item \textit{Describe the proxy data better. Is it annually resolved tree ring
    data, pollen data at irregular resolutions, etc? Has the data been
    standardized? Are the response of the proxies to climate linear? time
    uncertain?}

  The database gathers 692 records from 648 locations, including all continental regions and major ocean basins. The records are from trees, ice, marine and lake sediments, corals, speleothems, and documentary evidence. They range in length from 50 to 2000 years, with a median of 547 years, while temporal resolution ranges from biweekly to centennial.

  The vast majority of records are annually-resolved, with minimal dating uncertainty. Here, the data used have been mapped to a standard normal using the method of \cite{vanAlbada2007}. \jeg{I put this in the text, as other readers might be interested.}
  
\item \textit{On lines 44-45 of page 3, you say “to ensure reproducible
    workflows” I suggest you do the same and post the code as an appendix or on
    a online repository.}

  Thanks for the suggestion. We included the R code as an online repository.

  
\item \textit{By doing “screening” of the data, the inference is no longer truly
    “Bayesian”. I'm always skeptical of picking and choosing proxy data. Perhaps
    you are missing a signal by throwing out the baby with the bathwater.
    Perhaps you could assimilate the proxy data with each record having a
    different variance. E.g. for proxy $j$ at time $t$ with proxy data value $y_j(t)$ and climate variable $X(t)$, $y_j(t)=\beta_0+\beta_1X(t)+\epsilon_j(t)$
with $\epsilon_j(t)\sim N(0,\sigma_j^2)$. Then, important proxies will have a
smaller $\sigma_j^2$ and unimportant proxies will have their influence reduced due to a larger $\sigma_j^2$.
}

With the purpose of comparing the reduced proxies with and without the
screening procedure we computed the five reduced-proxy sets (one for each
method) for each case. The average coefficient of variation of $RP$ (with and
without screening), the
average correlation coefficient between the two dataset versions and the average
correlation coefficient between the observed temperature and each dataset
version are shown in Table \ref{tab:screening}.
\begin{table}
  \centering
  \begin{tabular}{l|rr|r|rr}
  \toprule
\textbf{Method} & \textbf{Sc. CV} & \textbf{Unsc. CV} & \textbf{Correlation: Sc. vs Unsc.} & \textbf{Correlation: Sc. vs T} & \textbf{Correlation: Unsc. vs T}\\ 
  \midrule
PCR & 0.6189 & 0.6741 & 0.9511 & 0.8534 & 0.8638 \\ 
  sPCR & 0.6065 & 0.6194 & 0.9843 & 0.8448 & 0.8431 \\ 
  LASSO & 0.5780 & 0.5814 & 0.9820 & 0.9186 & 0.9235 \\ 
  SPLS & 0.6060 & 0.6437 & 0.9454 & 0.9095 & 0.9098 \\ 
  SIR & 0.6384 & 0.8025 & 0.7591 & 0.9029 & 0.8600 \\ 
   \bottomrule
\end{tabular}
  \caption{Comparison of $RP$'s obtained with and without screening procedure. \textbf{Sc}: Screened dataset, \textbf{Unsc}: Unscreened dataset, \textbf{CV}: coefficient of
    variation, \textbf{T}: observed temperature (1900-2000) }
  \label{tab:screening}
\end{table}
The third column confirms that the signal is not missed when we
use the screening procedure because there is a large correlation between the two
procedures for all the reduction methods, except SIR. In general the coefficient
of variation increases when we use the unscreened dataset, and the change is
quite large for the SIR method, which is the main reason why the correlation is
the smallest. In addition, the association between the observed temperatures and
the proxies is very similar among procedures, but the SIR method can be improved
by using the screening. We think all the above confirms that the screening procedure can capture
the overall signal by means of a low-noise and low-dimensional alternative as we
do in the article. It is important to note that the screening procedure was
performed against the \textbf{local} (not global) temperature by means of the nearest grid points
within a 2000km radius.       


\lb{I'm trying an unscreened version of the proxy dataset in order to answer
  this question.}
\bl{Thanks for trying this. Let's see what the results would tell. Intuitively, I feel this might be worse, as some data that are screened out are really noisy so including them may just deterioriate the resonstruction.}
\jeg{Important to point out that the screening was never against the global mean, but against *local* temperature (nearest grid points within a 2000km radius).}

\item \textit{Why not just go fully Bayesian and reconstruct the proxy data that
    is unobserved given the observed proxies and the model? Why try to do the
    ``RegEM'' type of methods (nested reconstructions, etc.) and not take
    advantage of the full power of the Bayesian framework? The dimensions don't
    seem too large to me - especially because you aren't doing any Gaussian
    Process type estimation.
}

The intention of the article is the reconstruction of the average anomalies,
not the reconstruction of a particular biological proxy. This is the main reason
why we consider a hierarchical Bayesian model to represent the dynamics of the temperature
process. The proxy data represents an approximation of the observed temperature
series and the reconstructed temperatures are past estimates of the observed
process, they are not estimates of the proxy time-series. We think that the
usage of an state-space approach in this problem allows us to successfully
explain the uncertainties in the system by means of a Bayesian approach. 
\bl{We agree that fully Bayesian is another valid method to model the problem. But since our main focus is the reconstruction of the temperatures, we opt to use our model ....  }
\jeg{I think that the reviewer refers to is estimating the missing proxy values as missing parameters (conditional on other proxies), which would then avoid aving the slice reconstructions together. I have done that in the RegEM context, with very mixed success. What we want is the temperature given the proxies, and estimating missing proxies values appears to bring no benefit here.}


\item \textit{I have a hard time using Figure 4. Please rescale the correlations
    to only positive values and add numeric values to the correlations.}

  Thanks for the suggestion. We rescaled and labeled Figure 4 according to your comment.

  
\item \textit{The model validation and comparison is interesting. What is the
    point of using a proper scoring rule if you are using an estimated reconstruction? You are just calibrating to that reconstruction regardless of
whether the previous reconstruction is reasonable. For example, sPCR and
PCR-mixed predict well for MSE using the borehole data but seems to do poorly in
predicting the instrumental data - this seems concerning. In addition, are you
calibrating to a smoothed version of your reconstruction? Then the scoring rule
is likely not proper even when you are using CRPS. In addition, make sure you
state that the scoring rules are negatively oriented - lower scores are better.
A pseudo-proxy experiment can be useful in validating reconstructions. See Smerdon (2012) and Tipton et. al. (2016) for examples.
}

As we mentioned above the reconstruction seeks to estimate the observed mean
temperature during the past. That is the reason why we compare the reconstructed
series with the observed temperature during the validation period. The
calibration process only uses data from proxies and forcings, so we think that
the comparison of the reconstructed series vs the mean temperatures is not
restricting, on the contrary it guarantees more robustness. The purpose of the
proper scoring rules is to measure nominal coverage together with prediction
sharpness, and we think that an scoring rule can be applied without any
restriction in our problem. We do not calibrate to a smoothed version of the
reconstruction, instead we measure the MSE between the smoothed reconstruction
and the PS04 dataset in order to validate the reconstruction in periods beyond
the one with observed temperatures available. We stated in the article that the scoring rules
are negatively oriented as you suggested. We thank the reviewer for the last
comment, but due to the length restrictions of the
article, we think that a pseudo-proxy validation can be considered in future
work. 
\jeg{we do not have the *true* temperature. We have an observational estimate of the true temperature, which has errors too. Perhaps the reviewer's point is that we should incorporate errors in instrumental estimates? Certainly PS04 has errors that would need to be taken into account.}
\bl{I think we may first clarify that we did the scores for validation period where we have observed temperatures though not of high quality. We did compared our reconstruction to another reconstruction (PS04) in terms of MSE. Luis may justify the validity of using that reconsturction as reference. Then in the very end of the disucssion, we can add that pseudo-proxy validation will be considered in the future work. }

\item \textit{What is the computational time for the MCMC model? Was it coded in
    R, Python, or c++? Was the code profiled and optimized? This model seems
    like it should be easy to fit in Stan Nimble, or JAGS as there is nothing non-standard about the model. What about effective sample size per second? It seems
to me that the model is relatively simple - Is INLA really even needed for this
model? If you can do a full MCMC what is the need for INLA - especially when the
INLA approximation is not performing as well (e.g. large dips in climate state
in $\approx$ 1450, 1250, etc)? Is 8 hours really that much of a burden to offset
the improved
inference from MCMC? If this were a spatio-temporal problem with large data I
would agree about INLA, but the dataset and the model are actually small.
}

We agree that the reconstruction in this paper can be performed in full MCMC and any of the R packages you mentioned. Indeed we wrote our own full MCMC sampling code using R in Barboza et al. (2014) for the reconstruction. We also agree that INLA is an approximation method and has limitations, We therefore have a particular section to investigate the effect of INLA approximations. Despite that,  there are two main reasons why we introduce INLA here. One is that paleoclimate reconstructions involve many choices that cannot be made a priori (e.g. comparing different data reduction methods) and are time-consuming to explore. INLA allows us to conduct such experiments quickly, so we can implement many experiments with a light computational burden. The other reason is that we hope this work also plays the role of verifying that INLA can be used for climate reconstruction so later we can employ it in the space-time climate reconstruction, as you mentioned. We now have added a brief discussion about the limitation of INLA to the paper.


\item \textit{page 5 line 27 - what type of splines? In the abstract you say Bsplines - is this what you use here?}

Yes, we used BSplines. We clarified the sentence as you suggested.

\item \textit{page 11 lines 48-50 - You state the central hypothesis of the
    article is to test “no-forcing” to “forcing” models. Why are you just now
    introducing this here? If this is the central thesis it needs to be front
    and center.}

 \bl{In the second last paragraph of Introduction, we indeed seemed to only emphasize INLA. We need to give a brief summary of the contribution of this paper to both methodology and science. Comparing ``no-forcing" to ``forcing" can be mentioned there.}

\item \textit{There are many undefined acronyms in the abstract. (INLA, MCMC,
    HadCRUT4, etc.). Make sure you define all acronyms in place.}

Thanks for the suggestion, we made the corresponding corrections.


\end{itemize}

\begin{thebibliography}{}

\bibitem[1]{vanAlbada2007}
van Albada, S. and Robinson, P. (2007).
\newblock Transformation of arbitrary distributions to the normal distribution
  with application to \{EEG\} test--retest reliability.
\newblock {\em Journal of Neuroscience Methods}, 161(2):205 -- 211.

\end{thebibliography}
\end{document}


