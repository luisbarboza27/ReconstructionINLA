\documentclass[11pt]{article}

\usepackage{amsmath,amsfonts,amssymb,amsthm,verbatim,multirow,hyperref,subfig,footnote,graphicx,array,xr,booktabs}
\usepackage{times,natbib}
\usepackage[textheight=600pt]{geometry}
\usepackage[usenames,dvipsnames]{color}
%%%%%%%%%%%%%%
%\usepackage[spanish]{babel}
%\usepackage[utf8]{inputenc}
%%%%%%%%%%%%%



% \oddsidemargin 0pt
% \topmargin 0pt
% \headheight 0pt
% \headsep 0pt
% \textwidth 15cm  
% \textheight  24cm
% \footskip 1cm

\newcommand{\lb}[1]{\color{ForestGreen}\textbf{[Luis B.: #1]}\normalcolor}
\newcommand{\bl}[1]{\color{red}\textbf{[Bo: #1]}\normalcolor}
\newcommand{\jeg}[1]{\color{blue}\textbf{Julien: #1]}\normalcolor}

\begin{document}
\begin{center}
  {\Large \textbf{Response to the comments of the Referee \#1.}}
\end{center}

We are grateful for the reviewer's careful commentary on the paper, which has improved
our presentation of the article. In what follows, we state the reviewer's
comments in italics, and describe our response in roman.

\begin{itemize}
\item \textit{The paper is interesting and important, but could use some more
focus. For example, it is unclear what the primary thesis of the manuscript is.
Is it the dimension reduction? INLA? or the forcings? Throughout the manuscript,
it seems like each of the above is the primary focus and then later the topic is
dropped. I think a revision that provides more focus and clarity about these
points is important.
}

We agree that the goals could have been made more clear, particularly in the abstract. The latter was substantially revised to better lay them out. In addition, the penultimate paragraph of the introduction states:  ``In this article we extend the work of B14 by leveraging Integrated Nested Laplace Approximations (INLA). Doing so lightens the computational burden, which allows to (a) explore
 three new model designs inspired by the physics of the problem; (b) consider various choices for data reduction; and (c) take the non-stationary nature of the observational network into account. In addition, this work makes use of the latest estimates of radiative forcing, as well as a state-of-the-art, open-access compilation of community-curated paleoclimate observations (PAGES2k Consortium, 2017). This ensures that our calculations are using the best available data and are completely reproducible''.
 
 %We think this is a straightforward exposition of the goals of the paper, but welcome other suggestions if the reviewer think more is necessary. 

 % \lb{Once we finished the revision we can address this remark, because we have  to adjust the writing and the length of the article to 25 pages.}
 % \bl{I suggest to shorten the description of the five data reduction methods. I can do that if you agree. The sampling with INLA may also has room to be shortened. Also some papers in references have very long list of names that take up a lot space. For those papers, we might just simplify the authorship to leading author et al. }
 % \jeg{I agree with the reviewer that the objective of the article is not as clear as it could be, and we can clarify that better from the outset.}
  
\item \textit{Describe the proxy data better. Is it annually resolved tree ring
    data, pollen data at irregular resolutions, etc? Has the data been
    standardized? Are the response of the proxies to climate linear? time
    uncertain?}

  The database gathers 692 records from 648 locations, including all continental regions and major ocean basins. The records are from trees, ice, marine and lake sediments, corals, speleothems, and documentary evidence. They range in length from 50 to 2000 years, with a median of 547 years, while temporal resolution ranges from biweekly to centennial.

  The vast majority of records are annually-resolved, with minimal dating uncertainty. Here, the data used have been mapped to a standard normal using the method of \cite{vanAlbada2007}. This information has been added to the main text.
  
\item \textit{On lines 44-45 of page 3, you say “to ensure reproducible
    workflows” I suggest you do the same and post the code as an appendix or on
    a online repository.}

  Thanks for the suggestion. We included the R code as an online repository.

  
\item \textit{By doing “screening” of the data, the inference is no longer truly
    “Bayesian”. I'm always skeptical of picking and choosing proxy data. Perhaps
    you are missing a signal by throwing out the baby with the bathwater.
    Perhaps you could assimilate the proxy data with each record having a
    different variance. E.g. for proxy $j$ at time $t$ with proxy data value $y_j(t)$ and climate variable $X(t)$, $y_j(t)=\beta_0+\beta_1X(t)+\epsilon_j(t)$
with $\epsilon_j(t)\sim N(0,\sigma_j^2)$. Then, important proxies will have a
smaller $\sigma_j^2$ and unimportant proxies will have their influence reduced due to a larger $\sigma_j^2$.
}

With the purpose of comparing the reduced proxies with and without the
screening procedure we computed the five reduced-proxy sets (one for each
method) for each case. The average coefficient of variation of $RP$ (with and
without screening), the
average correlation coefficient between the two dataset versions and the average
correlation coefficient between the observed temperature and each dataset
version are shown in Table \ref{tab:screening}. All averages are taken with respect to the eight reduced proxies.
\begin{table}
  \centering
  \begin{tabular}{l|cc|c|cc}
  \toprule
 &  \multicolumn{2}{c|}{CV} & \multicolumn{3}{c}{Correlation}\\
 \hline
\textbf{Method} & \textbf{S.c} & \textbf{Unsc.} & \textbf{Sc. vs Unsc.} & \textbf{Sc. vs T} & \textbf{Unsc. vs T}\\ 
  \midrule
PCR & 0.6189 & 0.6741 & 0.9511 & 0.8534 & 0.8638 \\ 
  sPCR & 0.6065 & 0.6194 & 0.9843 & 0.8448 & 0.8431 \\ 
  LASSO & 0.5780 & 0.5814 & 0.9820 & 0.9186 & 0.9235 \\ 
  SPLS & 0.6060 & 0.6437 & 0.9454 & 0.9095 & 0.9098 \\ 
  SIR & 0.6384 & 0.8025 & 0.7591 & 0.9029 & 0.8600 \\ 
   \bottomrule
\end{tabular}
  \caption{Comparison of $RP$'s obtained with and without screening procedure. \textbf{Sc}: Screened dataset; \textbf{Unsc}: Unscreened dataset; \textbf{CV}: coefficient of variation; \textbf{T}: observed temperature (1900-2000). }
  \label{tab:screening}
\end{table}
The third column in Table \ref{tab:screening} confirms that the signal is not missed when we
use the screening procedure because there is a large correlation between the two
procedures for all the reduction methods, except SIR. In general the coefficient
of variation increases when we use the unscreened dataset, and the change is
quite large for the SIR method, which is the main reason why the correlation is
the smallest. In addition, the association between the observed temperatures and
either the screened or unscreened proxies is very similar, with SIR method can even be improved
by using the screening. We think all the above confirms that the screening procedure can capture
the overall signal by means of a low-noise and low-dimensional alternative as we
do in the article. It is important to note that the screening procedure was
performed against the \textbf{local} (not global) temperature by means of the nearest grid points
within a 2000km radius.       


%\lb{I'm trying an unscreened version of the proxy dataset in order to answer
%  this question.}
%\bl{Thanks for trying this. Let's see what the results would tell. Intuitively, I feel this might be worse, as some data that are screened out are really noisy so including them may just deterioriate the resonstruction.}
%\jeg{Important to point out that the screening was never against the global mean, but against *local* temperature (nearest grid points within a 2000km radius).}

\item \textit{Why not just go fully Bayesian and reconstruct the proxy data that
    is unobserved given the observed proxies and the model? Why try to do the
    ``RegEM'' type of methods (nested reconstructions, etc.) and not take
    advantage of the full power of the Bayesian framework? The dimensions don't
    seem too large to me - especially because you aren't doing any Gaussian
    Process type estimation.
}

%The intention of the article is the reconstruction of the average anomalies, not the reconstruction of a particular biological proxy. This is the main reason why we consider a hierarchical Bayesian model to represent the dynamics of the temperature process. The proxy data represents an approximation of the observed temperature series and the reconstructed temperatures are past estimates of the observed process, they are not estimates of the proxy time-series. We think that the usage of an state-space approach in this problem allows us to successfully explain the uncertainties in the system by means of a Bayesian approach. 

There appears to be some confusion here; none of what we are doing is akin to RegEM. However, in the past one of us (Emile-Geay) has attempted to infer missing values of the proxies from the observed proxies using RegEM, and obtained only noise. Since we are inferring temperature, not proxies, the value of this approach seems dubious.


%We agree that fully Bayesian including the estimation of missing proxies is another valid method for our problem. One of our co-authors, Dr. Julien Emile-Geay, tried this method in the RegEM context, and they found that estimating missing proxies added no value to the temperature reconstruction. Since our main focus is the reconstruction of temperatures, we opt to use our current model choice. \bl{Julien, can you give the reference of you RegEM work?}

%\jeg{I think that the reviewer refers to is estimating the missing proxy values as missing parameters (conditional on other proxies), which would then avoid aving the slice reconstructions together. I have done that in the RegEM context, with very mixed success. What we want is the temperature given the proxies, and estimating missing proxies values appears to bring no benefit here.}


\item \textit{I have a hard time using Figure 4. Please rescale the correlations
    to only positive values and add numeric values to the correlations.}

  Thanks for the suggestion. We rescaled and labeled Figure 4 according to your comment.

  
\item \textit{The model validation and comparison is interesting. What is the
    point of using a proper scoring rule if you are using an estimated reconstruction? You are just calibrating to that reconstruction regardless of
whether the previous reconstruction is reasonable. For example, sPCR and
PCR-mixed predict well for MSE using the borehole data but seems to do poorly in
predicting the instrumental data - this seems concerning. In addition, are you
calibrating to a smoothed version of your reconstruction? Then the scoring rule
is likely not proper even when you are using CRPS. In addition, make sure you
state that the scoring rules are negatively oriented - lower scores are better.
A pseudo-proxy experiment can be useful in validating reconstructions. See Smerdon (2012) and Tipton et. al. (2016) for examples.
}

There are really four points in this comment, which we address in turn:
\begin{enumerate}
\item Why use another reconstruction (PS04) to evaluate our own? 
\item Are the scores proper in this case? 
\item State that scores are negatively oriented
\item Consider pseudoproxy experiments to evaluate the reconstruction. 
\end{enumerate}
\begin{enumerate}
    \item  We acknowledge that the dual validation (instrumental and borehole temperatures was confusing. The revised text of section 4.1 should now make this clear : {\it Validation to the early instrumental record, important though it is, says little about a reconstruction's behavior on centennial scales, which is of primary interest to climate scientists. To constrain this behavior, we leverage the estimates of \citet[hereafter, PS04]{Pollack2004} whose borehole-based
temperature inversions estimate surface temperature trends over 1600-2000. Because of the diffusive transfer of heat in Earth's crust, this dataset only speaks to centennial trends; we thus compare it to smoothed versions of our reconstructions, obtained through a Butterworth low-pass filter with a 100-year cutoff and order equal to 4.} 
\item  Sorry for the confusion. We did not calibrate our reconstruction to a smoothed temperature process (PS04), neither did we calibrate a smoothed version of our reconstruction to PS04. The calibration of our reconstruction was against the annual observations of temperature process. Then later, we compared the smoothed version of our reconstruction to another smoothed reconstruction (PS04) and calculated the MSE, but this comparison was after the calibration process. Therefore, we do not think there is risk that the scores would not be proper in our implementation.% The
% calibration process only uses data from proxies and forcings, so we think that
% the comparison of the reconstructed series vs the mean temperatures is not
% restricting, on the contrary it guarantees more robustness. The purpose of the
% proper scoring rules is to measure nominal coverage together with prediction
% sharpness, and we think that an scoring rule can be applied without any
% restriction in our problem. We do not calibrate to a smoothed version of the
% reconstruction, instead we use the observations to calibrate the entire model by
% means of the reduced proxies. In addition, we measure the MSE between the smoothed reconstruction
% and the PS04 dataset in order to validate the reconstruction in periods beyond
% the one with observed temperatures available.
% \bl{After I read one more time of the reviewer comment. Looks like the concern of proper or not proper just comes from his or her conjecture that we calibrate against smoothed temperature. So we could just reply like ``Sorry for the confusion. We did not calibrate our reconstruction to a smoothed temperature process (PS04), neither did we calibrate a smoothed version of our reconstruction to PS04. The calibration of our reconstruction was against the annual observations of temperature process. Then later, we compared the smoothed version of our reconstruction to another smoothed reconstruction (PS04) and calculated the MSE, but this comparison is after the calibration process. Therefore, we do not think there is risk that the scores would not be proper in our implementation. " Agree?}

\item The caption of Table 2 now mentions the scoring rules
are negatively oriented, as per your suggestion. In addition, it better explains the difference between validation to HadCRUT4 and PS04. 

\item Pseudoproxy experiments are indeed useful, but due to the length restrictions of the article, such explorations will be considered in future work. One issue is that the usual pseudoproxy designs (temperature + Gaussian noise) are over-simplistic, and more realistic designs are necessary to make these tests more relevant to real-world reconstructions \citep{Evans_grl2014}. Properly setting these up for such a large and diverse proxy dataset would be its own study, which is currently in the works. 

\end{enumerate}



 
%\jeg{we do not have the *true* temperature. We have an observational estimate of the true temperature, which has errors too. Perhaps the reviewer's point is that we should incorporate errors in instrumental estimates? Certainly PS04 has errors that would need to be taken into account.}

%\bl{I do not think we are answering all the questions. In my understanding, we can split the reviewer's comments into four questions:}

%\bl{Suggest to add ``Although PS04 are not the actual observations, this is the best approach to validating our results in a centennial scale without resorting to pseudoproxy experiments (Smerdon, 2012), to our knowledge" }


\item \textit{What is the computational time for the MCMC model? Was it coded in
    R, Python, or c++? Was the code profiled and optimized? This model seems
    like it should be easy to fit in Stan Nimble, or JAGS as there is nothing non-standard about the model. What about effective sample size per second? It seems
to me that the model is relatively simple - Is INLA really even needed for this
model? If you can do a full MCMC what is the need for INLA - especially when the
INLA approximation is not performing as well (e.g. large dips in climate state
in $\approx$ 1450, 1250, etc)? Is 8 hours really that much of a burden to offset
the improved
inference from MCMC? If this were a spatio-temporal problem with large data I
would agree about INLA, but the dataset and the model are actually small.
}

We agree that the reconstruction in this paper can be performed in full MCMC and any of the R packages you mentioned. Indeed we wrote our own full MCMC sampling code using R in Barboza et al. (2014) for the reconstruction. We also agree that INLA is an approximation method and has limitations, We therefore have a particular section to investigate the effect of INLA approximations. Despite that,  there are two main reasons why we introduce INLA here. One is that paleoclimate reconstructions involve many choices that cannot be made a priori (e.g. comparing different data reduction methods) and are time-consuming to explore. INLA allows us to conduct such experiments quickly, so we can implement many experiments with a light computational burden. The other reason is that we hope this work also plays the role of verifying that INLA can be used for climate reconstruction so later we can employ it in the space-time climate reconstruction, as you mentioned. We now have added a brief discussion about the limitation of INLA to the paper.


\item \textit{page 5 line 27 - what type of splines? In the abstract you say Bsplines - is this what you use here?}

Yes, we used BSplines. We clarified the sentence as you suggested.

\item \textit{page 11 lines 48-50 - You state the central hypothesis of the
    article is to test “no-forcing” to “forcing” models. Why are you just now
    introducing this here? If this is the central thesis it needs to be front
    and center.}

We are sorry for the confusion. Our paper was trying to cover many aspects in both methodological and scientific investigations, but as you pointed out this made the emphases of the paper unclear. In the revision we now have focused more on the methodological component. The sentence you mentioned in the comment has been removed.  

 %\bl{In the second last paragraph of Introduction, we indeed seemed to only emphasize INLA. We need to give a brief summary of the contribution of this paper to both methodology and science. Comparing ``no-forcing" to ``forcing" can be mentioned there.}

\item \textit{There are many undefined acronyms in the abstract. (INLA, MCMC,
    HadCRUT4, etc.). Make sure you define all acronyms in place.}

Thank you for the suggestion. We have made the corresponding corrections.


\end{itemize}

\bibliographystyle{apalike}
\bibliography{biblioteca}


\end{document}


