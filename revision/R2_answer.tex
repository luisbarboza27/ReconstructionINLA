\documentclass[11pt]{article}

\usepackage{amsmath,amsfonts,amssymb,amsthm,verbatim,multirow,hyperref,subfig,footnote,graphicx,array,xr,booktabs}
\usepackage{times}
\usepackage[textheight=600pt]{geometry}
\usepackage[usenames,dvipsnames]{color}
%%%%%%%%%%%%%%
%\usepackage[spanish]{babel}
%\usepackage[utf8]{inputenc}
%%%%%%%%%%%%%



% \oddsidemargin 0pt
% \topmargin 0pt
% \headheight 0pt
% \headsep 0pt
% \textwidth 15cm  
% \textheight  24cm
% \footskip 1cm

\newcommand{\lb}[1]{\color{ForestGreen}\textbf{[Luis B.: #1]}\normalcolor}
\newcommand{\bl}[1]{\color{red}\textbf{[Bo: #1]}\normalcolor}
\newcommand{\jeg}[1]{\color{blue}\textbf{Julien: #1]}\normalcolor}

\begin{document}
\begin{center}
  {\Large \textbf{Response to the comments of the Referee \#2.}}
\end{center}

We are grateful for the reviewer's careful commentary on the paper, which has improved
our presentation of the article. In what follows, we state the reviewer's
comments in italics, and describe our response in roman.

\begin{itemize}
\item \textit{The above is particularly important when it comes to the proxy
  dimension reduction; in a properly specified hierarchical model,
  such as in the paper, the main loss from reducing the proxies is an
  increased prediction uncertainty for the temperature and model
  parameters.  For a "reverse" specified model, I would expect
  hard-to-avoid bias and incorrect conclusions about appropriate
  number of proxies etc, if those are treated as covariates to be
  "selected". Having them as model responses removes that part of the
  tendency to treat it as a model selection problem, when the
  dimension reduction is really done for basic computational cost
  reasons; The model should (ideally) be defined so that the original
  proxies could be used, given enough computational power.  It should
  be made clearer if the computational cost reduction is the only
  reason for doing this, or if it also might reduce individual proxy
  biases by averaging some of them out.  Reducing the proxies by
  averaging (or otherwise) within groups removes information relevant
  for uncertainty quantification, so it should be made clear what such
  information might be lost in the dimension reduction step. The above also relates to the discussion on page 10; using the
  proxies as "input"/"covariates" would definitely run the risk of
  ignoring them. The generative hierarchical construction used in the
  paper makes them much less ignorable, as they are the means through
  which everything else is estimated.}

Thank you for your insightful comments. Computational cost is absolutely one consideration for us to perform proxy reduction, but on the other hand, we also conjecture that using reduced proxies will benefit the reconstruction. First, if we directly use original proxies in the reconstruction, there will be a huge number of parameters to estimate. Some proxies alone may not be very  informative to estimate the forward model that relates proxies to temperatures. Therefore, the model fitting with original proxies can be challenging. Second, we conjecture that the reduced proxies still retain global climate information but only discard the inherent noise that is related to local temperatures, so the reconstruction with reduced proxies will carry less uncertainty compared to that with original proxies. This is consistent with your intuition about changing uncertainty quantification, but we feel the information lost in the proxy reduction may not be very useful for recovering the global temperature. We did not find an easy way to verify this, but it may be analogous to the investigation whether the screening procedure will result into information loss.    


%We thank the reviewer for these insightful comments. Of course one major benefit of using reduced proxies is that the computation is much eased, but actually the primary reason for considering reduction is that it can also benefit the reconstruction. We agree that the uncertainty of the reconstruction will be changed most likely toward the direction of shrinkage when we used reduced proxies. However, since we focus on the global temperature rather than local temperatures, reducing proxies will not deteriorate the reconstruction but only make the reconstruction less uncertain by removing the part of noise in proxies that is related to local temperatures. 
Table \ref{tab:screening2} demonstrates that screened proxies still retain the global climate information although it misses some local temperature information.
\begin{table}[htp]
  \centering
  \begin{tabular}{l|cc|c|cc}
  \toprule
 &  \multicolumn{2}{c|}{CV} & \multicolumn{3}{c}{Correlation}\\
 \hline
\textbf{Method} & \textbf{S.c} & \textbf{Unsc.} & \textbf{Sc. vs Unsc.} & \textbf{Sc. vs T} & \textbf{Unsc. vs T}\\ 
  \midrule
PCR & 0.6189 & 0.6741 & 0.9511 & 0.8534 & 0.8638 \\ 
  sPCR & 0.6065 & 0.6194 & 0.9843 & 0.8448 & 0.8431 \\ 
  LASSO & 0.5780 & 0.5814 & 0.9820 & 0.9186 & 0.9235 \\ 
  SPLS & 0.6060 & 0.6437 & 0.9454 & 0.9095 & 0.9098 \\ 
  SIR & 0.6384 & 0.8025 & 0.7591 & 0.9029 & 0.8600 \\ 
   \bottomrule
\end{tabular}
  \caption{Comparison of $RP$'s obtained with and without screening procedure. \textbf{Sc}: Screened data set; \textbf{Unsc}: Unscreened data set; \textbf{CV}: coefficient of variation; \textbf{T}: observed temperature (1900-2000). The columns of CV are the CV of reduced proxies, and the columns of Correlation are the correlations between reduced proxies and instrumental temperatures.}
  \label{tab:screening2}
\end{table}

The third column confirms that the signal is not missed when we
use the screening procedure because there is a large correlation between the two
procedures for all the reduction methods, except SIR. In general the coefficient
of variation increases when we use the unscreened dataset, and the change is
quite large for the SIR method, which is the main reason why the correlation is
the smallest. In addition, the association between the observed temperatures and
the proxies is very similar among procedures, and the SIR method can even be improved
by using the screening. We think all the above confirms that the screening procedure can capture
the overall signal by means of a low-noise and low-dimensional alternative as we
do in the article. It is important to note that the screening procedure was
performed against the \textbf{local} (not global) temperature by means of the nearest grid points
within a 2000km radius. We now have added in Section 3.1: ``Reducing proxies not only eases the computation, but
also makes the reconstruction less uncertain by removing the part of noise in
proxies that is related to local temperatures.'' 

% \bl{We now have added
% ``We reduce proxies to ease the computational cost and meantime make the reconstruction less uncertain compared to using original proxies." }



\item \textit{Are the reduced proxy model residuals temporally independent? (see
  comment above) May need to add an AR(1) random effect to the model to capture
  dependence (but be careful about confounding with the independent
  noise term that forms the response likelihood; ideally, one would
  have a joint BYM model that parametrises the joint effect).}

To answer this question, we first examined the correlation structure of residuals based on the linear regression of
each of the 8 reduced proxies vs the observed temperatures. We only had sufficient
evidence to reject the hypothesis of temporal independence in 5 out of 40
possible residuals (5 reduced-proxy methods $\times$ 8 reduced proxies). Therefore, the assumption of temporal independence is not unreasonable. Despite these results, we implemented an AR(1) random effect to the model as you
suggested. The results are shown in Table \ref{tab:comparisontot}. In general, 
all measures using an AR(1) error term are slightly smaller than the measures using independent errors, but the best
models chosen based on measures using AR(1) error remain the same, so we do not think independent errors are a bad choice. Thank you for pointing this out. Due to the page limit, we cannot include the results with AR(1) error in the paper, but we have added a comment ``If we allow for a more flexible AR(1) structure for the error terms, the conclusions remain the same.''


\begin{table}[htp]
  \centering
  {\small
\begin{tabular}{lll|rrr|r}
  \toprule
\textbf{Model} & N & \textbf{Method} & $IS_{80}$ & $IS_{95}$ & CRPS & MSE \\ 
  \midrule
WF & 1 & PCR & 0.4595 & 0.1759 & 0.1536 & 0.1475 \\ 
WF & 1 & sPCR & 0.5105 & 0.1952 & 0.1850 & 0.1885 \\ 
WF & 1 & LASSO & 0.4572 & 0.1749 & 0.1569 & 0.1738 \\ 
WF & 1 & SPLS & 0.4978 & 0.1904 & 0.1873 & 0.1689 \\ 
  WF & 1 & SIR & 0.4650 & 0.1779 & 0.1566 & 0.1689 \\
  \midrule
WF & 8 & PCR & 0.2300 & 0.0794 & 0.0893 & 0.0457 \\ 
WF & 8 & sPCR & 0.1848 & 0.0673 & 0.0743 & 0.1028 \\ 
WF & 8 & LASSO & 0.5826 & 0.4019 & 0.1732 & 0.1923 \\ 
WF & 8 & SPLS & 0.3096 & 0.1369 & 0.1092 & 0.1484 \\ 
  WF & 8 & SIR & 0.6647 & 0.4988 & 0.1904 & 0.2156 \\
  \midrule
NF & 8 & PCR & 0.3705 & 0.1157 & 0.1698 & 0.0249 \\ 
NF & 8 & sPCR & 0.3641 & 0.1368 & 0.1350 & 0.0250 \\ 
NF & 8 & LASSO & 0.6083 & 0.4329 & 0.1788 & 0.1983 \\ 
NF & 8 & SPLS & 0.2844 & 0.1129 & 0.1025 & 0.1398 \\ 
  NF & 8 & SIR & 0.3753 & 0.1856 & 0.1265 & 0.1477 \\
  \midrule
Mixed & 8 & PCR & 0.4774 & 0.1439 & 0.2255 & 0.0444 \\ 
Mixed & 8 & sPCR & 0.3366 & 0.1260 & 0.1226 & 0.0284 \\ 
Mixed & 8 & LASSO & 0.5988 & 0.4214 & 0.1769 & 0.1959 \\ 
Mixed & 8 & SPLS & 0.2830 & 0.1118 & 0.1022 & 0.1395 \\ 
Mixed & 8 & SIR & 0.3735 & 0.1837 & 0.1261 & 0.1468 \\ 
   \bottomrule
\end{tabular}
}
\caption{Comparison of predictive measures with an AR(1) error term.}
\label{tab:comparisontot}
\end{table}


\item \textit{What is the prior used for the weights on the B-spline basis
  constructions? For independent weights, the results can be highly
  dependent on the location and spacing of the knots. A nearly always
  better alternative is to use a properly penalised spline with a well
  defined interpretation in the limit as the number of knots approach
  infinity, such as a continuous domain 2nd order random walk model
  (when penalisisng the integral of the square of the 2nd order
  derivatives, which gives a 2nd order intrinsic stationary prior on
  the weights; in INLA either a "rw2" model can be used for the
  continuous limit, or a 1D SPDE model on 2nd order B-spline bases).}

We agree that the results are possibly dependent on the location of the knots. 
In our work, we carefully chose the knots location based on the degree of linear association between HadCRUT4 temperature and the BSplines. 
We also agree that the knots spacing is another typical and challenging issue with splines. Due to the
nature of the temperature series and its diverse spatial composition, we feel it is a bit stretchy to assume a continuous time model for the temperature process.  Hence, we prefer our simple and conservative assumption of uniformity in the distribution of nodes. However, the suggestion of random walk would be very interesting to investigate in future research. We now have added ``the location and spacing of knots can possibly affect the results.  A compelling alternative to our choice is to use a properly penalized spline with a well
defined interpretation in the limit as the number of knots approach infinity. We leave this issue for future investigation. ''



\item \textit{Page 12: The method is referred to as "Sampling with INLA", but it's
  not clear if only the direct posterior density computations were
  used, or if also the independent Monte Carlo posterior sample method
  available in R-INLA was also used (even apart from the sampling
  error inherent in estimates based on Monte Carlo samples, this
  sampling method does not exactly match the computed marginal
  posteriors).  Since the model is linear, all the results should be
  accessible through the direct methods, but then it would be sensible
  to refer to the method differently, e.g. "Computing posteriors with
  INLA". If posterior samples are used, it should be made clear in the
  text.} 

We are sorry for confusion. We used the direct posterior density computations
for most of the calculations throughout the article, except when we compute the
CRPS measure where we used Monte Carlo samples to construct the independent
samples within its definition. We have changed the title and made clarifications in the article as you suggested.

\item \textit{"Model (4) leads to conditional independence of $y$ given $\theta$ and
  $\psi$": Conditional independence between the response variables (the
  reduced proxy values) is a simplifying restriction of the R-INLA
  implementation, separate from the the prior independence between
  different structured random effects components. While conditional
  independence between proxies seems reasonable (note \textbf{conditional}
  independence, as one should expect strong \textbf{marginal} dependence, as
  noted also on page 9), I have a harder time accepting conditional
  independence within each proxy; are the model residuals really
  statistically independent over time?}

Please see our answer about using AR(1) error model above (second bullet).

\item \textit{What $f(.)$ models were used for the different inputs? (see comment
  about rw2 etc, above) Linear? rw2? Other?}

We used an i.i.d. random effect for the latent variable $T_t$ and we made as
many copies as necessary of $T_t$ in order to define the latent variable
multiplied by a random variable in each proxy equation. In addition, we added an
independent random effect to model the error term in the latent equation and we
also tried the AR(1) error term for comparison purposes.  

\item \textit{Page 13: "(ii) the vector $\theta$ is conditionally independent given
  the hyperparameters"; this statement is unclear. The R-INLA model
  components are conditionally independent, but any structured effect
  will have conditional dependence within such a components, so it's
  not correct to say that the entire $\theta$ vector is conditionally
  independent (unless it's meant to refer only to a specific subset of
  models, but the preceeding text seems to descrive the general INLA
  model setting). The next sentence, "These two assumptions specify $\theta$ as a
  Gaussian Markov random field" is also not quite right; \textbf{all}
  multivariate normal distributions are markov random fields on \textbf{some}
  graph. The key is that for models such as "iid" and "AR(1)", "rw2",
  etc, that graph is very very sparse, so that the Markov property on
  that graph becomes useful for computations.} 

Thanks for catching this mistake. We now have deleted both sentences. 


\item \textit{Page 13, last line: delete "their" in "their approximations"; there
  might be other approximations!}
  
Thanks, we changed the sentence as you suggested.

\item \textit{It's not clear what the differences between the models estimated
  with MCMC and INLA are in Figure 8. The text implies that the model
  estimated with MCMC uses the same priors (and model structure) as in
  Barboza et al (2014). Is the systematic difference in reconstruction
  mean only due to the difference in priors for the model component
  variances?  If the two models are the same, some more effort should
  be put on determining why the posterior means are so different. If
  the two models are different (which includes all the parameter prior
  distributions), could another INLA run be done where exactly the
  same priors as for the MCMC run is used, so that it's possible to
  see if the two methods compute/estimate the same posteriors?}

We first would like to make it clear that  (1) we used priors with larger
variance hyperparameter (less precise) for INLA and (2) we used
the same model for both approaches.  But we conjecture that the difference is not caused by the prior choice. Instead, we think the level of mixing in the MCMC chains of the external-forcing parameters may be the main cause of the difference. We had experience in Barboza et al. (2014) that an acceptable level of mixing of such chains was difficult to attain. The current
dimension of latent variables is much larger compared to that in Barboza et al,
2014, so this may exacerbate the mixing issue in the MCMC procedure and lead to
the difference. 
%\bl{can you not solve it by thinning the posterior?}\lb{Thinning is a possible solution, but the MCMC's computational time makes thinning challeging.}


\item \textit{The computational cost benefit of INLA seems very clear; there is
  only a small number of variance parameters in the model, and the
  lack of explicit temporal dependence (other than that implied by the
  forcings) means that MCMC is overkill for this problem, and inposes
  a large Monte Carlo error; Even with explicit random temporal
  dependence in the model, the likelihood calculations would be order
  N in cost, and would only add one extra covariance parameter, so
  INLA should be expected to be more efficient.  Also, since the
  entire model is Gaussian, the Laplace steps are exact, so only the
  mode seeking and numerical integration steps are approximations.
  Thus, the absolute Monte Carlo error from MCMC should be expected to
  be large unless a very long run is made, and the relative bias in
  the posterior densities from INLA should be expected to be small.}

You are correct that the computation has been largely reduced by using INLA. We have tried the AR(1) error model and the computation is still very manageable as you expected.  We also noticed that very long runs of MCMC are required to attain satisfactory results, and we conjecture this is the reason why the posterior density of forcing parameters from INLA is different than that from MCMC.


\item \textit{I'm curious about the bimodal posterior density for the Greenhouse
  effect in Figure 9, as I wouldn't expect that from a model of this
  simple type.  How was the figure produced? If done via posterior
  sampling, how many samples? The number of samples needs to be very
  large to get stable empirical density estimates.}

Thank you for pointing this out. The figure was produced by plotting the posterior marginal of the fixed effect
related to the Greenhouse-gas external forcing. We used the INLA's attribute
\verb|marginals.fixed|, which does not involve sampling for making 
this figure. The bimodal density indeed looks odd. We carefully looked into this issue, and we found that this bimodal pattern was mainly due to the resolution of default values in x-axis when applying INLA to compute the posterior density. After adjusting the values in x-axis at which the density was calculated, the distribution plot now looks quite reasonable. 


\item \textit{I really like that several assessment criteria are used; it's clear
  from the table that relying on only one would run the risk of
  overinterpreting the method differences.}

Thank you for your comment. We concur.

\end{itemize}
\end{document}


