@article{Bair2006,
abstract = {In regression problems where the number of predictors greatly exceeds the number of observations, conventional regression techniques may produce unsatisfactory results. We describe a technique called supervised principal components that can be applied to this type of problem. Supervised principal components is similar to conventional principal components analysis except that it uses a subset of the predictors selected based on their association with the outcome. Supervised principal components can be applied to regression and generalized regression problems, such as survival analysis. It compares favorably to other techniques for this type of problem, and can also account for the effects of other covariates and help identify which predictor variables are most important. We also provide asymptotic consistency results to help support our empirical findings. These methods could become important tools for DNA microarray data, where they may be used to more accurately diagnose and treat cancer.},
author = {Bair, Eric and Hastie, Trevor and Paul, Debashis and Tibshirani, Robert},
doi = {10.1198/016214505000000628},
file = {:home/luis/Dropbox/Biblioteca/articulos/Bair et al. - 2006 - Prediction by Supervised Principal Components(2).pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {gene expression,microarray,regression,survival analysis},
number = {473},
pages = {119--137},
title = {{Prediction by Supervised Principal Components}},
volume = {101},
year = {2006}
}
@article{Bair2006a,
abstract = {In regression problems where the number of predictors greatly exceeds the number of observations, conventional regression techniques may produce unsatisfactory results. We describe a technique called supervised principal components that can be applied to this type of problem. Supervised principal components is similar to conventional principal components analysis except that it uses a subset of the predictors selected based on their association with the outcome. Supervised principal components can be applied to regression and generalized regression problems, such as survival analysis. It compares favorably to other techniques for this type of problem, and can also account for the effects of other covariates and help identify which predictor variables are most important. We also provide asymptotic consistency results to help support our empirical findings. These methods could become important tools for DNA microarray data, where they may be used to more accurately diagnose and treat cancer.},
author = {Bair, Eric and Hastie, Trevor and Paul, Debashis and Tibshirani, Robert},
doi = {10.1198/016214505000000628},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Gene expression,Microarray,Regression,Survival analysis},
number = {473},
pages = {119--137},
title = {{Prediction by supervised principal components}},
volume = {101},
year = {2006}
}
@article{Barboza2014,
author = {Barboza, Luis and Li, Bo and Tingley, Martin P. and Viens, Frederi G.},
doi = {???????},
file = {:home/luis/Dropbox/Biblioteca/articulos/Barboza et al. - 2014 - Reconstructing Past Temperatures from Natural Proxies and Estimated Climate Forcings using Short- and Long-Memor.pdf:pdf},
journal = {The Annals of Applied Statistics},
number = {4},
pages = {1966--2001},
title = {{Reconstructing Past Temperatures from Natural Proxies and Estimated Climate Forcings using Short- and Long-Memory Models}},
volume = {8},
year = {2014}
}
@article{Blangiardo2013,
abstract = {During the last three decades, Bayesian methods have developed greatly in the field of epidemiology. Their main challenge focusses around computation, but the advent of Markov Chain Monte Carlo methods (MCMC) and in particular of the WinBUGS software has opened the doors of Bayesian modelling to the wide research community. However model complexity and database dimension still remain a constraint.Recently the use of Gaussian random fields has become increasingly popular in epidemiology as very often epidemiological data are characterised by a spatial and/or temporal structure which needs to be taken into account in the inferential process. The Integrated Nested Laplace Approximation (INLA) approach has been developed as a computationally efficient alternative to MCMC and the availability of an R package (R-INLA) allows researchers to easily apply this method.In this paper we review the INLA approach and present some applications on spatial and spatio-temporal data. ?? 2012 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Blangiardo, Marta and Cameletti, Michela and Baio, Gianluca and Rue, H{\aa}vard},
doi = {10.1016/j.sste.2013.07.003},
eprint = {arXiv:1011.1669v3},
isbn = {1877-5845},
issn = {18775845},
journal = {Spatial and Spatio-temporal Epidemiology},
keywords = {Area-level data,Bayesian approach,Integrated Nested Laplace Approximation,Point-level data,Stochastic Partial Differential Equation approach,integrated nested laplace approximation,stochastic partial differential equation},
pages = {39--55},
pmid = {24377114},
publisher = {Elsevier Ltd},
title = {{Spatial and spatio-temporal models with R-INLA}},
url = {http://dx.doi.org/10.1016/j.sste.2013.07.003},
volume = {7},
year = {2013}
}
@article{Chun2010,
abstract = {Partial least squares regression has been an alternative to ordinary least squares for handling multicollinearity in several areas of scientific research since the 1960s. It has recently gained much attention in the analysis of high dimensional genomic data. We show that known asymptotic consistency of the partial least squares estimator for a univariate response does not hold with the very large p and small n paradigm. We derive a similar result for a multivariate response regression with partial least squares. We then propose a sparse partial least squares formulation which aims simultaneously to achieve good predictive performance and variable selection by producing sparse linear combinations of the original predictors. We provide an efficient implementation of sparse partial least squares regression and compare it with well-known variable selection and dimension reduction approaches via simulation experiments. We illustrate the practical utility of sparse partial least squares regression in a joint analysis of gene expression and genomewide binding data.},
author = {Chun, Hyonho and Keleş, S{\"{u}}nd{\"{u}}z},
doi = {10.1111/j.1467-9868.2009.00723.x},
isbn = {1369-7412 (Print)$\backslash$r1369-7412 (Linking)},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Chromatin immuno-precipitation,Dimension reduction,Gene expression,Lasso,Microarrays,Partial least squares,Sparsity,Variable and feature selection},
number = {1},
pages = {3--25},
pmid = {20107611},
title = {{Sparse partial least squares regression for simultaneous dimension reduction and variable selection}},
volume = {72},
year = {2010}
}
@misc{Chung2013,
author = {Chung, Dongjun and Chun, Hyonho and Keles, Sunduz},
title = {{spls: Sparse Partial Least Squares (SPLS) Regression and Classification}},
url = {https://cran.r-project.org/package=spls},
year = {2013}
}
@article{Cook2004,
abstract = {We develop tests of the hypothesis of no effect for selected predictors in regression, without assuming a model for the conditional distribution of the response given the predictors. Predictor effects need not be limited to the mean function and smoothing is not required. The general approach is based on sufficient dimension reduction, the idea being to replace the predictor vector with a lower-dimensional version without loss of information on the regression. Methodology using sliced inverse regression is developed in detail.},
archivePrefix = {arXiv},
arxivId = {math/0406520},
author = {Cook, R. Dennis},
doi = {10.1214/009053604000000292},
eprint = {0406520},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Central subspace,Nonparametric regression,Sliced inverse regression},
number = {3},
pages = {1062--1092},
primaryClass = {math},
title = {{Testing predictor contributions in sufficient dimension reduction}},
volume = {32},
year = {2004}
}
@article{Coudret2014,
abstract = {Among methods to analyze high-dimensional data, the sliced inverse regression (SIR) is of particular interest for non-linear relations between the dependent variable and some indices of the covariate. When the dimension of the covariate is greater than the number of observations, classical versions of SIR cannot be applied. Various upgrades were then proposed to tackle this issue such as regularized SIR (RSIR) and sparse ridge SIR (SR-SIR), to estimate the parameters of the underlying model and to select variables of interest. In this paper, we introduce two new estimation methods respectively based on the QZ algorithm and on the Moore-Penrose pseudo-inverse.We also describe a new selection procedure of the most relevant components of the covariate that relies on a proximity criterion between submodels and the initial one. These approaches are compared with RSIR and SR-SIR in a simulation study. Finally we applied SIR-QZ and the associated selection procedure to a genetic dataset in order to find markers that are linked to the expression of a gene. These markers are called expression quantitative trait loci (eQTL).},
author = {Coudret, R. and Liquet, B. and Saracco, J.},
journal = {Journal de la Soci{\'{e}}t{\'{e}} Fran{\c{c}}aise de Statistique},
keywords = {High-dimensional data,dimension reduction,semiparametric regression,sparsity},
number = {2},
pages = {72--96},
title = {{Comparison of sliced inverse regression aproaches for undetermined cases}},
url = {http://journal-sfds.fr/index.php/J-SFdS/article/view/278},
volume = {155},
year = {2014}
}
@misc{Coudret2017,
author = {Coudret, Rapha{\"{e}}l and Liquet, Beno{\^{i}}t and Saracco, J{\'{e}}r{\^{o}}me},
title = {{edrGraphicalTools: Provides Tools for Dimension Reduction Methods}},
url = {https://cran.r-project.org/package=edrGraphicalTools},
year = {2017}
}
@book{DeBoor2001,
author = {de Boor, Carl},
publisher = {Springer},
series = {Applied Mathematical Sciences},
title = {{A Practical Guide to Splines}},
year = {2001}
}
@article{Duan1991,
author = {Duan, Naihua and Li, Ker-Chau},
file = {:home/luis/Dropbox/Biblioteca/articulos/Duan, Li - 1991 - Slicing Regression A Link-Free Regression Method.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {505--530},
title = {{Slicing Regression : A Link-Free Regression Method}},
volume = {19},
year = {1991}
}
@inproceedings{Emile-Geay2015,
address = {Boulder, Colorado, USA},
author = {Emile-Geay, Julien and McKay, Nicholas P. and Wang, Jianghao and Kaufman, Darrell and {PAGES2K Consortium}},
booktitle = {5th International Workshop on Climate Informatics.},
title = {{A Semantic Database of Temperature Proxies covering the Common Era}},
year = {2015}
}
@article{Friedman2010,
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
file = {:home/luis/Dropbox/Biblioteca/articulos/Friedman, Hastie, Tibshirani - 2010 - Regularization Paths for Generalized Linear Models via Coordinate Descent.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {fmri,functional connectivity,spatial model},
number = {1},
title = {{Regularization Paths for Generalized Linear Models via Coordinate Descent}},
volume = {33},
year = {2010}
}
@article{Gergis2016,
abstract = {AbstractWe present multi-proxy warm season (September–February) temperature reconstructions for the combined land–ocean region of Australasia (0°S–50°S, 110°E–180°E) covering A.D. 1000-2001. Using between two (R2) and 28 (R28) proxy records we compare four 1000-member ensemble reconstructions of regional temperature using four statistical methods: Principal Component Regression (PCR), Composite Plus Scale (CPS), Bayesian Hierarchical Models (LNA) and Pairwise Comparison (PaiCo). The reconstructions are compared with a three-member ensemble of GISS-E2-R model simulations and independent palaeoclimate records.Decadal fluctuations in Australasian temperatures are remarkably similar between the four reconstruction methods. There are, however, differences in the amplitude of temperature variations between the different statistical methods and proxy networks. When the R28 network is used, the warmest 30-year periods occur after 1950 in more than 70{\%} of ensemble members for all methods. However, reconstructions ...},
author = {Gergis, Jo{\"{e}}lle and Neukom, Raphael and Gallant, Ailie J.E. and Karoly, David J.},
doi = {10.1175/JCLI-D-13-00781.1},
file = {:home/luis/Dropbox/Biblioteca/articulos/Gergis et al. - 2016 - Australasian temperature reconstructions spanning the Last Millennium.pdf:pdf},
issn = {08948755},
journal = {Journal of Climate},
keywords = {Australia,Climate change,Climate variability,Geographic location/entity,Paleoclimate,Physical Meteorology and Climatology,Variability},
number = {15},
pages = {5365--5392},
title = {{Australasian temperature reconstructions spanning the Last Millennium}},
volume = {29},
year = {2016}
}
@article{Gneiting2007a,
abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the ...},
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
file = {:home/luis/Dropbox/Biblioteca/articulos/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Bregman divergence,Brier score,Coherent,Continuous ranked probability score,Cross-validation,Entropy,Kernel score,Loss function,Minimum contrast estimation,Negative definite function,Prediction interval,Predictive distribution,Quantile forecast,Scoring rule,Skill score,Strictly proper,Utility function},
language = {en},
month = {mar},
number = {477},
pages = {359--378},
publisher = {Taylor {\&} Francis},
title = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000001437{\#}.UfauueGm1As},
volume = {102},
year = {2007}
}
@article{Hakim2016,
abstract = {An “offline” approach to DA is used, where static ensemble samples are drawn from existing CMIP climate-model simulations to serve as the prior estimate of climate variables.We use linear, univariate forward models (“proxy systemmodels (PSMs)”) that map climate variables to proxy measurements by fitting proxy data to 2mair temperature fromgridded instrumental temperature data; the linear PSMs are then used to predict proxy values from the prior estimate. Results for the LMR are compared against six gridded instrumental temperature data sets and 25{\%} of the proxy records are withheld from assimilation for independent verification. Results show broad agreement with previous reconstructions of Northern Hemisphere mean 2mair temperature, with millennial-scale cooling, a multicentennial warm period around 1000 C.E., and a cold period coincident with the Little Ice Age (circa 1450–1800 C.E.). Verification against gridded instrumental data sets during 1880–2000 C.E. reveals greatest skill in the tropics and lowest skill over Northern Hemisphere land areas. Verification against independent proxy records indicates substantial improvement relative to the model (prior) data without proxy assimilation. As an illustrative example, we present multivariate reconstructed fields for a singular event, the 1808/1809 “mystery” volcanic eruption, which reveal global cooling that is strongly enhanced locally due to the presence of the Pacific-North America wave pattern in the 500 hPa geopotential height field},
author = {Hakim, Gregory J. and Emile-Geay, Julien and Steig, Eric J. and Noone, David and Anderson, David M. and Tardif, Robert and Steiger, Nathan and Perkins, Walter A.},
doi = {10.1002/2016JD024751},
file = {:home/luis/Dropbox/Biblioteca/articulos/Hakim et al. - 2016 - The last millennium climate reanalysis project Framework and first results.pdf:pdf},
issn = {21562202},
journal = {Journal of Geophysical Research},
keywords = {10.1002/2016JD024751 and paleoclimate,data assimilation,proxies,volcanic eruption},
number = {12},
pages = {6745--6764},
title = {{The last millennium climate reanalysis project: Framework and first results}},
volume = {121},
year = {2016}
}
@article{Hanhijarvi2013,
abstract = {Existing multi-proxy climate reconstruction methods assume the suitably transformed proxy time series are linearly related to the target climate variable, which is likely a simplifying assumption for many proxy records. Furthermore, with a single exception, these methods face problems with varying temporal resolutions of the proxy data. Here we introduce a new reconstruction method that uses the ordering of all pairs of proxy observations within each record to arrive at a consensus time series that best agrees with all proxy records. The resulting unitless composite time series is subsequently calibrated to the instrumental record to provide an estimate of past climate. By considering only pairwise comparisons, this method, which we call PaiCo, facilitates the inclusion of records with differing temporal resolutions, and relaxes the assumption of linearity to the more general assumption of a monotonically increasing relationship between each proxy series and the target climate variable. We apply PaiCo to a newly assembled collection of high-quality proxy data to reconstruct the mean temperature of the Northernmost Atlantic region, which we call Arctic Atlantic, over the last 2,000 years. The Arctic Atlantic is a dynamically important region known to feature substantial temperature variability over recent millennia, and PaiCo allows for a more thorough investigation of the Arctic Atlantic regional climate as we include a diverse array of terrestrial and marine proxies with annual to multidecadal temporal resolutions. Comparisons of the PaiCo reconstruction to recent reconstructions covering larger areas indicate greater climatic variability in the Arctic Atlantic than for the Arctic as a whole. The Arctic Atlantic reconstruction features temperatures during the Roman Warm Period and Medieval Climate Anomaly that are comparable or even warmer than those of the twentieth century, and coldest temperatures in the middle of the nineteenth century, just prior to the onset of the recent warming trend.},
author = {Hanhij{\"{a}}rvi, Sami and Tingley, Martin P. and Korhola, Atte},
doi = {10.1007/s00382-013-1701-4},
file = {:home/luis/Dropbox/Biblioteca/articulos/Hanhij{\"{a}}rvi, Tingley, Korhola - 2013 - Pairwise comparisons to reconstruct mean temperature in the Arctic Atlantic Region over the last.pdf:pdf},
isbn = {0930-7575$\backslash$r1432-0894},
issn = {09307575},
journal = {Climate Dynamics},
keywords = {Multiproxy reconstruction,Non-linear method,North Atlantic,Pairwise comparisons,Temperature},
number = {7-8},
pages = {2039--2060},
title = {{Pairwise comparisons to reconstruct mean temperature in the Arctic Atlantic Region over the last 2,000 years}},
volume = {41},
year = {2013}
}
@article{Jolliffe1982,
author = {Jolliffe, Ian T.},
file = {:home/luis/Dropbox/Biblioteca/articulos/Jolliffe - 1982 - A Note on the Use of Principal Components in Regression.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {3},
pages = {300--303},
title = {{A Note on the Use of Principal Components in Regression}},
volume = {31},
year = {1982}
}
@article{Kaufman2014,
abstract = {Proxy-based reconstructions of past climate provide insights into externally forced and intrinsic variability over regional to global scales and can be used to place recent trends in a long-term context. Comparisons between these reconstructions and the output of Earth system models provide evaluation opportunities to improve our understanding of climate forcings on time scales that are not adequately represented by the instrumental record. They also provide a heuristic tool to explore mechanisms of climate variability, with implications for future climate projections [Schmidt et al., 2014].},
author = {Kaufman, Darrell},
doi = {10.1002/2014EO400001},
file = {:home/luis/Dropbox/Biblioteca/articulos/Kaufman - 2014 - A Community-Driven Framework for Climate Reconstructions.pdf:pdf},
issn = {2324-9250},
journal = {Eos, Transactions American Geophysical Union},
keywords = {1616 Climate variability,1626 Global climate models,1637 Regional climate change,Climate reconstructions},
number = {40},
pages = {361--362},
title = {{A Community-Driven Framework for Climate Reconstructions}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/2014EO400001/abstract},
volume = {95},
year = {2014}
}
@article{boli1,
abstract = {Understanding the dynamics of climate change in its full richness
requires the knowledge of long temperature time series. Although
long-term, widely distributed temperature observations are not available,
there are other forms of data, known as climate proxies, that can
have a statistical relationship with temperatures and have been used
to infer temperatures in the past before direct measurements. We
propose a Bayesian hierarchical model to reconstruct past temperatures
that integrates information from different sources, such as proxies
with different temporal resolution and forcings acting as the external
drivers of large scale temperature evolution. Additionally, this
method allows us to quantify the uncertainty of the reconstruction
in a rigorous manner. The reconstruction method is assessed, using
a global climate model as the true climate system and with synthetic
proxy data derived from the simulation. The target is to reconstruct
Northern Hemisphere temperature from proxies that mimic the sampling
and errors from tree ring measurements, pollen indices, and borehole
temperatures. The forcing series used as covariates are solar irradiance,
volcanic aerosols, and greenhouse gas concentrations. The Bayesian
model was successful in integrating these different sources of information
in creating a coherent reconstruction. Within the context of this
numerical testbed, a statistical process model that includes the
external forcings can improve the quality of a hemispheric reconstruction
when long time scale proxy information is not available. This article
has supplementary material online.},
author = {Li, Bo and Nychka, Douglas W and Ammann, Caspar M},
journal = {Journal of the American Statistical Association},
keywords = {*file-import-12-04-11,bayes,climate,reconstruction},
number = {491},
pages = {883--895},
title = {{The Value of Multiproxy Reconstruction of Past Climate}},
volume = {105},
year = {2010}
}
@article{Li1991,
abstract = {Modem advances in computing power have greatly widened scientists' scope in gathering which might have been ignored in the past. Yet to effectively many variables, information an easy task, although our ability to interact this article, we propose a novel data-analytic x without going through any parametric properties vectors. This model looks like a nonlinear unknown. For effectively space] generated regression coefficients. for e.d.r. directions. Furthermore, model-fitting inverse view of regression; that is, instead of regressing regression, the univariate and investigating with data has been much enhanced by recent innovations tool, sliced inverse regression or nonparametric (SIR), for reducing x against y. Forward regression and inverse regression are connected by a theorem that motivates of SIR are investigated by the 3k'S. This makes our goal different In fact, the fk'S themselves covariance, the inverse regression curve, E(x I y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix shows that under a suitable condition, if the distribution for the estimated inverse regression smoothing is needed. SIR can be easily implemented effectively curve can be conducted to locate its main orientation, yielding the dimension information process. This method explores the simplicity output variable y against the multivariate that the functional reducing the dimension, we need only to estimate the space [effective are not identifiable of x has been standardized from scan a large pool of variables is not in dynamic graphics. In of the input variable of the x, we regress under a model of the form, y = f(lx, ..., 8Kx, e), where the 3ks are the unknown row except for the crucial difference from the usual one in regression analysis, the estimation without a specific structural this method. The theoretical form off is completely dimension reduction (e.d.r.) of all the form onf. Our main theorem we use a simple step function to estimate the inverse regression curve. No complicated on personal computers. By simulation, we demonstrate plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin- is proposed to address the issue of whether or not a direction KEY WORDS: Dynamic graphics; Principal component analysis; Projection pursuit. 1. INTRODUCTION lationship variable x, a p-dimensional parametric model is parsimonious, as the maximum likelihood Regression analysis is a popular way of studying between a response variable y and its explanatory column vector. Quite often, a the re- model is used to guide the analysis. When the such have standard estimation proved to be successful in gathering techniques or the least squares method information data. In most applications, however, any parametric at best an approximation alternatives that offer from the model is an adequate model is not easy. When there are no persua- sive models available, nonparametric emerge as promising to the true one, and the search for regression regression only the continuity function. on the presence point of interest information. is the idea of local smoothing, or differentiability of sufficiently * Ker-Chau Li is Professor, ematics, University For one-dimensional property ibility in modeling. A common theme of nonparametric regression the needed flex- which explores The success of local smoothing many data points around each in the design space to provide adequate problems, many smooth- of California, Los Angeles, CA 90024. This research the 3's as an effective Division of Statistics, Department of Math- was supported in part by the National Science Foundation under grants DMS86-02018 and DMS89-02494. It has been a long time since I intro- duced SIR in talks given at Berkeley, Bell Labs, and Rutgers in 1985. I received many useful questions and suggestions from these audiences. I am indebted to Naihua Duan for stimulating leading eventually to Duan and Li (1991). Peter Bickel brought semi- parametric literature to my attention. edge in areas of dimension reduction and multivariate Dennis Cook, who introduced Section 6.3 would have been further regression" this article. Three referees for improving the presentation. David Brillinger, delayed. As a replacement for "slice gested to me by Don Ylvisaker, who also helped me clear hurdles in publishing suggestions whose work has inspired me so much. 316 or "slicing regression," used earlier, the name SIR was sug- nice and an associate editor offered Finally, I would like to thank Jan de Leeuw broadened my knowl- XLISP-STAT to me, the appearance of techniques found by SIR is spurious.},
author = {Li, Ker-Chau},
doi = {10.2307/2290568},
file = {:home/luis/Dropbox/Biblioteca/articulos/Li - 1991 - Sliced Inverse Regression for Dimension Reduction.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {dynamic graphics,principal component analysis,projection pursuit},
number = {414},
pages = {316--327},
title = {{Sliced Inverse Regression for Dimension Reduction}},
volume = {86},
year = {1991}
}
@article{Li2008,
abstract = {In high-dimensional data analysis, sliced inverse regression (SIR) has proven to be an effective dimension reduction tool and has enjoyed wide applications. The usual SIR, however, cannot work with problems where the number of predictors, p, exceeds the sample size, n, and can suffer when there is high collinearity among the predictors. In addition, the reduced dimensional space consists of linear combinations of all the original predictors and no variable selection is achieved. In this article, we propose a regularized SIR approach based on the least-squares formulation of SIR. The L2 regularization is introduced, and an alternating least-squares algorithm is developed, to enable SIR to work with n {\textless} p and highly correlated predictors. The L1 regularization is further introduced to achieve simultaneous reduction estimation and predictor selection. Both simulations and the analysis of a microarray expression data set demonstrate the usefulness of the proposed method.},
author = {Li, Lexin and Yin, Xiangrong},
doi = {10.1111/j.1541-0420.2007.00836.x},
issn = {0006341X},
journal = {Biometrics},
keywords = {Regularized least squares,Sliced inverse regression,Sufficient dimension reduction},
number = {1},
pages = {124--131},
pmid = {17651455},
title = {{Sliced inverse regression with regularizations}},
volume = {64},
year = {2008}
}
@article{Liquet2012,
abstract = {Sliced inverse regression (SIR) and related methods were introduced in order to reduce the dimensionality of regression problems. In general semiparametric regression framework, these methods determine linear combinations of a set of explanatory variables X related to the response variable Y, without losing information on the conditional distribution of Y given X. They are based on a "slicing step" in the population and sample versions. They are sensitive to the choice of the number H of slices, and this is particularly true for SIR-II and SAVE methods. At the moment there are no theoretical results nor practical techniques which allows the user to choose an appropriate number of slices. In this paper, we propose an approach based on the quality of the estimation of the effective dimension reduction (EDR) space: the square trace correlation between the true EDR space and its estimate can be used as goodness of estimation. We introduce a na {\textless} ve bootstrap estimation of the square trace correlation criterion to allow selection of an "optimal" number of slices. Moreover, this criterion can also simultaneously select the corresponding suitable dimension K (number of the linear combination of X). From a practical point of view, the choice of these two parameters H and K is essential. We propose a 3D-graphical tool, implemented in R, which can be useful to select the suitable couple (H, K). An R package named "edrGraphicalTools" has been developed. In this article, we focus on the SIR-I, SIR-II and SAVE methods. Moreover the proposed criterion can be use to determine which method seems to be efficient to recover the EDR space, that is the structure between Y and X. We indicate how the proposed criterion can be used in practice. A simulation study is performed to illustrate the behavior of this approach and the need for selecting properly the number H of slices and the dimension K. A short real-data example is also provided.},
author = {Liquet, Beno{\^{i}}t and Saracco, J{\'{e}}r{\^{o}}me},
doi = {10.1007/s00180-011-0241-9},
file = {:home/luis/Dropbox/Biblioteca/articulos/Liquet, Saracco - 2012 - A graphical tool for selecting the number of slices and the dimension of the model in SIR and SAVE approaches.pdf:pdf},
issn = {09434062},
journal = {Computational Statistics},
keywords = {Bootstrap,Dimension reduction,Sliced average variance estimation (SAVE),Sliced inverse regression (SIR),Square trace correlation},
number = {1},
pages = {103--125},
title = {{A graphical tool for selecting the number of slices and the dimension of the model in SIR and SAVE approaches}},
volume = {27},
year = {2012}
}
@article{luterbacher2004european,
abstract = {Multiproxy reconstructions of monthly and seasonal surface temperature
fields for Europe back to 1500 show that the late 20th- and early
21st-century European climate is very likely ({\textgreater}95{\%} confidence level)
warmer than that of any time during the past 500 years. This agrees
with findings for the entire Northern Hemisphere. European winter
average temperatures during the period 1500 to 1900 were reduced
by ∼0.5°C (0.25°C for annual mean temperatures) compared to the 20th
century. Summer temperatures did not experience systematic century-scale
cooling relative to present conditions. The coldest European winter
was 1708/1709; 2003 was by far the hottest summer.},
author = {Luterbacher, J and Dietrich, D and Xoplaki, E and Grosjean, M and Wanner, H},
journal = {Science},
keywords = {*file-import-12-09-20},
number = {5663},
pages = {1499--1503},
publisher = {American Association for the Advancement of Science},
title = {{European seasonal and annual temperature variability, trends, and extremes since 1500}},
volume = {303},
year = {2004}
}
@article{mann2005testing,
abstract = {Two widely used statistical approaches to reconstructing past climate
histories from climate ” proxy” data such as tree rings, corals,
and ice cores are investigated using synthetic ” pseudoproxy” data
derived from a simulation of forced climate changes over the past
1200 yr. These experiments suggest that both statistical approaches
should yield reliable reconstructions of the true climate history
within estimated uncertainties, given estimates of the signal and
noise attributes of actual proxy data networks.},
author = {Mann, M E and Rutherford, S and Wahl, E and Ammann, C},
journal = {Journal of Climate},
keywords = {*file-import-12-09-20},
number = {20},
pages = {4097--4107},
title = {{Testing the fidelity of methods used in proxy-based reconstructions of past climate}},
volume = {18},
year = {2005}
}
@article{mann2007robust,
abstract = {We present results from continued investigations into the fidelity
of covariance-based climate field reconstruction (CFR) approaches
used in proxy-based climate reconstruction. Our experiments employ
synthetic ” pseudoproxy” data derived from simulations of forced
climate changes over the past millennium. Using networks of these
pseudoproxy data, we investigate the sensitivity of CFR performance
to signal-to-noise ratios, the noise spectrum, the spatial sampling
of pseudoproxy locations, the statistical representation of predictors
used, and the diagnostic used to quantify reconstruction skill. Our
results reinforce previous conclusions that CFR methods, correctly
implemented and applied to suitable networks of proxy data, should
yield reliable reconstructions of past climate histories within estimated
uncertainties. Our results also demonstrate the deleterious impact
of a linear detrending procedure performed recently in certain CFR
studies and illustrate flaws in some previously proposed metrics
of reconstruction skill.},
author = {Mann, Michael E and Rutherford, Scott and Wahl, Eugene and Ammann, Caspar},
doi = {10.1029/2006JD008272},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
number = {D12},
pages = {D12109+},
publisher = {American Geophysical Union},
title = {{Robustness of proxy-based climate field reconstruction methods}},
url = {http://dx.doi.org/10.1029/2006JD008272},
volume = {112},
year = {2007}
}
@article{Martins2013,
abstract = {The INLA approach for approximate Bayesian inference for latent Gaussian models has been shown to give fast and accurate estimates of posterior marginals and also to be a valuable tool in practice via the R-package R-INLA. New developments in the R-INLA are formalized and it is shown how these features greatly extend the scope of models that can be analyzed by this interface. The current default method in R-INLA to approximate the posterior marginals of the hyperparameters using only a modest number of evaluations of the joint posterior distribution of the hyperparameters, without any need for numerical integration, is discussed. ?? 2013 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0333v1},
author = {Martins, Thiago G. and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
doi = {10.1016/j.csda.2013.04.014},
eprint = {arXiv:1210.0333v1},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximate,Bayesian inference,INLA,Latent Gaussian models},
pages = {68--83},
publisher = {Elsevier B.V.},
title = {{Bayesian computing with INLA: New features}},
url = {http://dx.doi.org/10.1016/j.csda.2013.04.014},
volume = {67},
year = {2013}
}
@article{Meinshausen2016,
abstract = {Atmospheric greenhouse gas concentrations are at unprecedented, record-high levels compared to pre-industrial reconstructions over the last 800,000 years. Those elevated greenhouse gas concentrations warm the planet and together with net cooling effects by aerosols, they are the reason of observed climate change over the past 150 years. An accurate representation of those concentrations is hence important to understand and model recent and future climate change. So far, community efforts to create composite datasets with seasonal and latitudinal information have focused on marine boundary layer conditions and recent trends since 1980s. Here, we provide consolidated data sets of historical atmospheric (volume) mixing ratios of 43 greenhouse gases specifically for the purpose of climate model runs. The presented datasets are based on AGAGE and NOAA networks and a large set of literature studies. In contrast to previous intercomparisons, the new datasets are latitudinally resolved, and include seasonality over the period between year 0 to 2014. We assimilate data for CO2, methane (CH4) and nitrous oxide (N2O), 5 chlorofluorocarbons (CFCs), 3 hydrochlorofluorocarbons (HCFCs), 16 hydrofluorocarbons (HFCs), 3 halons, methyl bromide (CH3Br), 3 perfluorocarbons (PFCs), sulfur hexafluoride (SF6), nitrogen triflouride (NF3) and sulfuryl fluoride (SO2F2). We estimate 1850 annual and global mean surface mixing ratios of CO2 at 284.3 ppmv, CH4 at 808.2 ppbv and N2O at 273.0 ppbv and quantify the seasonal and hemispheric gradients of surface mixing ratios. Compared to earlier intercomparisons, the stronger implied radiative forcing in the northern hemisphere winter (due to the latitudinal gradient and seasonality) may help to improve the skill of climate models to reproduce past climate and thereby reduce uncertainty in future projections.},
author = {Meinshausen, Malte and Vogel, Elisabeth and Nauels, Alexander and Lorbacher, Katja and Meinshausen, Nicolai and Etheridge, David and Fraser, Paul and Montzka, Stephen A. and Rayner, Peter and Trudinger, Cathy and Krummel, Paul and Beyerle, Urs and Cannadell, Josep G. and Daniel, John S. and Enting, Ian and Law, Rachel M. and O'Doherty, Simon and Prinn, Ron G. and Reimann, Stefan and Rubino, Mauro and Velders, Guus J. M. and Vollmer, Martin K. and Weiss, Ray},
doi = {10.5194/gmd-2016-169},
issn = {1991-962X},
journal = {Geoscientific Model Development Discussions},
number = {August},
pages = {1--122},
title = {{Historical greenhouse gas concentrations}},
url = {http://www.geosci-model-dev-discuss.net/gmd-2016-169/},
volume = {1},
year = {2016}
}
@article{Morice2012,
abstract = {Recent developments in observational near-surface air temperature and sea-surface temperature analyses are combined to produce HadCRUT4, a new data set of global and regional temperature evolution from 1850 to the present. This includes the addition of newly digitised measurement data, both over land and sea, new sea-surface temperature bias adjustments and a more comprehensive error model for describing uncertainties in sea-surface temperature measurements. An ensemble approach has been adopted to better describe complex temporal and spatial interdependencies of measurement and bias uncertainties and to allow these correlated uncertainties to be taken into account in studies that are based upon HadCRUT4. Climate diagnostics computed from the gridded data set broadly agree with those of other global near-surface temperature analyses. Fitted linear trends in temperature anomalies are approximately 0.07 degC/decade from 1901 to 2010 and 0.17 degC/decade from 1979 to 2010 globally. Northern/southern hemispheric trends are 0.08/0.07 degC/decade over 1901 to 2010 and 0.24/0.10 degC/decade over 1979 to 2010. Linear trends in other prominent near-surface temperature analyses agree well with the range of trends computed from the HadCRUT4 ensemble members.},
author = {Morice, Colin P. and Kennedy, John J. and Rayner, Nick A. and Jones, Phil D.},
doi = {10.1029/2011JD017187},
isbn = {2156-2202},
issn = {01480227},
journal = {Journal of Geophysical Research Atmospheres},
keywords = {climate,historical temperature,near surface temperature,observational ensemble},
number = {8},
pages = {1--22},
title = {{Quantifying uncertainties in global and regional temperature change using an ensemble of observational estimates: The HadCRUT4 data set}},
volume = {117},
year = {2012}
}
@article{Muff2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.3065v2},
author = {Muff, Stefanie and Riebler, Andrea and Held, Leonhard and Rue, H{\aa}vard and Saner, Philippe},
doi = {10.1111/rssc.12069},
eprint = {arXiv:1302.3065v2},
issn = {00359254},
journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
keywords = {approximation,bayesian analysis,berkson error,classical error,integrated nested laplace,measurement error},
number = {2},
pages = {231--252},
title = {{Bayesian analysis of measurement error models using integrated nested Laplace approximations}},
volume = {64},
year = {2015}
}
@article{PAGES2kConsortium2017,
abstract = {Reproducible climate reconstructions of the Common Era are key to placing twentieth century warming in the context of natural variability. Here we present a community-sourced database of temperature-sensitive proxy records. The database gathers 661 records from 628 locations around the globe, including all continental regions and major ocean basis, with temporal resolution varying from monthly to centennial. The rela-tionship to a gridded temperature dataset (HadCRUT4.2) is analyzed at seasonal and annual scales, and a preliminary reconstruction of globally-averaged annual-mean temperature is presented. The database is shared in the Linked Paleo Data (LiPD) format, including serializations in the Matlab, R and Python languages.},
author = {{PAGES 2k Consortium}},
doi = {DOI: 10.1038/sdata.2017.88},
file = {:home/luis/Dropbox/Biblioteca/articulos/PAGES 2k Consortium - 2017 - A global multiproxy database for temperature reconstructions of the Common Era.pdf:pdf},
issn = {2052-4463},
journal = {Scientific Data},
number = {170088},
pages = {1--33},
title = {{A global multiproxy database for temperature reconstructions of the Common Era}},
volume = {4},
year = {2017}
}
@article{PAGES2KConsortium2013,
abstract = {ERRATUM Continental-scale temperature variability during the past two millennia PAGES 2k Consortium Nature Geoscience 6, 339–346 (2013); published online 21 April 2013; corrected aſter print 14 May 2013. In the version of this Progress Article originally published, the references “43–54” cited in the caption of Fig. 4a should have read “43–45”. This has been corrected in the PDF and HTML versions. NATURE},
author = {{PAGES2K Consortium}},
doi = {10.1038/ngeo1849},
file = {:home/luis/Dropbox/Biblioteca/articulos/PAGES2K Consortium - 2013 - Continental-scale temperature variability during the past two millennia.pdf:pdf},
isbn = {1752-0894},
issn = {1752-0894},
journal = {Nature Geoscience},
number = {6},
pages = {503--503},
title = {{Continental-scale temperature variability during the past two millennia}},
url = {http://www.nature.com/doifinder/10.1038/ngeo1849},
volume = {6},
year = {2013}
}
@book{Ramsay2005,
address = {New York, NY},
author = {Ramsay, J.O. and Silverman, B.W.},
edition = {Second},
publisher = {Springer},
series = {Springer Series in Statistics},
title = {{Functional Data Analysis}},
year = {2005}
}
@article{Rue2009,
author = {Rue, H{\^{a}}vard and Martino, Sara and Chopin, Nicolas},
journal = {Journal of the Royal Statistical Society . Series B ( Methodological )},
keywords = {laplace approxima-,latent gaussian,models using integrated nested,roximate bayesian inference for},
number = {2},
pages = {319--392},
title = {{Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations}},
volume = {71},
year = {2009}
}
@article{Ruiz-Cardenas2012,
abstract = {Inference in state-space models usually relies on recursive forms for filtering and smoothing of the state vectors regarding the temporal structure of the observations, an assumption that is, from our view point, unnecessary if the dataset is fixed, that is, completely available before analysis. In this paper, we propose a computational framework to perform approximate full Bayesian inference in linear and generalized dynamic linear models based on the Integrated Nested Laplace Approximation (INLA) approach. The proposed framework directly approximates the posterior marginals of interest disregarding the assumption of recursive updating/estimation of the states and hyperparameters in the case of fixed datasets and, therefore, enable us to do fully Bayesian analysis of complex state-space models more easily and in a short computational time. The proposed framework overcomes some limitations of current tools in the dynamic modeling literature and is vastly illustrated with a series of simulated as well as well known real-life examples from the literature, including realistically complex models with correlated error structures and models with more than one state vector, being mutually dependent on each other. R code is available online for all the examples presented. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Ruiz-C{\'{a}}rdenas, Ramiro and Krainski, Elias T. and Rue, H{\aa}vard},
doi = {10.1016/j.csda.2011.10.024},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximate Bayesian inference,Augmented model,Laplace approximation,Spatio-temporal dynamic models,State-space models},
pages = {1808--1828},
publisher = {Elsevier B.V.},
title = {{Direct fitting of dynamic models using integrated nested Laplace approximations - INLA}},
url = {http://dx.doi.org/10.1016/j.csda.2011.10.024},
volume = {56},
year = {2012}
}
@article{rutherford2003climate,
abstract = {The fidelity of climate reconstructions employing covariance-based
calibration techniques is tested with varying levels of sparseness
of available data during intervals of relatively constant (stationary)
and increasing (nonstationary) forcing. These tests employ a regularized
expectation-maximization algorithm using surface temperature data
from both the instrumental record and coupled ocean–atmosphere model
integrations. The results indicate that if radiative forcing is relatively
constant over a data-rich calibration period and increases over a
data-sparse reconstruction period, the imputed temperatures in the
reconstruction period may be biased and may underestimate the true
temperature trend. However, if radiative forcing is stationary over
a data-sparse reconstruction period and increases over a data-rich
calibration period, the imputed values in the reconstruction period
are nearly unbiased. These results indicate that using the data-rich
part of the twentieth-century instrumental record (which contains
an increasing temperature trend plausibly associated with increasing
radiative forcing) for calibration does not significantly bias reconstructions
of prior climate.},
author = {Rutherford, S and Mann, M E and Delworth, T L and Stouffer, R J},
journal = {Journal of Climate},
keywords = {*file-import-12-09-21},
number = {3},
pages = {462--479},
title = {{Climate field reconstruction under stationary and nonstationary forcing}},
volume = {16},
year = {2003}
}
@article{bradley2005proxy,
abstract = {Results are presented from a set of experiments designed to investigate
factors that may influence proxy-based reconstructions of large-scale
temperature patterns in past centuries. The factors investigated
include 1) the method used to assimilate proxy data into a climate
reconstruction, 2) the proxy data network used, 3) the target season,
and 4) the spatial domain of the reconstruction. Estimates of hemispheric-mean
temperature are formed through spatial averaging of reconstructed
temperature patterns that are based on either the local calibration
of proxy and instrumental data or a more elaborate multivariate climate
field reconstruction approach. The experiments compare results based
on the global multiproxy dataset used by Mann and coworkers, with
results obtained using the extratropical Northern Hemisphere (NH)
maximum latewood tree-ring density set used by Briffa and coworkers.
Mean temperature reconstructions are compared for the full NH (Tropics
and extratropics, land and ocean) and extratropical continents only,
withvarying target seasons (cold-season half year, warm-season half
year, and annual mean). The comparisons demonstrate dependence of
reconstructions on seasonal, spatial, and methodological considerations,
emphasizing the primary importance of the target region and seasonal
window of the reconstruction. The comparisons support the generally
robust nature of several previously published estimates of NH mean
temperature changes in past centuries and suggest that further improvements
in reconstructive skill are most likely to arise from an emphasis
on the quality, rather than quantity, of available proxy data.},
author = {Rutherford, S and Mann, M E and Osborn, T J and Briffa, K R and Jones, P D and Bradley, R S and Hughes, M K},
journal = {Journal of Climate},
keywords = {*file-import-12-09-21},
number = {13},
pages = {2308--2329},
title = {{Proxy-Based Northern Hemisphere Surface Temperature Reconstructions: Sensitivity to Method, Predictor Network, Target Season, and Target Domain}},
volume = {18},
year = {2005}
}
@article{Scheuerer2014,
abstract = {Statistical post-processing of dynamical forecast ensembles is an essential component of weather forecasting. In this article, we present a post-processing method which generates full predictive probability distributions for precipitation accumulations based on ensemble model output statistics (EMOS). We model precipitation amounts by a generalized extreme value distribution which is left-censored at zero. This distribution permits modelling precipitation on the original scale without prior transformation of the data. A closed form expression for its continuous ranked probability score can be derived and permits computationally efficient model fitting. We discuss an extension of our approach which incorporates further statistics characterizing the spatial variability of precipitation amounts in the vicinity of the location of interest. The proposed EMOS method is applied to daily 18 h forecasts of 6 h accumulated precipitation over Germany in 2011 using the COSMO-DE ensemble prediction system operated by the German Meteorological Service. It yields calibrated and sharp predictive distributions and compares favourably with extended logistic regression and Bayesian model averaging which are state-of-the-art approaches for precipitation post-processing. The incorporation of neighbourhood information further improves predictive performance and turns out to be a useful strategy to account for displacement errors of the dynamical forecasts in a probabilistic forecasting framework. {\textcopyright} 2013 Royal Meteorological Society.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.0893v1},
author = {Scheuerer, M.},
doi = {10.1002/qj.2183},
eprint = {arXiv:1302.0893v1},
issn = {1477870X},
journal = {Quarterly Journal of the Royal Meteorological Society},
keywords = {Ensemble prediction,Forecast calibration,Precipitation,Short-range forecasting},
number = {680},
pages = {1086--1096},
title = {{Probabilistic quantitative precipitation forecasting using Ensemble Model Output Statistics}},
volume = {140},
year = {2014}
}
@article{smerdon2010pseudoproxy,
abstract = {Canonical correlation analysis (CCA) is evaluated for paleoclimate
field reconstructions in the context of pseudoproxy experiments assembled
from the millennial integration (850–1999 c.e.) of the National Center
for Atmospheric Research Community Climate System Model, version
1.4. A parsimonious method for selecting the order of the CCA model
is presented. Results suggest that the method is capable of resolving
multiple (3–13) climatic patterns given the estimated proxy observational
network and the amount of observational uncertainty. CCA reconstructions
are compared to those derived from the regularized expectation maximization
method using ridge regression regularization (RegEM-Ridge). CCA and
RegEM-Ridge yield similar skill patterns that are characterized by
high correlation regions collocated with dense pseudoproxy sampling
areas in North America and Europe. Both methods also produce reconstructions
characterized by spatially variable warm biases and variance losses,
particularly at high pseudoproxy noise levels. RegEM-Ridge in particular
is subject to significantly larger variance losses than CCA, even
though the spatial correlation patterns of the two methods are comparable.
Results collectively indicate the importance of evaluating the field
performance of methods that target spatial climate patterns during
the last several millennia and indicate that the results of currently
available climate field reconstructions should be interpreted carefully.},
author = {Smerdon, J E and Kaplan, A and Chang, D and Evans, M N},
journal = {Journal of Climate},
keywords = {*file-import-12-09-21},
number = {18},
pages = {4856--4880},
title = {{A Pseudoproxy Evaluation of the CCA and RegEM Methods for Reconstructing Climate Fields of the Last Millennium}},
volume = {23},
year = {2010}
}
@article{steig2009,
abstract = {Assessments of Antarctic temperature change have emphasized the contrast
between strong warming of the Antarctic Peninsula and slight cooling
of the Antarctic continental interior in recent decades1. This pattern
of temperature change has been attributed to the increased strength
of the circumpolar westerlies, largely in response to changes in
stratospheric ozone2. This picture, however, is substantially incomplete
owing to the sparseness and short duration of the observations. Here
we show that significant warming extends well beyond the Antarctic
Peninsula to cover most of West Antarctica, an area of warming much
larger than previously reported. West Antarctic warming exceeds 0.1 °C
per decade over the past 50 years, and is strongest in winter and
spring. Although this is partly offset by autumn cooling in East
Antarctica, the continent-wide average near-surface temperature trend
is positive. Simulations using a general circulation model reproduce
the essential features of the spatial pattern and the long-term trend,
and we suggest that neither can be attributed directly to increases
in the strength of the westerlies. Instead, regional changes in atmospheric
circulation and associated changes in sea surface temperature and
sea ice are required to explain the enhanced warming in West Antarctica.},
author = {Steig, Eric J and Schneider, David P and Rutherford, Scott D and Mann, Michael E and Comiso, Josefino C and Shindell, Drew T},
doi = {10.1038/nature07669},
issn = {0028-0836},
journal = {Nature},
number = {7228},
pages = {459--462},
publisher = {Nature Publishing Group},
title = {{Warming of the Antarctic ice-sheet surface since the 1957 International Geophysical Year}},
url = {http://dx.doi.org/10.1038/nature07669},
volume = {457},
year = {2009}
}
@article{Thomason2016,
author = {Thomason, Larry and Vernier, Jean-Paul and Bourassa, Adam and Arfeuille, Florian and Bingen, Christine and Peter, Thomas and Luo, Beiping},
journal = {To be submitted to Geoscientific Model Development Discussions},
title = {{Stratospheric Aerosol Data Set (SADS Version 2) Prospectus}},
year = {2016}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:home/luis/Dropbox/Biblioteca/articulos/Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society . Series B ( Methodological )},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@article{Tierney1986,
author = {Tierney, Luke and Kadane, Joseph B},
file = {:home/luis/Dropbox/Biblioteca/articulos/Tierney, Kadane - 1986 - Accurate Approximations for Posterior Moments and Marginal Densities.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {a user of bayesian,asymp-,bayesian inference,computation of integrals,laplace method,log posterior density,methods in practice needs,second derivative of the,the inverse of the,to be able,totic expansions},
number = {393},
pages = {82--86},
title = {{Accurate Approximations for Posterior Moments and Marginal Densities}},
volume = {81},
year = {1986}
}
@article{tingley2013_Ext,
author = {Tingley, M P and Huybers, P},
journal = {Nature},
number = {7444},
pages = {201--205},
publisher = {Nature Publishing Group},
title = {{Recent temperature extremes at high northern latitudes unprecedented in the past 600 years}},
volume = {496},
year = {2013}
}
@article{tingley2,
abstract = {Abstract Part I presented a Bayesian algorithm for reconstructing
climate anomalies in space and time (BARCAST). This method involves
specifying simple parametric forms for the spatial covariance and
temporal evolution of the climate field as well as ?observation equations?
describing the relationships between the data types and the corresponding
true values of the climate field. As this Bayesian approach to reconstructing
climate fields is new and different, it is worthwhile to compare
it in detail to the more established regularized expectation?maximization
(RegEM) algorithm, which is based on an empirical estimate of the
joint data covariance matrix and a multivariate regression of the
instrumental time series onto the proxy time series. The differing
assumptions made by BARCAST and RegEM are detailed, and the impacts
of these differences on the analysis are discussed. Key distinctions
between BARCAST and RegEM include their treatment of spatial and
temporal covariance, the prior information that enters into each
analysis, the quantities they seek to impute, the end product of
each analysis, the temporal variance of the reconstructed field,
and the treatment of uncertainty in both the imputed values and functions
of these imputations. Differences between BARCAST and RegEM are illustrated
by applying the two approaches to various surrogate datasets. If
the assumptions inherent to BARCAST are not strongly violated, then
in scenarios comparable to practical applications BARCAST results
in reconstructions of both the field and the spatial mean that are
more skillful than those produced by RegEM, as measured by the coefficient
of efficiency. In addition, the uncertainty intervals produced by
BARCAST are narrower than those estimated using RegEM and contain
the true values with higher probability. Abstract Part I presented
a Bayesian algorithm for reconstructing climate anomalies in space
and time (BARCAST). This method involves specifying simple parametric
forms for the spatial covariance and temporal evolution of the climate
field as well as ?observation equations? describing the relationships
between the data types and the corresponding true values of the climate
field. As this Bayesian approach to reconstructing climate fields
is new and different, it is worthwhile to compare it in detail to
the more established regularized expectation?maximization (RegEM)
algorithm, which is based on an empirical estimate of the joint data
covariance matrix and a multivariate regression of the instrumental
time series onto the proxy time series. The differing assumptions
made by BARCAST and RegEM are detailed, and the impacts of these
differences on the analysis are discussed. Key distinctions between
BARCAST and RegEM include their treatment of spatial and temporal
covariance, the prior information that enters into each analysis,
the quantities they seek to impute, the end product of each analysis,
the temporal variance of the reconstructed field, and the treatment
of uncertainty in both the imputed values and functions of these
imputations. Differences between BARCAST and RegEM are illustrated
by applying the two approaches to various surrogate datasets. If
the assumptions inherent to BARCAST are not strongly violated, then
in scenarios comparable to practical applications BARCAST results
in reconstructions of both the field and the spatial mean that are
more skillful than those produced by RegEM, as measured by the coefficient
of efficiency. In addition, the uncertainty intervals produced by
BARCAST are narrower than those estimated using RegEM and contain
the true values with higher probability.},
author = {Tingley, Martin P and Huybers, Peter},
doi = {10.1175/2009JCLI3016.1},
journal = {J. Climate},
keywords = {*file-import-12-04-11},
number = {10},
pages = {2782--2800},
publisher = {American Meteorological Society},
title = {{A Bayesian Algorithm for Reconstructing Climate Anomalies in Space and Time. Part II: Comparison with the Regularized Expectation–Maximization Algorithm}},
url = {http://dx.doi.org/10.1175/2009JCLI3016.1},
volume = {23},
year = {2010}
}
@article{tingley1,
abstract = {Abstract Reconstructing the spatial pattern of a climate field through
time from a dataset of overlapping instrumental and climate proxy
time series is a nontrivial statistical problem. The need to transform
the proxy observations into estimates of the climate field, and the
fact that the observed time series are not uniformly distributed
in space, further complicate the analysis. Current leading approaches
to this problem are based on estimating the full covariance matrix
between the proxy time series and instrumental time series over a
?calibration? interval and then using this covariance matrix in the
context of a linear regression to predict the missing instrumental
values from the proxy observations for years prior to instrumental
coverage. A fundamentally different approach to this problem is formulated
by specifying parametric forms for the spatial covariance and temporal
evolution of the climate field, as well as ?observation equations?
describing the relationship between the data types and the corresponding
true values of the climate field. A hierarchical Bayesian model is
used to assimilate both proxy and instrumental datasets and to estimate
the probability distribution of all model parameters and the climate
field through time on a regular spatial grid. The output from this
approach includes an estimate of the full covariance structure of
the climate field and model parameters as well as diagnostics that
estimate the utility of the different proxy time series. This methodology
is demonstrated using an instrumental surface temperature dataset
after corrupting a number of the time series to mimic proxy observations.
The results are compared to those achieved using the regularized
expectation?maximization algorithm, and in these experiments the
Bayesian algorithm produces reconstructions with greater skill. The
assumptions underlying these two methodologies and the results of
applying each to simple surrogate datasets are explored in greater
detail in Part II. Abstract Reconstructing the spatial pattern of
a climate field through time from a dataset of overlapping instrumental
and climate proxy time series is a nontrivial statistical problem.
The need to transform the proxy observations into estimates of the
climate field, and the fact that the observed time series are not
uniformly distributed in space, further complicate the analysis.
Current leading approaches to this problem are based on estimating
the full covariance matrix between the proxy time series and instrumental
time series over a ?calibration? interval and then using this covariance
matrix in the context of a linear regression to predict the missing
instrumental values from the proxy observations for years prior to
instrumental coverage. A fundamentally different approach to this
problem is formulated by specifying parametric forms for the spatial
covariance and temporal evolution of the climate field, as well as
?observation equations? describing the relationship between the data
types and the corresponding true values of the climate field. A hierarchical
Bayesian model is used to assimilate both proxy and instrumental
datasets and to estimate the probability distribution of all model
parameters and the climate field through time on a regular spatial
grid. The output from this approach includes an estimate of the full
covariance structure of the climate field and model parameters as
well as diagnostics that estimate the utility of the different proxy
time series. This methodology is demonstrated using an instrumental
surface temperature dataset after corrupting a number of the time
series to mimic proxy observations. The results are compared to those
achieved using the regularized expectation?maximization algorithm,
and in these experiments the Bayesian algorithm produces reconstructions
with greater skill. The assumptions underlying these two methodologies
and the results of applying each to simple surrogate datasets are
explored in greater detail in Part II.},
author = {Tingley, Martin P and Huybers, Peter},
doi = {10.1175/2009JCLI3015.1},
journal = {J. Climate},
keywords = {*file-import-12-04-11},
number = {10},
pages = {2759--2781},
publisher = {American Meteorological Society},
title = {{A Bayesian Algorithm for Reconstructing Climate Anomalies in Space and Time. Part I: Development and Applications to Paleoclimate Reconstruction Problems}},
url = {http://dx.doi.org/10.1175/2009JCLI3015.1},
volume = {23},
year = {2010}
}
@article{Toohey2016,
abstract = {The Easy Volcanic Aerosol (EVA) forcing generator produces stratospheric aerosol optical properties as a function of time, latitude, height and wavelength for a given input list of volcanic eruption attributes. EVA is based on a parameterized three-box model of stratospheric transport, and simple scaling relationships used to derive mid-visible (550 nm) aerosol optical depth and aerosol effective radius from stratospheric sulfate mass. Pre-calculated look up tables computed from Mie theory are used to produce wavelength dependent aerosol extinction, single scattering albedo and scattering asymmetry factor values. The structural form of EVA, and the tuning of its parameters, are chosen to produce best agreement with the satellite-based reconstruction of stratospheric aerosol properties following the 1991 Pinatubo eruption, and with prior millennial-time scale forcing reconstructions including the 1815 eruption of Tambora. EVA can be used to produce volcanic forcing for climate models which is based on recent observations and physical understanding, but internally self-consistent over any time-scale of choice. In addition, EVA is constructed so as to allow for easy modification of different aspects of aerosol properties, in order to be used in model experiments to help advance understanding of what aspects of the volcanic aerosol are important for the climate system.},
author = {Toohey, Matthew and Stevens, Bjorn and Schmidt, Hauke and Timmreck, Claudia},
doi = {10.5194/gmd-9-4049-2016},
issn = {19919603},
journal = {Geoscientific Model Development},
number = {11},
pages = {4049--4070},
title = {{Easy Volcanic Aerosol (EVA v1.0): An idealized forcing generator for climate simulations}},
volume = {9},
year = {2016}
}
@article{Vieira2011,
abstract = {Aims. We present a physically consistent reconstruction of the total solar irradiance for the Holocene. Methods. We extend the SATIRE models to estimate the evolution of the total (and partly spectral) solar irradiance over the Holocene. The basic assumption is that the variations of the solar irradiance are due to the evolution of the dark and bright magnetic features on the solar surface. The evolution of the decadally averaged magnetic flux is computed from decadal values of cosmogenic isotope concentrations recorded in natural archives employing a series of physics-based models connecting the processes from the modulation of the cosmic ray flux in the heliosphere to their record in natural archives. We then compute the total solar irradiance (TSI) as a linear combination of the jth and jth + 1 decadal values of the open magnetic flux. Results. Reconstructions of the TSI over the Holocene, each valid for a di{\_}erent paleomagnetic time series, are presented. Our analysis suggests that major sources of uncertainty in the TSI in this model are the heritage of the uncertainty of the TSI since 1610 reconstructed from sunspot data and the uncertainty of the evolution of the Earth's magnetic dipole moment. The analysis of the distribution functions of the reconstructed irradiance for the last 3000 years indicates that the estimates based on the virtual axial dipole moment are significantly lower at earlier times than the reconstructions based on the virtual dipole moment. Conclusions. We present the first physics-based reconstruction of the total solar irradiance over the Holocene, which will be of interest for studies of climate change over the last 11500 years. The reconstruction indicates that the decadally averaged total solar irradiance ranges over approximately 1.5 W/m2 from grand maxima to grand minima.},
archivePrefix = {arXiv},
arxivId = {1103.4958},
author = {Vieira, Luis Eduardo A. and Solanki, Sami K. and Krivova, Natalie A. and Usoskin, Ilya},
doi = {10.1051/0004-6361/201015843},
eprint = {1103.4958},
isbn = {doi:10.1051/0004-6361/201015843},
issn = {0004-6361},
journal = {Astronomy and Astrophysics},
keywords = {activity,faculae,plages,solar-terrestrial relations,sun,sunspots,surface magnetism,uv radiation},
number = {A6},
pages = {1--20},
title = {{Evolution of the solar irradiance during the Holocene}},
url = {http://arxiv.org/abs/1103.4958},
volume = {531},
year = {2011}
}
@article{Wahl2012,
author = {Wahl, Eugene R. and Smerdon, Jason E.},
doi = {10.1029/2012GL051086},
issn = {00948276},
journal = {Geophysical Research Letters},
month = {mar},
number = {6},
pages = {L06703},
title = {{Comparative performance of paleoclimate field and index reconstructions derived from climate proxies and noise-only predictors}},
url = {http://doi.wiley.com/10.1029/2012GL051086},
volume = {39},
year = {2012}
}
@article{Weisberg2002,
abstract = {Regression is the study of the dependence of a response variable tors collected in . In dimension reduction regression, we seek to find a few linear combinations ??? ? ??? ? ??? ? ? ? on a collection ? predic- , such that all the information about the regression is contained in these linear com- binations. If is very small, perhaps one or two, then the regression problem can be summarized using simple graphics; for example, for , the plot of versus information. When , a 3D plot contains all the information. ? ? ? ? ? contains all the regression Several methods for estimating and relevant functions of "! ? ? ? ? have been suggested in the literature. In this paper, we describe an R package for three important dimension reduction methods: sliced inverse regression or sir, sliced average variance estimates, or save, and principal Hessian directions, or phd. The package is very general and flexible, and can be easily extended to include other methods of dimension reduction. It includes tests and estimates of the dimension , estimates of the relevant information including ? ? ? ? ? , and some useful graphical summaries as well},
author = {Weisberg, Sanford},
doi = {http://dx.doi.org/10.18637/jss.v007.i01},
file = {:home/luis/Dropbox/Biblioteca/articulos/Weisberg - 2002 - Dimension reduction regression in R.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1998},
pages = {1--22},
title = {{Dimension reduction regression in R}},
volume = {7},
year = {2002}
}
@article{werner2012pseudoproxy,
author = {Werner, J P and Luterbacher, J and Smerdon, J E},
journal = {Journal of Climate},
number = {3},
pages = {851--867},
title = {{A Pseudoproxy Evaluation of Bayesian Hierarchical Modelling and Canonical Correlation Analysis for Climate Field Reconstructions over Europe}},
volume = {26},
year = {2013}
}
@article{Zhong2005,
abstract = {Identification of transcription factor binding motifs ({\{}TFBMs{\}}) is$\backslash$na crucial first step towards the understanding of regulatory circuitries$\backslash$ncontrolling the expression of genes. In this paper, we propose a$\backslash$nnovel procedure called regularized sliced inverse regression ({\{}RSIR{\}})$\backslash$nfor identifying {\{}TFBMs{\}}. {\{}RSIR{\}} follows a recent trend to combine$\backslash$ninformation contained in both gene expression measurements and genes'$\backslash$npromoter sequences. Compared with existing methods, {\{}RSIR{\}} is efficient$\backslash$nin computation, very stable for data with high dimensionality and$\backslash$nhigh collinearity, and improves motif detection sensitivities and$\backslash$nspecificities by avoiding inappropriate model specification.},
author = {Zhong, Wenxuan and Zeng, Peng and Ma, Ping and Liu, Jun S. and Zhu, Yu},
doi = {10.1093/bioinformatics/bti680},
file = {:home/luis/Dropbox/Biblioteca/articulos/Zhong et al. - 2005 - RSIR Regularized sliced inverse regression for motif discovery.pdf:pdf},
isbn = {1367-4803 (Print)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {22},
pages = {4169--4175},
pmid = {16166098},
title = {{RSIR: Regularized sliced inverse regression for motif discovery}},
volume = {21},
year = {2005}
}
