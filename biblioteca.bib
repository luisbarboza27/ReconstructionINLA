@article{Bair2006,
abstract = {In regression problems where the number of predictors greatly exceeds the number of observations, conventional regression techniques may produce unsatisfactory results. We describe a technique called supervised principal components that can be applied to this type of problem. Supervised principal components is similar to conventional principal components analysis except that it uses a subset of the predictors selected based on their association with the outcome. Supervised principal components can be applied to regression and generalized regression problems, such as survival analysis. It compares favorably to other techniques for this type of problem, and can also account for the effects of other covariates and help identify which predictor variables are most important. We also provide asymptotic consistency results to help support our empirical findings. These methods could become important tools for DNA microarray data, where they may be used to more accurately diagnose and treat cancer.},
author = {Bair, Eric and Hastie, Trevor and Paul, Debashis and Tibshirani, Robert},
doi = {10.1198/016214505000000628},
file = {:home/luis/Dropbox/Biblioteca/articulos/Bair et al. - 2006 - Prediction by Supervised Principal Components.pdf:pdf;:home/luis/Dropbox/Biblioteca/articulos/Bair et al. - 2006 - Prediction by Supervised Principal Components.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {gene expression,microarray,regression,survival analysis},
number = {473},
pages = {119--137},
title = {{Prediction by Supervised Principal Components}},
volume = {101},
year = {2006}
}
@article{Barboza2014,
author = {Barboza, Luis and Li, Bo and Tingley, Martin P. and Viens, Frederi G.},
doi = {???????},
file = {:home/luis/Dropbox/Biblioteca/articulos/Barboza et al. - 2014 - Reconstructing Past Temperatures from Natural Proxies and Estimated Climate Forcings using Short- and Long-Memor.pdf:pdf},
journal = {The Annals of Applied Statistics},
number = {4},
pages = {1966--2001},
title = {{Reconstructing Past Temperatures from Natural Proxies and Estimated Climate Forcings using Short- and Long-Memory Models}},
volume = {8},
year = {2014}
}
@article{Blangiardo2013,
abstract = {During the last three decades, Bayesian methods have developed greatly in the field of epidemiology. Their main challenge focusses around computation, but the advent of Markov Chain Monte Carlo methods (MCMC) and in particular of the WinBUGS software has opened the doors of Bayesian modelling to the wide research community. However model complexity and database dimension still remain a constraint.Recently the use of Gaussian random fields has become increasingly popular in epidemiology as very often epidemiological data are characterised by a spatial and/or temporal structure which needs to be taken into account in the inferential process. The Integrated Nested Laplace Approximation (INLA) approach has been developed as a computationally efficient alternative to MCMC and the availability of an R package (R-INLA) allows researchers to easily apply this method.In this paper we review the INLA approach and present some applications on spatial and spatio-temporal data. ?? 2012 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Blangiardo, Marta and Cameletti, Michela and Baio, Gianluca and Rue, H{\aa}vard},
doi = {10.1016/j.sste.2013.07.003},
eprint = {arXiv:1011.1669v3},
file = {:home/luis/Dropbox/Biblioteca/articulos/Blangiardo et al. - 2013 - Spatial and spatio-temporal models with R-INLA.pdf:pdf},
isbn = {1877-5845},
issn = {18775845},
journal = {Spatial and Spatio-temporal Epidemiology},
keywords = {Area-level data,Bayesian approach,Integrated Nested Laplace Approximation,Point-level data,Stochastic Partial Differential Equation approach,integrated nested laplace approximation,stochastic partial differential equation},
pages = {39--55},
pmid = {24377114},
publisher = {Elsevier Ltd},
title = {{Spatial and spatio-temporal models with R-INLA}},
url = {http://dx.doi.org/10.1016/j.sste.2013.07.003},
volume = {7},
year = {2013}
}
@article{Chun2010,
abstract = {Partial least squares regression has been an alternative to ordinary least squares for handling multicollinearity in several areas of scientific research since the 1960s. It has recently gained much attention in the analysis of high dimensional genomic data. We show that known asymptotic consistency of the partial least squares estimator for a univariate response does not hold with the very large p and small n paradigm. We derive a similar result for a multivariate response regression with partial least squares. We then propose a sparse partial least squares formulation which aims simultaneously to achieve good predictive performance and variable selection by producing sparse linear combinations of the original predictors. We provide an efficient implementation of sparse partial least squares regression and compare it with well-known variable selection and dimension reduction approaches via simulation experiments. We illustrate the practical utility of sparse partial least squares regression in a joint analysis of gene expression and genomewide binding data.},
author = {Chun, Hyonho and Keleş, S{\"{u}}nd{\"{u}}z},
doi = {10.1111/j.1467-9868.2009.00723.x},
file = {:home/luis/Dropbox/Biblioteca/articulos/Chun, Keleş - 2010 - Sparse partial least squares regression for simultaneous dimension reduction and variable selection.pdf:pdf},
isbn = {1369-7412 (Print)$\backslash$r1369-7412 (Linking)},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Chromatin immuno-precipitation,Dimension reduction,Gene expression,Lasso,Microarrays,Partial least squares,Sparsity,Variable and feature selection},
number = {1},
pages = {3--25},
pmid = {20107611},
title = {{Sparse partial least squares regression for simultaneous dimension reduction and variable selection}},
volume = {72},
year = {2010}
}
@misc{Chung2013,
author = {Chung, Dongjun and Chun, Hyonho and Keles, Sunduz},
title = {{spls: Sparse Partial Least Squares (SPLS) Regression and Classification}},
url = {https://cran.r-project.org/package=spls},
year = {2013}
}
@article{Cook2004,
abstract = {We develop tests of the hypothesis of no effect for selected predictors in regression, without assuming a model for the conditional distribution of the response given the predictors. Predictor effects need not be limited to the mean function and smoothing is not required. The general approach is based on sufficient dimension reduction, the idea being to replace the predictor vector with a lower-dimensional version without loss of information on the regression. Methodology using sliced inverse regression is developed in detail.},
archivePrefix = {arXiv},
arxivId = {math/0406520},
author = {Cook, R. Dennis},
doi = {10.1214/009053604000000292},
eprint = {0406520},
file = {:home/luis/Dropbox/Biblioteca/articulos/Cook - 2004 - Testing predictor contributions in sufficient dimension reduction.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Central subspace,Nonparametric regression,Sliced inverse regression},
number = {3},
pages = {1062--1092},
primaryClass = {math},
title = {{Testing predictor contributions in sufficient dimension reduction}},
volume = {32},
year = {2004}
}
@article{Coudret2014,
abstract = {Among methods to analyze high-dimensional data, the sliced inverse regression (SIR) is of particular interest for non-linear relations between the dependent variable and some indices of the covariate. When the dimension of the covariate is greater than the number of observations, classical versions of SIR cannot be applied. Various upgrades were then proposed to tackle this issue such as regularized SIR (RSIR) and sparse ridge SIR (SR-SIR), to estimate the parameters of the underlying model and to select variables of interest. In this paper, we introduce two new estimation methods respectively based on the QZ algorithm and on the Moore-Penrose pseudo-inverse.We also describe a new selection procedure of the most relevant components of the covariate that relies on a proximity criterion between submodels and the initial one. These approaches are compared with RSIR and SR-SIR in a simulation study. Finally we applied SIR-QZ and the associated selection procedure to a genetic dataset in order to find markers that are linked to the expression of a gene. These markers are called expression quantitative trait loci (eQTL).},
author = {Coudret, R. and Liquet, B. and Saracco, J.},
file = {:home/luis/Dropbox/Biblioteca/articulos/Coudret, Liquet, Saracco - 2014 - Comparison of sliced inverse regression aproaches for undetermined cases.pdf:pdf},
journal = {Journal de la Soci{\'{e}}t{\'{e}} Fran{\c{c}}aise de Statistique},
keywords = {High-dimensional data,dimension reduction,semiparametric regression,sparsity},
number = {2},
pages = {72--96},
title = {{Comparison of sliced inverse regression aproaches for undetermined cases}},
url = {http://journal-sfds.fr/index.php/J-SFdS/article/view/278},
volume = {155},
year = {2014}
}
@misc{Coudret2017,
author = {Coudret, Rapha{\"{e}}l and Liquet, Beno{\^{i}}t and Saracco, J{\'{e}}r{\^{o}}me},
title = {{edrGraphicalTools: Provides Tools for Dimension Reduction Methods}},
url = {https://cran.r-project.org/package=edrGraphicalTools},
year = {2017}
}
@book{DeBoor2001,
author = {de Boor, Carl},
publisher = {Springer},
series = {Applied Mathematical Sciences},
title = {{A Practical Guide to Splines}},
year = {2001}
}
@article{Duan1991,
author = {Duan, Naihua and Li, Ker-Chau},
file = {:home/luis/Dropbox/Biblioteca/articulos/Duan, Li - 1991 - Slicing Regression A Link-Free Regression Method.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {505--530},
title = {{Slicing Regression : A Link-Free Regression Method}},
volume = {19},
year = {1991}
}
@inproceedings{Emile-Geay2015,
address = {Boulder, Colorado, USA},
author = {Emile-Geay, Julien and McKay, Nicholas P. and Wang, Jianghao and Kaufman, Darrell and {PAGES2K Consortium}},
booktitle = {5th International Workshop on Climate Informatics.},
file = {:home/luis/Dropbox/Biblioteca/articulos/Emile-Geay et al. - 2015 - A Semantic Database of Temperature Proxies covering the Common Era.pdf:pdf},
title = {{A Semantic Database of Temperature Proxies covering the Common Era}},
year = {2015}
}
@article{Friedman2010,
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
file = {:home/luis/Dropbox/Biblioteca/articulos/Friedman, Hastie, Tibshirani - 2010 - Regularization Paths for Generalized Linear Models via Coordinate Descent.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {fmri,functional connectivity,spatial model},
number = {1},
title = {{Regularization Paths for Generalized Linear Models via Coordinate Descent}},
volume = {33},
year = {2010}
}
@article{Gneiting2007a,
abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the ...},
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
file = {:home/luis/Dropbox/Biblioteca/articulos/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Bregman divergence,Brier score,Coherent,Continuous ranked probability score,Cross-validation,Entropy,Kernel score,Loss function,Minimum contrast estimation,Negative definite function,Prediction interval,Predictive distribution,Quantile forecast,Scoring rule,Skill score,Strictly proper,Utility function},
language = {en},
month = {mar},
number = {477},
pages = {359--378},
publisher = {Taylor {\&} Francis},
title = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000001437{\#}.UfauueGm1As},
volume = {102},
year = {2007}
}
@article{Jolliffe1982,
author = {Jolliffe, Ian T.},
file = {:home/luis/Dropbox/Biblioteca/articulos/Jolliffe - 1982 - A Note on the Use of Principal Components in Regression.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {3},
pages = {300--303},
title = {{A Note on the Use of Principal Components in Regression}},
volume = {31},
year = {1982}
}
@article{Kaufman2014,
abstract = {Proxy-based reconstructions of past climate provide insights into externally forced and intrinsic variability over regional to global scales and can be used to place recent trends in a long-term context. Comparisons between these reconstructions and the output of Earth system models provide evaluation opportunities to improve our understanding of climate forcings on time scales that are not adequately represented by the instrumental record. They also provide a heuristic tool to explore mechanisms of climate variability, with implications for future climate projections [Schmidt et al., 2014].},
author = {Kaufman, Darrell},
doi = {10.1002/2014EO400001},
file = {:home/luis/Dropbox/Biblioteca/articulos/Kaufman - 2014 - A Community-Driven Framework for Climate Reconstructions.pdf:pdf},
issn = {2324-9250},
journal = {Eos, Transactions American Geophysical Union},
keywords = {1616 Climate variability,1626 Global climate models,1637 Regional climate change,Climate reconstructions},
number = {40},
pages = {361--362},
title = {{A Community-Driven Framework for Climate Reconstructions}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/2014EO400001/abstract},
volume = {95},
year = {2014}
}
@article{Li1991,
abstract = {Modem advances in computing power have greatly widened scientists' scope in gathering which might have been ignored in the past. Yet to effectively many variables, information an easy task, although our ability to interact this article, we propose a novel data-analytic x without going through any parametric properties vectors. This model looks like a nonlinear unknown. For effectively space] generated regression coefficients. for e.d.r. directions. Furthermore, model-fitting inverse view of regression; that is, instead of regressing regression, the univariate and investigating with data has been much enhanced by recent innovations tool, sliced inverse regression or nonparametric (SIR), for reducing x against y. Forward regression and inverse regression are connected by a theorem that motivates of SIR are investigated by the 3k'S. This makes our goal different In fact, the fk'S themselves covariance, the inverse regression curve, E(x I y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix shows that under a suitable condition, if the distribution for the estimated inverse regression smoothing is needed. SIR can be easily implemented effectively curve can be conducted to locate its main orientation, yielding the dimension information process. This method explores the simplicity output variable y against the multivariate that the functional reducing the dimension, we need only to estimate the space [effective are not identifiable of x has been standardized from scan a large pool of variables is not in dynamic graphics. In of the input variable of the x, we regress under a model of the form, y = f(lx, ..., 8Kx, e), where the 3ks are the unknown row except for the crucial difference from the usual one in regression analysis, the estimation without a specific structural this method. The theoretical form off is completely dimension reduction (e.d.r.) of all the form onf. Our main theorem we use a simple step function to estimate the inverse regression curve. No complicated on personal computers. By simulation, we demonstrate plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin- is proposed to address the issue of whether or not a direction KEY WORDS: Dynamic graphics; Principal component analysis; Projection pursuit. 1. INTRODUCTION lationship variable x, a p-dimensional parametric model is parsimonious, as the maximum likelihood Regression analysis is a popular way of studying between a response variable y and its explanatory column vector. Quite often, a the re- model is used to guide the analysis. When the such have standard estimation proved to be successful in gathering techniques or the least squares method information data. In most applications, however, any parametric at best an approximation alternatives that offer from the model is an adequate model is not easy. When there are no persua- sive models available, nonparametric emerge as promising to the true one, and the search for regression regression only the continuity function. on the presence point of interest information. is the idea of local smoothing, or differentiability of sufficiently * Ker-Chau Li is Professor, ematics, University For one-dimensional property ibility in modeling. A common theme of nonparametric regression the needed flex- which explores The success of local smoothing many data points around each in the design space to provide adequate problems, many smooth- of California, Los Angeles, CA 90024. This research the 3's as an effective Division of Statistics, Department of Math- was supported in part by the National Science Foundation under grants DMS86-02018 and DMS89-02494. It has been a long time since I intro- duced SIR in talks given at Berkeley, Bell Labs, and Rutgers in 1985. I received many useful questions and suggestions from these audiences. I am indebted to Naihua Duan for stimulating leading eventually to Duan and Li (1991). Peter Bickel brought semi- parametric literature to my attention. edge in areas of dimension reduction and multivariate Dennis Cook, who introduced Section 6.3 would have been further regression" this article. Three referees for improving the presentation. David Brillinger, delayed. As a replacement for "slice gested to me by Don Ylvisaker, who also helped me clear hurdles in publishing suggestions whose work has inspired me so much. 316 or "slicing regression," used earlier, the name SIR was sug- nice and an associate editor offered Finally, I would like to thank Jan de Leeuw broadened my knowl- XLISP-STAT to me, the appearance of techniques found by SIR is spurious.},
author = {Li, Ker-Chau},
doi = {10.2307/2290568},
file = {:home/luis/Dropbox/Biblioteca/articulos/Li - 1991 - Sliced Inverse Regression for Dimension Reduction.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {dynamic graphics,principal component analysis,projection pursuit},
number = {414},
pages = {316--327},
title = {{Sliced Inverse Regression for Dimension Reduction}},
volume = {86},
year = {1991}
}
@article{Li2008,
abstract = {In high-dimensional data analysis, sliced inverse regression (SIR) has proven to be an effective dimension reduction tool and has enjoyed wide applications. The usual SIR, however, cannot work with problems where the number of predictors, p, exceeds the sample size, n, and can suffer when there is high collinearity among the predictors. In addition, the reduced dimensional space consists of linear combinations of all the original predictors and no variable selection is achieved. In this article, we propose a regularized SIR approach based on the least-squares formulation of SIR. The L2 regularization is introduced, and an alternating least-squares algorithm is developed, to enable SIR to work with n {\textless} p and highly correlated predictors. The L1 regularization is further introduced to achieve simultaneous reduction estimation and predictor selection. Both simulations and the analysis of a microarray expression data set demonstrate the usefulness of the proposed method.},
author = {Li, Lexin and Yin, Xiangrong},
doi = {10.1111/j.1541-0420.2007.00836.x},
file = {:home/luis/Dropbox/Biblioteca/articulos/Li, Yin - 2008 - Sliced inverse regression with regularizations.pdf:pdf},
issn = {0006341X},
journal = {Biometrics},
keywords = {Regularized least squares,Sliced inverse regression,Sufficient dimension reduction},
number = {1},
pages = {124--131},
pmid = {17651455},
title = {{Sliced inverse regression with regularizations}},
volume = {64},
year = {2008}
}
@article{Liquet2012,
abstract = {Sliced inverse regression (SIR) and related methods were introduced in order to reduce the dimensionality of regression problems. In general semiparametric regression framework, these methods determine linear combinations of a set of explanatory variables X related to the response variable Y, without losing information on the conditional distribution of Y given X. They are based on a "slicing step" in the population and sample versions. They are sensitive to the choice of the number H of slices, and this is particularly true for SIR-II and SAVE methods. At the moment there are no theoretical results nor practical techniques which allows the user to choose an appropriate number of slices. In this paper, we propose an approach based on the quality of the estimation of the effective dimension reduction (EDR) space: the square trace correlation between the true EDR space and its estimate can be used as goodness of estimation. We introduce a na {\textless} ve bootstrap estimation of the square trace correlation criterion to allow selection of an "optimal" number of slices. Moreover, this criterion can also simultaneously select the corresponding suitable dimension K (number of the linear combination of X). From a practical point of view, the choice of these two parameters H and K is essential. We propose a 3D-graphical tool, implemented in R, which can be useful to select the suitable couple (H, K). An R package named "edrGraphicalTools" has been developed. In this article, we focus on the SIR-I, SIR-II and SAVE methods. Moreover the proposed criterion can be use to determine which method seems to be efficient to recover the EDR space, that is the structure between Y and X. We indicate how the proposed criterion can be used in practice. A simulation study is performed to illustrate the behavior of this approach and the need for selecting properly the number H of slices and the dimension K. A short real-data example is also provided.},
author = {Liquet, Beno{\^{i}}t and Saracco, J{\'{e}}r{\^{o}}me},
doi = {10.1007/s00180-011-0241-9},
file = {:home/luis/Dropbox/Biblioteca/articulos/Liquet, Saracco - 2012 - A graphical tool for selecting the number of slices and the dimension of the model in SIR and SAVE approaches.pdf:pdf},
issn = {09434062},
journal = {Computational Statistics},
keywords = {Bootstrap,Dimension reduction,Sliced average variance estimation (SAVE),Sliced inverse regression (SIR),Square trace correlation},
number = {1},
pages = {103--125},
title = {{A graphical tool for selecting the number of slices and the dimension of the model in SIR and SAVE approaches}},
volume = {27},
year = {2012}
}
@article{Martins2013,
abstract = {The INLA approach for approximate Bayesian inference for latent Gaussian models has been shown to give fast and accurate estimates of posterior marginals and also to be a valuable tool in practice via the R-package R-INLA. New developments in the R-INLA are formalized and it is shown how these features greatly extend the scope of models that can be analyzed by this interface. The current default method in R-INLA to approximate the posterior marginals of the hyperparameters using only a modest number of evaluations of the joint posterior distribution of the hyperparameters, without any need for numerical integration, is discussed. ?? 2013 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0333v1},
author = {Martins, Thiago G. and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
doi = {10.1016/j.csda.2013.04.014},
eprint = {arXiv:1210.0333v1},
file = {:home/luis/Dropbox/Biblioteca/articulos/Martins et al. - 2013 - Bayesian computing with INLA New features.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximate,Bayesian inference,INLA,Latent Gaussian models},
pages = {68--83},
publisher = {Elsevier B.V.},
title = {{Bayesian computing with INLA: New features}},
url = {http://dx.doi.org/10.1016/j.csda.2013.04.014},
volume = {67},
year = {2013}
}
@article{Meinshausen2016,
abstract = {Atmospheric greenhouse gas concentrations are at unprecedented, record-high levels compared to pre-industrial reconstructions over the last 800,000 years. Those elevated greenhouse gas concentrations warm the planet and together with net cooling effects by aerosols, they are the reason of observed climate change over the past 150 years. An accurate representation of those concentrations is hence important to understand and model recent and future climate change. So far, community efforts to create composite datasets with seasonal and latitudinal information have focused on marine boundary layer conditions and recent trends since 1980s. Here, we provide consolidated data sets of historical atmospheric (volume) mixing ratios of 43 greenhouse gases specifically for the purpose of climate model runs. The presented datasets are based on AGAGE and NOAA networks and a large set of literature studies. In contrast to previous intercomparisons, the new datasets are latitudinally resolved, and include seasonality over the period between year 0 to 2014. We assimilate data for CO2, methane (CH4) and nitrous oxide (N2O), 5 chlorofluorocarbons (CFCs), 3 hydrochlorofluorocarbons (HCFCs), 16 hydrofluorocarbons (HFCs), 3 halons, methyl bromide (CH3Br), 3 perfluorocarbons (PFCs), sulfur hexafluoride (SF6), nitrogen triflouride (NF3) and sulfuryl fluoride (SO2F2). We estimate 1850 annual and global mean surface mixing ratios of CO2 at 284.3 ppmv, CH4 at 808.2 ppbv and N2O at 273.0 ppbv and quantify the seasonal and hemispheric gradients of surface mixing ratios. Compared to earlier intercomparisons, the stronger implied radiative forcing in the northern hemisphere winter (due to the latitudinal gradient and seasonality) may help to improve the skill of climate models to reproduce past climate and thereby reduce uncertainty in future projections.},
author = {Meinshausen, Malte and Vogel, Elisabeth and Nauels, Alexander and Lorbacher, Katja and Meinshausen, Nicolai and Etheridge, David and Fraser, Paul and Montzka, Stephen A. and Rayner, Peter and Trudinger, Cathy and Krummel, Paul and Beyerle, Urs and Cannadell, Josep G. and Daniel, John S. and Enting, Ian and Law, Rachel M. and O'Doherty, Simon and Prinn, Ron G. and Reimann, Stefan and Rubino, Mauro and Velders, Guus J. M. and Vollmer, Martin K. and Weiss, Ray},
doi = {10.5194/gmd-2016-169},
file = {:home/luis/Dropbox/Biblioteca/articulos/Meinshausen et al. - 2016 - Historical greenhouse gas concentrations.pdf:pdf},
issn = {1991-962X},
journal = {Geoscientific Model Development Discussions},
number = {August},
pages = {1--122},
title = {{Historical greenhouse gas concentrations}},
url = {http://www.geosci-model-dev-discuss.net/gmd-2016-169/},
volume = {1},
year = {2016}
}
@article{Morice2012,
abstract = {Recent developments in observational near-surface air temperature and sea-surface temperature analyses are combined to produce HadCRUT4, a new data set of global and regional temperature evolution from 1850 to the present. This includes the addition of newly digitised measurement data, both over land and sea, new sea-surface temperature bias adjustments and a more comprehensive error model for describing uncertainties in sea-surface temperature measurements. An ensemble approach has been adopted to better describe complex temporal and spatial interdependencies of measurement and bias uncertainties and to allow these correlated uncertainties to be taken into account in studies that are based upon HadCRUT4. Climate diagnostics computed from the gridded data set broadly agree with those of other global near-surface temperature analyses. Fitted linear trends in temperature anomalies are approximately 0.07 degC/decade from 1901 to 2010 and 0.17 degC/decade from 1979 to 2010 globally. Northern/southern hemispheric trends are 0.08/0.07 degC/decade over 1901 to 2010 and 0.24/0.10 degC/decade over 1979 to 2010. Linear trends in other prominent near-surface temperature analyses agree well with the range of trends computed from the HadCRUT4 ensemble members.},
author = {Morice, Colin P. and Kennedy, John J. and Rayner, Nick A. and Jones, Phil D.},
doi = {10.1029/2011JD017187},
file = {:home/luis/Dropbox/Biblioteca/articulos/Morice et al. - 2012 - Quantifying uncertainties in global and regional temperature change using an ensemble of observational estimates.pdf:pdf},
isbn = {2156-2202},
issn = {01480227},
journal = {Journal of Geophysical Research Atmospheres},
keywords = {climate,historical temperature,near surface temperature,observational ensemble},
number = {8},
pages = {1--22},
title = {{Quantifying uncertainties in global and regional temperature change using an ensemble of observational estimates: The HadCRUT4 data set}},
volume = {117},
year = {2012}
}
@article{Muff2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.3065v2},
author = {Muff, Stefanie and Riebler, Andrea and Held, Leonhard and Rue, H{\aa}vard and Saner, Philippe},
doi = {10.1111/rssc.12069},
eprint = {arXiv:1302.3065v2},
file = {:home/luis/Dropbox/Biblioteca/articulos/Muff et al. - 2015 - Bayesian analysis of measurement error models using integrated nested Laplace approximations.pdf:pdf},
issn = {00359254},
journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
keywords = {approximation,bayesian analysis,berkson error,classical error,integrated nested laplace,measurement error},
number = {2},
pages = {231--252},
title = {{Bayesian analysis of measurement error models using integrated nested Laplace approximations}},
volume = {64},
year = {2015}
}
@article{PAGES2kConsortium2017,
abstract = {Reproducible climate reconstructions of the Common Era are key to placing twentieth century warming in the context of natural variability. Here we present a community-sourced database of temperature-sensitive proxy records. The database gathers 661 records from 628 locations around the globe, including all continental regions and major ocean basis, with temporal resolution varying from monthly to centennial. The rela-tionship to a gridded temperature dataset (HadCRUT4.2) is analyzed at seasonal and annual scales, and a preliminary reconstruction of globally-averaged annual-mean temperature is presented. The database is shared in the Linked Paleo Data (LiPD) format, including serializations in the Matlab, R and Python languages.},
author = {{PAGES 2k Consortium}},
doi = {DOI: 10.1038/sdata.2017.88},
file = {:home/luis/Dropbox/Biblioteca/articulos/PAGES 2k Consortium - 2017 - A global multiproxy database for temperature reconstructions of the Common Era.pdf:pdf},
issn = {2052-4463},
journal = {Scientific Data},
number = {170088},
pages = {1--33},
title = {{A global multiproxy database for temperature reconstructions of the Common Era}},
volume = {4},
year = {2017}
}
@article{PAGES2KConsortium2013,
abstract = {ERRATUM Continental-scale temperature variability during the past two millennia PAGES 2k Consortium Nature Geoscience 6, 339–346 (2013); published online 21 April 2013; corrected aſter print 14 May 2013. In the version of this Progress Article originally published, the references “43–54” cited in the caption of Fig. 4a should have read “43–45”. This has been corrected in the PDF and HTML versions. NATURE},
author = {{PAGES2K Consortium}},
doi = {10.1038/ngeo1849},
file = {:home/luis/Dropbox/Biblioteca/articulos/PAGES2K Consortium - 2013 - Continental-scale temperature variability during the past two millennia.pdf:pdf},
isbn = {1752-0894},
issn = {1752-0894},
journal = {Nature Geoscience},
number = {6},
pages = {503--503},
title = {{Continental-scale temperature variability during the past two millennia}},
url = {http://www.nature.com/doifinder/10.1038/ngeo1849},
volume = {6},
year = {2013}
}
@book{Ramsay2005,
address = {New York, NY},
author = {Ramsay, J.O. and Silverman, B.W.},
edition = {Second},
publisher = {Springer},
series = {Springer Series in Statistics},
title = {{Functional Data Analysis}},
year = {2005}
}
@article{Rue2009,
author = {Rue, H{\^{a}}vard and Martino, Sara and Chopin, Nicolas},
file = {:home/luis/Dropbox/Biblioteca/articulos/Rue, Martino, Chopin - 2009 - Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximation.pdf:pdf},
journal = {Journal of the Royal Statistical Society . Series B ( Methodological )},
keywords = {laplace approxima-,latent gaussian,models using integrated nested,roximate bayesian inference for},
number = {2},
pages = {319--392},
title = {{Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations}},
volume = {71},
year = {2009}
}
@article{Ruiz-Cardenas2012,
abstract = {Inference in state-space models usually relies on recursive forms for filtering and smoothing of the state vectors regarding the temporal structure of the observations, an assumption that is, from our view point, unnecessary if the dataset is fixed, that is, completely available before analysis. In this paper, we propose a computational framework to perform approximate full Bayesian inference in linear and generalized dynamic linear models based on the Integrated Nested Laplace Approximation (INLA) approach. The proposed framework directly approximates the posterior marginals of interest disregarding the assumption of recursive updating/estimation of the states and hyperparameters in the case of fixed datasets and, therefore, enable us to do fully Bayesian analysis of complex state-space models more easily and in a short computational time. The proposed framework overcomes some limitations of current tools in the dynamic modeling literature and is vastly illustrated with a series of simulated as well as well known real-life examples from the literature, including realistically complex models with correlated error structures and models with more than one state vector, being mutually dependent on each other. R code is available online for all the examples presented. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Ruiz-C{\'{a}}rdenas, Ramiro and Krainski, Elias T. and Rue, H{\aa}vard},
doi = {10.1016/j.csda.2011.10.024},
file = {:home/luis/Dropbox/Biblioteca/articulos/Ruiz-C{\'{a}}rdenas, Krainski, Rue - 2012 - Direct fitting of dynamic models using integrated nested Laplace approximations - INLA.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximate Bayesian inference,Augmented model,Laplace approximation,Spatio-temporal dynamic models,State-space models},
pages = {1808--1828},
publisher = {Elsevier B.V.},
title = {{Direct fitting of dynamic models using integrated nested Laplace approximations - INLA}},
url = {http://dx.doi.org/10.1016/j.csda.2011.10.024},
volume = {56},
year = {2012}
}
@article{Scheuerer2014,
abstract = {Statistical post-processing of dynamical forecast ensembles is an essential component of weather forecasting. In this article, we present a post-processing method which generates full predictive probability distributions for precipitation accumulations based on ensemble model output statistics (EMOS). We model precipitation amounts by a generalized extreme value distribution which is left-censored at zero. This distribution permits modelling precipitation on the original scale without prior transformation of the data. A closed form expression for its continuous ranked probability score can be derived and permits computationally efficient model fitting. We discuss an extension of our approach which incorporates further statistics characterizing the spatial variability of precipitation amounts in the vicinity of the location of interest. The proposed EMOS method is applied to daily 18 h forecasts of 6 h accumulated precipitation over Germany in 2011 using the COSMO-DE ensemble prediction system operated by the German Meteorological Service. It yields calibrated and sharp predictive distributions and compares favourably with extended logistic regression and Bayesian model averaging which are state-of-the-art approaches for precipitation post-processing. The incorporation of neighbourhood information further improves predictive performance and turns out to be a useful strategy to account for displacement errors of the dynamical forecasts in a probabilistic forecasting framework. {\textcopyright} 2013 Royal Meteorological Society.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.0893v1},
author = {Scheuerer, M.},
doi = {10.1002/qj.2183},
eprint = {arXiv:1302.0893v1},
file = {:home/luis/Dropbox/Biblioteca/articulos/Scheuerer - 2014 - Probabilistic quantitative precipitation forecasting using Ensemble Model Output Statistics.pdf:pdf},
issn = {1477870X},
journal = {Quarterly Journal of the Royal Meteorological Society},
keywords = {Ensemble prediction,Forecast calibration,Precipitation,Short-range forecasting},
number = {680},
pages = {1086--1096},
title = {{Probabilistic quantitative precipitation forecasting using Ensemble Model Output Statistics}},
volume = {140},
year = {2014}
}
@article{Thomason2016,
author = {Thomason, Larry and Vernier, Jean-Paul and Bourassa, Adam and Arfeuille, Florian and Bingen, Christine and Peter, Thomas and Luo, Beiping},
journal = {To be submitted to Geoscientific Model Development Discussions},
title = {{Stratospheric Aerosol Data Set (SADS Version 2) Prospectus}},
year = {2016}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:home/luis/Dropbox/Biblioteca/articulos/Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society . Series B ( Methodological )},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@article{Tierney1986,
author = {Tierney, Luke and Kadane, Joseph B},
file = {:home/luis/Dropbox/Biblioteca/articulos/Tierney, Kadane - 1986 - Accurate Approximations for Posterior Moments and Marginal Densities.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {a user of bayesian,asymp-,bayesian inference,computation of integrals,laplace method,log posterior density,methods in practice needs,second derivative of the,the inverse of the,to be able,totic expansions},
number = {393},
pages = {82--86},
title = {{Accurate Approximations for Posterior Moments and Marginal Densities}},
volume = {81},
year = {1986}
}
@article{Toohey2016,
abstract = {The Easy Volcanic Aerosol (EVA) forcing generator produces stratospheric aerosol optical properties as a function of time, latitude, height and wavelength for a given input list of volcanic eruption attributes. EVA is based on a parameterized three-box model of stratospheric transport, and simple scaling relationships used to derive mid-visible (550 nm) aerosol optical depth and aerosol effective radius from stratospheric sulfate mass. Pre-calculated look up tables computed from Mie theory are used to produce wavelength dependent aerosol extinction, single scattering albedo and scattering asymmetry factor values. The structural form of EVA, and the tuning of its parameters, are chosen to produce best agreement with the satellite-based reconstruction of stratospheric aerosol properties following the 1991 Pinatubo eruption, and with prior millennial-time scale forcing reconstructions including the 1815 eruption of Tambora. EVA can be used to produce volcanic forcing for climate models which is based on recent observations and physical understanding, but internally self-consistent over any time-scale of choice. In addition, EVA is constructed so as to allow for easy modification of different aspects of aerosol properties, in order to be used in model experiments to help advance understanding of what aspects of the volcanic aerosol are important for the climate system.},
author = {Toohey, Matthew and Stevens, Bjorn and Schmidt, Hauke and Timmreck, Claudia},
doi = {10.5194/gmd-9-4049-2016},
file = {:home/luis/Dropbox/Biblioteca/articulos/Toohey et al. - 2016 - Easy Volcanic Aerosol (EVA v1.0) An idealized forcing generator for climate simulations.pdf:pdf},
issn = {19919603},
journal = {Geoscientific Model Development},
number = {11},
pages = {4049--4070},
title = {{Easy Volcanic Aerosol (EVA v1.0): An idealized forcing generator for climate simulations}},
volume = {9},
year = {2016}
}
@article{Vieira2011,
abstract = {Aims. We present a physically consistent reconstruction of the total solar irradiance for the Holocene. Methods. We extend the SATIRE models to estimate the evolution of the total (and partly spectral) solar irradiance over the Holocene. The basic assumption is that the variations of the solar irradiance are due to the evolution of the dark and bright magnetic features on the solar surface. The evolution of the decadally averaged magnetic flux is computed from decadal values of cosmogenic isotope concentrations recorded in natural archives employing a series of physics-based models connecting the processes from the modulation of the cosmic ray flux in the heliosphere to their record in natural archives. We then compute the total solar irradiance (TSI) as a linear combination of the jth and jth + 1 decadal values of the open magnetic flux. Results. Reconstructions of the TSI over the Holocene, each valid for a di{\_}erent paleomagnetic time series, are presented. Our analysis suggests that major sources of uncertainty in the TSI in this model are the heritage of the uncertainty of the TSI since 1610 reconstructed from sunspot data and the uncertainty of the evolution of the Earth's magnetic dipole moment. The analysis of the distribution functions of the reconstructed irradiance for the last 3000 years indicates that the estimates based on the virtual axial dipole moment are significantly lower at earlier times than the reconstructions based on the virtual dipole moment. Conclusions. We present the first physics-based reconstruction of the total solar irradiance over the Holocene, which will be of interest for studies of climate change over the last 11500 years. The reconstruction indicates that the decadally averaged total solar irradiance ranges over approximately 1.5 W/m2 from grand maxima to grand minima.},
archivePrefix = {arXiv},
arxivId = {1103.4958},
author = {Vieira, Luis Eduardo A. and Solanki, Sami K. and Krivova, Natalie A. and Usoskin, Ilya},
doi = {10.1051/0004-6361/201015843},
eprint = {1103.4958},
file = {:home/luis/Dropbox/Biblioteca/articulos/Vieira et al. - 2011 - Evolution of the solar irradiance during the Holocene.pdf:pdf},
isbn = {doi:10.1051/0004-6361/201015843},
issn = {0004-6361},
journal = {Astronomy and Astrophysics},
keywords = {activity,faculae,plages,solar-terrestrial relations,sun,sunspots,surface magnetism,uv radiation},
number = {A6},
pages = {1--20},
title = {{Evolution of the solar irradiance during the Holocene}},
url = {http://arxiv.org/abs/1103.4958},
volume = {531},
year = {2011}
}
@article{Weisberg2002,
abstract = {Regression is the study of the dependence of a response variable tors collected in . In dimension reduction regression, we seek to find a few linear combinations ??? ? ??? ? ??? ? ? ? on a collection ? predic- , such that all the information about the regression is contained in these linear com- binations. If is very small, perhaps one or two, then the regression problem can be summarized using simple graphics; for example, for , the plot of versus information. When , a 3D plot contains all the information. ? ? ? ? ? contains all the regression Several methods for estimating and relevant functions of "! ? ? ? ? have been suggested in the literature. In this paper, we describe an R package for three important dimension reduction methods: sliced inverse regression or sir, sliced average variance estimates, or save, and principal Hessian directions, or phd. The package is very general and flexible, and can be easily extended to include other methods of dimension reduction. It includes tests and estimates of the dimension , estimates of the relevant information including ? ? ? ? ? , and some useful graphical summaries as well},
author = {Weisberg, Sanford},
doi = {http://dx.doi.org/10.18637/jss.v007.i01},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1998},
pages = {1--22},
title = {{Dimension reduction regression in R}},
volume = {7},
year = {2002}
}
@article{Zhong2005,
abstract = {Identification of transcription factor binding motifs ({\{}TFBMs{\}}) is$\backslash$na crucial first step towards the understanding of regulatory circuitries$\backslash$ncontrolling the expression of genes. In this paper, we propose a$\backslash$nnovel procedure called regularized sliced inverse regression ({\{}RSIR{\}})$\backslash$nfor identifying {\{}TFBMs{\}}. {\{}RSIR{\}} follows a recent trend to combine$\backslash$ninformation contained in both gene expression measurements and genes'$\backslash$npromoter sequences. Compared with existing methods, {\{}RSIR{\}} is efficient$\backslash$nin computation, very stable for data with high dimensionality and$\backslash$nhigh collinearity, and improves motif detection sensitivities and$\backslash$nspecificities by avoiding inappropriate model specification.},
author = {Zhong, Wenxuan and Zeng, Peng and Ma, Ping and Liu, Jun S. and Zhu, Yu},
doi = {10.1093/bioinformatics/bti680},
file = {:home/luis/Dropbox/Biblioteca/articulos/Zhong et al. - 2005 - RSIR Regularized sliced inverse regression for motif discovery.pdf:pdf},
isbn = {1367-4803 (Print)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {22},
pages = {4169--4175},
pmid = {16166098},
title = {{RSIR: Regularized sliced inverse regression for motif discovery}},
volume = {21},
year = {2005}
}
