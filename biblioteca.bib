Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop



@article{vanAlbada2007,
	Author = {S.J. van Albada and P.A. Robinson},
	Doi = {10.1016/j.jneumeth.2006.11.004},
	Issn = {0165-0270},
	Journal = {Journal of Neuroscience Methods},
	Keywords = {Parametric statistics},
	Number = {2},
	Pages = {205 -- 211},
	Title = {Transformation of arbitrary distributions to the normal distribution with application to \{EEG\} test--retest reliability},
	Volume = {161},
	Year = {2007}}

@article{Hansen:2005,
	Author = {Hansen, James and Nazarenko, Larissa and Ruedy, Reto and Sato, Makiko and Willis, Josh and Del Genio, Anthony and Koch, Dorothy and Lacis, Andrew and Lo, Ken and Menon, Surabi and Novakov, Tica and Perlwitz, Judith and Russell, Gary and Schmidt, Gavin A. and Tausnev, Nicholas},
	Date-Added = {2018-10-03 23:00:45 +0000},
	Date-Modified = {2018-10-03 23:00:56 +0000},
	Doi = {10.1126/science.1110252},
	Eprint = {http://science.sciencemag.org/content/308/5727/1431.full.pdf},
	Issn = {0036-8075},
	Journal = {Science},
	Number = {5727},
	Pages = {1431--1435},
	Publisher = {American Association for the Advancement of Science},
	Title = {Earth{\textquoteright}s Energy Imbalance: Confirmation and Implications},
	Url = {http://science.sciencemag.org/content/308/5727/1431},
	Volume = {308},
	Year = {2005},
	Bdsk-Url-1 = {http://science.sciencemag.org/content/308/5727/1431},
	Bdsk-Url-2 = {https://dx.doi.org/10.1126/science.1110252}}


@article{Tingley_QSR2012,
	Author = {Martin P. Tingley and Peter F. Craigmile and Murali Haran and Bo Li and Elizabeth Mannshardt and Bala Rajaratnam},
	Date-Added = {2012-02-15 13:09:30 -0800},
	Date-Modified = {2012-09-06 14:06:00 -0700},
	Doi = {10.1016/j.quascirev.2012.01.012},
	Issn = {0277-3791},
	Journal = {Quaternary Science Reviews},
	Keywords = {Space--time modeling},
	Number = {0},
	Pages = {1 - 22},
	Title = {Piecing together the past: statistical insights into paleoclimatic reconstructions},
	Volume = {35},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0277379112000248},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.quascirev.2012.01.012}}


@article{HegerlZwiers:2011,
	Annote = {doi: 10.1002/wcc.121},
	Author = {Hegerl, Gabriele and Zwiers, Francis},
	Booktitle = {Wiley Interdisciplinary Reviews: Climate Change},
	Da = {2011/07/01},
	Date = {2011/05/26},
	Date-Added = {2018-10-03 16:17:34 +0000},
	Date-Modified = {2018-10-03 16:18:00 +0000},
	Doi = {10.1002/wcc.121},
	Isbn = {1757-7780},
	Journal = {Wiley Interdisciplinary Reviews: Climate Change},
	Journal1 = {Wiley Interdisciplinary Reviews: Climate Change},
	Journal2 = {WIREs Clim Change},
	Number = {4},
	Pages = {570--591},
	Publisher = {Wiley-Blackwell},
	Title = {Use of models in detection and attribution of climate change},
	Ty = {JOUR},
	Volume = {2},
	Year = {2011},
	Year1 = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1002/wcc.121}}


@article{Hegerl07,
	Author = {Hegerl, G. C. and T. Crowley and M. Allen and W. T. Hyde and H. Pollack and J. Smerdon and E. Zorita},
	Date-Added = {2008-10-03 16:34:26 -0400},
	Date-Modified = {2008-10-03 16:35:51 -0400},
	Journal = {J. Clim.},
	Pages = {650--666},
	Title = {Detection of human influence on a new 1500 yr climate reconstruction},
	Volume = {20},
	Year = {2007}}


@article{Schurer2013a,
	Annote = {doi: 10.1175/JCLI-D-12-00826.1},
	Author = {Schurer, Andrew P. and Hegerl, Gabriele C. and Mann, Michael E. and Tett, Simon F. B. and Phipps, Steven J.},
	Booktitle = {Journal of Climate},
	Da = {2013/09/01},
	Date = {2013/03/25},
	Date-Added = {2018-02-17 07:24:56 +0000},
	Date-Modified = {2018-02-17 07:24:56 +0000},
	Doi = {10.1175/JCLI-D-12-00826.1},
	Isbn = {0894-8755},
	Journal = {Journal of Climate},
	Journal1 = {J. Climate},
	M3 = {doi: 10.1175/JCLI-D-12-00826.1},
	Number = {18},
	Pages = {6954--6973},
	Publisher = {American Meteorological Society},
	Title = {Separating Forced from Chaotic Climate Variability over the Past Millennium},
	Ty = {JOUR},
	Url = {https://doi.org/10.1175/JCLI-D-12-00826.1},
	Volume = {26},
	Year = {2013}}

@article{Schurer2013b,
	Author = {Schurer, Andrew P. and Tett, Simon F. B. and Hegerl, Gabriele C.},
	Date = {2013/12/22/online},
	Day = {22},
	Journal = {Nature Geoscience},
	doi = {10.1038/ngeo2040},
	Pages = {104 EP -},
	Publisher = {Nature Publishing Group SN -},
	Title = {Small influence of solar variability on climate over the past millennium},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/ngeo2040},
	Volume = {7},
	Year = {2013}
}


@book{GelmanCarlinSternRubin04,
	Address = {New York, NY},
	Author = {A. Gelman and J. B. Carlin and H. S. Stern and D. B. Rubin},
	Booktitle = {Bayesian Data Analysis},
	Date-Added = {2010-01-19 23:11:06 -0800},
	Date-Modified = {2015-12-23 19:49:15 +0000},
	Edition = {2nd},
	Isbn = {978-1439840955},
	Pages = {675},
	Publisher = {Chapman and Hall},
	Title = {Bayesian Data Analysis},
	Year = {2013}}


@article{JungclausGMD17,
	Author = {Jungclaus, J. H. and et al.},
	Date-Added = {2018-10-01 18:17:34 +0000},
	Date-Modified = {2018-10-01 18:18:09 +0000},
	Doi = {10.5194/gmd-10-4005-2017},
	Journal = {Geoscientific Model Development},
	Number = {11},
	Pages = {4005--4033},
	Title = {The PMIP4 contribution to CMIP6 -- Part 3: The last millennium, scientific objective, and experimental design for the PMIP4 \textit{past1000} simulations},
	Url = {https://www.geosci-model-dev.net/10/4005/2017/},
	Volume = {10},
	Year = {2017},
	Bdsk-Url-1 = {https://www.geosci-model-dev.net/10/4005/2017/},
	Bdsk-Url-2 = {https://dx.doi.org/10.5194/gmd-10-4005-2017}}


@article{Kageyama_GMD2017,
	Author = {Kageyama, M. and et al.},
	Date-Added = {2018-04-07 00:14:17 +0000},
	Date-Modified = {2018-04-07 00:14:17 +0000},
	Doi = {10.5194/gmd-10-4035-2017},
	Journal = {Geoscientific Model Development},
	Number = {11},
	Pages = {4035--4055},
	Title = {The PMIP4 contribution to CMIP6 -- Part 4: Scientific objectives and experimental design of the PMIP4-CMIP6 Last Glacial Maximum experiments and PMIP4 sensitivity experiments},
	Url = {https://www.geosci-model-dev.net/10/4035/2017/},
	Volume = {10},
	Year = {2017},
	Bdsk-Url-1 = {https://www.geosci-model-dev.net/10/4035/2017/},
	Bdsk-Url-2 = {https://dx.doi.org/10.5194/gmd-10-4035-2017}}


@article{BenjaminiHochberg95,
	Abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	Author = {Benjamini, Yoav and Hochberg, Yosef},
	Cr = {Copyright \copyright 1995 Royal Statistical Society},
	Date-Added = {2015-09-04 01:04:16 +0000},
	Date-Modified = {2015-09-04 01:04:35 +0000},
	Doi = {10.2307/2346101},
	Isbn = {00359246},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	M1 = {ArticleType: research-article / Full publication date: 1995 / Copyright {\copyright}1995 Royal Statistical Society},
	Month = {01},
	Number = {1},
	Pages = {289--300},
	Publisher = {Wiley for the Royal Statistical Society},
	Title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
	Ty = {JOUR},
	Volume = {57},
	Year = {1995},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2346101},
	Bdsk-Url-2 = {http://dx.doi.org/10.2307/2346101}}


@article{lipd_cp,
	Author = {McKay, N. P. and Emile-Geay, J.},
	Date-Added = {2016-05-03 19:23:24 +0000},
	Date-Modified = {2016-05-03 19:24:06 +0000},
	Doi = {10.5194/cp-12-1093-2016},
	Journal = {Climate of the Past},
	Number = {4},
	Pages = {1093--1100},
	Title = {Technical note: The Linked Paleo Data framework : a common tongue for paleoclimatology},
	Url = {http://www.clim-past.net/12/1093/2016/},
	Volume = {12},
	Year = {2016},
	Bdsk-Url-1 = {http://www.clim-past.net/12/1093/2016/},
	Bdsk-Url-2 = {http://dx.doi.org/10.5194/cp-12-1093-2016}}

@misc{lipd-util,
	Author = {McKay, Nicholas and Heiser, Christopher and Julien, Emile-Geay and Khider, Deborah},
	Date-Added = {2018-04-05 22:06:22 +0000},
	Date-Modified = {2018-04-05 22:07:08 +0000},
	Doi = {10.5281/zenodo.60813},
	Month = aug,
	Title = {Linked Paleo Data: utilities in Python, R and Matlab},
	Url = {https://doi.org/10.5281/zenodo.60813},
	Year = 2016,
	Bdsk-Url-1 = {https://doi.org/10.5281/zenodo.60813}}


@book{Jaynes04,
	Address = {Cambridge},
	Author = {E. T. Jaynes},
	Booktitle = {Probability Theory: The Logic of Science},
	Date-Added = {2009-09-08 10:36:29 -0700},
	Date-Modified = {2009-09-08 10:37:42 -0700},
	Number = {727 pages},
	Publisher = {Cambridge University Press},
	Title = {Probability Theory: The Logic of Science},
	Year = {2004}}

@article{MBH98,
	Author = {{Mann}, M.~E. and {Bradley}, R.~S. and {Hughes}, M.~K.},
	Doi = {10.1038/33859},
	Journal = {Nature},
	Month = apr,
	Pages = {779-787},
	Title = {{Global-scale temperature patterns and climate forcing over the past six centuries}},
	Volume = 392,
	Year = 1998,
	doi = {10.1038/33859}}

@article{MBH99,
	Author = {{Mann}, M.~E. and {Bradley}, R.~S. and {Hughes}, M.~K.},
	Doi = {10.1029/1999GL900070},
	Journal = {Geophys. Res. Lett.},
	Month = mar,
	Pages = {759-762},
	Title = {{Northern hemisphere temperatures during the past millennium: Inferences, uncertainties, and limitations}},
	Volume = 26,
	Year = 1999,
	doi = {10.1029/1999GL900070}}


@article{Evans_grl2014,
	Author = {Evans, M. N. and Smerdon, J. E. and Kaplan, A. and Tolwinski-Ward, S. E. and Gonz{\'a}lez-Rouco, J. F.},
	C8 = {2014GL062063},
	Date-Added = {2015-12-23 19:43:25 +0000},
	Date-Modified = {2015-12-23 19:43:39 +0000},
	Doi = {10.1002/2014GL062063},
	Isbn = {1944-8007},
	Journal = {Geophysical Research Letters},
	Journal1 = {Geophys. Res. Lett.},
	Keywords = {paleoclimate; pseudoproxy experiment; climate field reconstruction; uncertainty; 1952 Modeling; 3275 Uncertainty quantification; 3344 Paleoclimatology; 4928 Global climate models; 4920 Dendrochronology},
	Number = {24},
	Pages = {9127--9134},
	Title = {Climate field reconstruction uncertainty arising from multivariate and nonlinear properties of predictors},
	Ty = {JOUR},
	Volume = {41},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/2014GL062063}}


@article{Wang_CP2014,
	Author = {Wang, J. and Emile-Geay, J. and Guillot, D. and Smerdon, J. E. and Rajaratnam, B.},
	Date-Added = {2014-01-21 17:42:43 +0000},
	Date-Modified = {2014-01-21 17:43:14 +0000},
	Doi = {10.5194/cp-10-1-2014},
	Journal = {Climate of the Past},
	Number = {1},
	Pages = {1--19},
	Title = {Evaluating climate field reconstruction techniques using improved emulations of real-world conditions},
	Url = {http://www.clim-past.net/10/1/2014/},
	Volume = {10},
	Year = {2014},
	Bdsk-Url-1 = {http://www.clim-past.net/10/1/2014/},
	Bdsk-Url-2 = {http://dx.doi.org/10.5194/cp-10-1-2014}}


@article{Neukom:2014,
	Author = {Neukom, Raphael and et al.},
	Date = {2014/03/30/online},
	Date-Added = {2018-09-28 23:57:45 +0000},
	Date-Modified = {2018-09-28 23:57:51 +0000},
	Day = {30},
	Journal = {Nature Climate Change},
	L3 = {10.1038/nclimate2174; https://www.nature.com/articles/nclimate2174#supplementary-information},
	Month = {03},
	Pages = {362 EP  -},
	Publisher = {Nature Publishing Group SN  -},
	Title = {Inter-hemispheric temperature variability over the past millennium},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/nclimate2174},
	Volume = {4},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nclimate2174}}


@article{Guillot_AOAS2015,
	Abstract = {Understanding centennial scale climate variability requires data sets that are accurate, long, continuous and of broad spatial coverage. Since instrumental measurements are generally only available after 1850, temperature fields must be reconstructed using paleoclimate archives, known as proxies. Various climate field reconstructions (CFR) methods have been proposed to relate past temperature to such proxy networks. In this work, we propose a new CFR method, called GraphEM, based on Gaussian Markov random fields embedded within an EM algorithm. Gaussian Markov random fields provide a natural and flexible framework for modeling high-dimensional spatial fields. At the same time, they provide the parameter reduction necessary for obtaining precise and well-conditioned estimates of the covariance structure, even in the sample-starved setting common in paleoclimate applications. In this paper, we propose and compare the performance of different methods to estimate the graphical structure of climate fields, and demonstrate how the GraphEM algorithm can be used to reconstruct past climate variations. The performance of GraphEM is compared to the widely used CFR method RegEM with regularization via truncated total least squares, using synthetic data. Our results show that GraphEM can yield significant improvements, with uniform gains over space, and far better risk properties. We demonstrate that the spatial structure of temperature fields can be well estimated by graphs where each neighbor is only connected to a few geographically close neighbors, and that the increase in performance is directly related to recovering the underlying sparsity in the covariance of the spatial field. Our work demonstrates how significant improvements can be made in climate reconstruction methods by better modeling the covariance structure of the climate field.},
	Author = {Guillot, Dominique and Rajaratnam, Bala and Emile-Geay, Julien},
	Da = {2015/03},
	Date-Added = {2015-05-07 18:59:35 +0000},
	Date-Modified = {2015-08-03 23:36:29 +0000},
	Doi = {10.1214/14-AOAS794},
	Isbn = {1932-6157},
	J2 = {Ann. Appl. Stat.},
	Journal = {Ann. Applied. Statist.},
	Keywords = {Climate reconstructions; Markov random fields; covariance matrix estimation; sparsity; model selection; pseudoproxies},
	La = {en},
	M1 = {1},
	Pages = {324--352},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {{{Statistical paleoclimate reconstructions via Markov random fields}}},
	Ty = {JOUR},
	Vn = {9},
	Year = {2015},
	Bdsk-Url-1 = {http://projecteuclid.org/euclid.aoas/1430226095},
	Bdsk-Url-2 = {http://dx.doi.org/10.1214/14-AOAS794}}


@article{Lee_CD08,
	Author = {{Lee}, T.~C.~K. and {Zwiers}, F.~W. and {Tsao}, M.},
	Date-Added = {2009-10-08 09:46:05 -0700},
	Date-Modified = {2009-10-08 09:46:21 -0700},
	Doi = {10.1007/s00382-007-0351-9},
	Journal = {Climate Dynamics},
	Keywords = {Kalman filter, State-space model, Temperature reconstruction},
	Month = aug,
	Pages = {263-281},
	Title = {{Evaluation of proxy-based millennial reconstruction methods}},
	Volume = 31,
	Year = 2008,
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00382-007-0351-9}}


@article{Wang_GRL15,
	Author = {Wang, Jianghao and Emile-Geay, Julien and Guillot, Dominique and McKay, Nicholas P. and Rajaratnam, Bala},
	C8 = {2015GL065265},
	Date-Added = {2015-08-19 05:33:14 +0000},
	Date-Modified = {2016-10-15 18:20:40 +0000},
	Doi = {10.1002/2015GL065265},
	Isbn = {1944-8007},
	Journal = {Geophysical Research Letters},
	Journal1 = {Geophys. Res. Lett.},
	Keywords = {Climate field reconstruction; Paleoclimate data-model comparison; Robustness of CFR patterns; Global temperature reconstruction; statistical model assessment; 1616 Climate variability; 1626 Global climate models; 3344 Paleoclimatology; 4914 Continental climate records},
	Pages = {7162--7170},
	Title = {Fragility of reconstructed temperature patterns over the Common Era: Implications for model evaluation.},
	Ty = {JOUR},
	Volume = {42},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/2015GL065265}}

<<<<<<< HEAD
@article{PAGES2kSD2017allauthors,
	Author = {Emile-Geay, J. and et al.},
	Booktitle = {A global multiproxy database for temperature reconstructions of the Common Era},
	Date-Added = {2015-08-03 22:23:29 +0000},
	Date-Modified = {2017-07-28 00:15:32 +0000},
	Doi = {10.1038/sdata.2017.88},
	Journal = {Scientific Data},
	Pages = {170088 EP},
	Title = {{{A global multiproxy database for temperature reconstructions of the Common Era}}},
	Volume = {4},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/sdata.2017.88}}
=======

>>>>>>> be609b2ddc27f68e0c32466a53f2816de435195b

@article{PAGES2kSD2017short,
	Author = {PAGES2k Consortium},
	Booktitle = {A global multiproxy database for temperature reconstructions of the Common Era},
	Date-Added = {2017-07-28 00:08:00 +0000},
	Date-Modified = {2018-02-17 05:28:41 +0000},
	Doi = {10.1038/sdata.2017.88},
	Journal = {Scientific Data},
	Month = {07},
	Pages = {170088 EP},
	Title = {{{A global multiproxy database for temperature reconstructions of the Common Era}}},
	Volume = {4},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/sdata.2017.88}}


@article{JEG10a,
	Author = {Emile-Geay, J. and Cobb, K.M. and Mann, M.E and Wittenberg, Andrew T.},
	Date-Added = {2008-10-03 16:27:12 -0400},
	Date-Modified = {2015-05-29 23:49:35 +0000},
	Doi = {10.1175/JCLI-D-11-00510.1},
	Journal = {J. Clim.},
	Pages = {2302--2328},
	Title = {{Estimating Central Equatorial Pacific SST variability over the Past Millennium. Part 1: Methodology and Validation}},
	Volume = {26},
	Year = {2013},
	Bdsk-Url-1 = {http://journals.ametsoc.org/doi/pdf/10.1175/JCLI-D-11-00510.1}}

@article{JEG10b,
	Author = {Emile-Geay, J. and Cobb, K.M. and Mann, M.E and Wittenberg, Andrew T.},
	Date-Added = {2010-08-10 17:32:15 -0700},
	Date-Modified = {2015-05-29 23:49:41 +0000},
	Doi = {10.1175/JCLI-D-11-00511.1},
	Journal = {J. Clim.},
	Pages = {2329--2352},
	Title = {{Estimating Central Equatorial Pacific SST variability over the Past Millennium. Part 2: Reconstructions and Implications}},
	Volume = {26},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QVi4uLy4uLy4uL0xpYnJhcnkvQ2FjaGVzL01ldGFkYXRhL2VkdS51Y3NkLmNzLm1tY2NyYWNrLmJpYmRlc2svQWx2b3JkX01XUjE5MjEuYmRza2NhY2hl0hcLGBlXTlMuZGF0YU8RAiIAAAAAAiIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAANJIQXFIKwAAA1r5YxhBbHZvcmRfTVdSMTkyMS5iZHNrY2FjaGUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADXrMC18quSAAAAAAAAAAAAAMABQAACSAAAAAAAAAAAAAAAAAAAAAcZWR1LnVjc2QuY3MubW1jY3JhY2suYmliZGVzawAQAAgAANJIo+EAAAARAAgAANfLELgAAAABABgDWvljAH+EgQAJYMgACWDHAAlgwgAGKpoAAgBvTWFjaW50b3NoIEhEOlVzZXJzOgBqdWxpZW5lZzoATGlicmFyeToAQ2FjaGVzOgBNZXRhZGF0YToAZWR1LnVjc2QuY3MubW1jY3JhY2suYmliZGVzazoAQWx2b3JkX01XUjE5MjEuYmRza2NhY2hlAAAOADIAGABBAGwAdgBvAHIAZABfAE0AVwBSADEAOQAyADEALgBiAGQAcwBrAGMAYQBjAGgAZQAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFxVc2Vycy9qdWxpZW5lZy9MaWJyYXJ5L0NhY2hlcy9NZXRhZGF0YS9lZHUudWNzZC5jcy5tbWNjcmFjay5iaWJkZXNrL0Fsdm9yZF9NV1IxOTIxLmJkc2tjYWNoZQATAAEvAAAVAAIAD///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAOcA7AD0AxoDHAMhAywDNQNDA0cDTgNXA1wDaQNsA34DgQOGAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA4g=},
	Bdsk-Url-1 = {http://journals.ametsoc.org/doi/pdf/10.1175/JCLI-D-11-00511.1}}


@article{ghil2002natural,
	Author = {Ghil, Michael},
	Date-Added = {2017-10-12 23:27:29 +0000},
	Date-Modified = {2017-10-12 23:27:29 +0000},
	Journal = {Encyclopedia of global environmental change},
	Pages = {544--549},
	Publisher = {Wiley New York},
	Title = {Natural climate variability},
	Volume = {1},
	Year = {2002}}

@article{Jones_Holocene09,
	Author = {Jones, P.D. and et al.}, 
	Date-Added = {2009-08-28 06:23:19 -0700},
	Date-Modified = {2010-12-08 17:19:58 -0800},
	Doi = {10.1177/0959683608098952},
	Journal = {The Holocene},
	Number = {1},
	Pages = {3-49},
	Title = {{High-resolution palaeoclimatology of the last millennium: a review of current status and future prospects}},
	Volume = {19},
	Year = {2009},
	Bdsk-Url-1 = {http://hol.sagepub.com/cgi/content/abstract/19/1/3},
	Bdsk-Url-2 = {http://dx.doi.org/10.1177/0959683608098952}}


@article{huybers_links_2006,
	Abstract = {Climate variability exists at all timescales---and climatic processes are intimately coupled, so that understanding variability at any one timescale requires some understanding of the whole. Records of the Earth's surface temperature illustrate this interdependence, having a continuum of variability following a power-law scaling. But although specific modes of interannual variability are relatively well understood, the general controls on continuum variability are uncertain and usually described as purely stochastic processes. Here we show that power-law relationships of surface temperature variability scale with annual and Milankovitch-period (23,000- and 41,000-year) cycles. The annual cycle corresponds to scaling at monthly to decadal periods, while millennial and longer periods are tied to the Milankovitch cycles. Thus the annual, Milankovitch and continuum temperature variability together represent the response to deterministic insolation forcing. The identification of a deterministic control on the continuum provides insight into the mechanisms governing interannual and longer-period climate variability.},
	Author = {Huybers, Peter and Curry, William},
	Date-Modified = {2018-04-09 22:17:29 +0000},
	Doi = {10.1038/nature04745},
	Issn = {0028-0836},
	Journal = {Nature},
	Langid = {english},
	Number = {7091},
	Pages = {329--332},
	Title = {Links between annual, Milankovitch and continuum temperature variability},
	Volume = {441},
	Year = {2006},
	Bdsk-Url-1 = {https://dx.doi.org/10.1038/nature04745}}


@article{pelletier_power_1998,
	Abstract = {After removing annual variability, power spectral analyses of local atmospheric temperature from hundreds of stations and ice core records have been carried out from time scales of 10−2 to 106 yr. A clear sequence of power-law behaviors is found as follows: (1) from 40 ka to 1 Ma a flat spectrum is observed; (2) from 2 ka to 40 ka the spectrum is proportional to f−2 where f is the frequency; and (3) below time scales of 2 ka the power spectrum is proportional to f−1/2. At time scales less than 1 month we observe that the power spectra of continental stations become proportional to f−3/2 while maritime stations continue to have power spectra proportional to f−1/2 down to time scales of 1 day. To explain these observations, we model the vertical transport of heat in the atmosphere as a stochastic diffusion process. The power spectrum of temperature fluctuations at the earth's surface expected from this model equation in a two-layer geometry with thermal and eddy diffusion properties appropriate to the atmosphere and the ocean and a radiation condition at the top of the atmosphere agrees with the observed spectrum. The difference in power spectra between continental and marine stations can be understood with this approach as a consequence of the air mass above a maritime station exchanging heat with both the atmosphere above and the ocean below while a continental station exchanges heat mostly with the atmosphere above.},
	Author = {Pelletier, Jon D},
	Date-Modified = {2018-03-22 20:18:30 +0000},
	Doi = {10.1016/S0012-821X(98)00051-X},
	Issn = {0012-821X},
	Journal = {Earth and Planetary Science Letters},
	Keywords = {spectral analysis, atmosphere, heat transfer, temperature},
	Number = {3},
	Pages = {157--164},
	Shortjournal = {EPSL},
	Title = {The power spectral density of atmospheric temperature from time scales of $10^{-2}$ to $10^6$ yr},
	Url = {http://www.sciencedirect.com/science/article/pii/S0012821X9800051X},
	Urldate = {2018-02-13},
	Volume = {158},
	Year = {1998},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0012821X9800051X},
	Bdsk-Url-2 = {https://dx.doi.org/10.1016/S0012-821X(98)00051-X}}


@inproceedings{AR5_chap5,
	Address = {Cambridge, United Kingdom and New York, NY, USA},
	Author = {V. Masson-Delmotte and et al.},
	Booktitle = {{Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change}},
	Date-Added = {2014-09-08 19:03:37 +0000},
	Date-Modified = {2014-09-08 20:20:57 +0000},
	Doi = {10.1017/CBO9781107415324.013},
	Editor = {T. F. Stocker and D. Qin and G.-K. Plattner and M. Tignor and S.K. Allen and J. Boschung and A. Nauels and Y. Xia and V. Bex and P.M. Midgley},
	Pages = {383--464},
	Publisher = {Cambridge University Press},
	Title = {{Information from Paleoclimate Archives}},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1017/CBO9781107415324.013}}



@article{Kennedy2011a,
abstract = {New estimates of measurement and sampling uncertainties of gridded in situ sea surface temperature anomalies are calculated for 1850 to 2006. The measurement uncertainties account for correlations between errors in observations made by the same ship or buoy due, for example, to miscalibration of the thermometer. Correlations between the errors increase the estimated uncertainties on grid box averages. In grid boxes where there are many observations from only a few ships or drifting buoys, this increase can be large. The correlations also increase uncertainties of regional, hemispheric, and global averages above and beyond the increase arising solely from the inflation of the grid box uncertainties. This is due to correlations in the errors between grid boxes visited by the same ship or drifting buoy. At times when reliable estimates can be made, the uncertainties in global average, Southern Hemisphere, and tropical sea surface temperature anomalies are between 2 and 3 times as large as when calculated assuming the errors are uncorrelated. Uncertainties of Northern Hemisphere averages are approximately double. A new estimate is also made of sampling uncertainties. They are largest in regions of high sea surface temperature variability such as the western boundary currents and along the northern boundary of the Southern Ocean. The sampling uncertainties are generally smaller in the tropics and in the ocean gyres.},
author = {Kennedy, J. J. and Rayner, N. A. and Smith, R. O. and Parker, D. E. and Saunby, M.},
doi = {10.1029/2010JD015218},
file = {:home/luis/Dropbox/Biblioteca/articulos/Kennedy et al. - 2011 - Reassessing biases and other uncertainties in sea surface temperature observations measured in situ since 1850 1.pdf:pdf},
isbn = {2156-2202},
issn = {01480227},
journal = {Journal of Geophysical Research Atmospheres},
keywords = {http://dx.doi.org/10.1029/2010JD015218, doi:10.102},
number = {14},
pages = {1--13},
title = {{Reassessing biases and other uncertainties in sea surface temperature observations measured in situ since 1850: 1. Measurement and sampling uncertainties}},
volume = {116},
year = {2011}
}

@article{Cook2004,
abstract = {We develop tests of the hypothesis of no effect for selected predictors in regression, without assuming a model for the conditional distribution of the response given the predictors. Predictor effects need not be limited to the mean function and smoothing is not required. The general approach is based on sufficient dimension reduction, the idea being to replace the predictor vector with a lower-dimensional version without loss of information on the regression. Methodology using sliced inverse regression is developed in detail.},
archivePrefix = {arXiv},
arxivId = {math/0406520},
author = {Cook, R. Dennis},
doi = {10.1214/009053604000000292},
eprint = {0406520},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Central subspace,Nonparametric regression,Sliced inverse regression},
number = {3},
pages = {1062--1092},
primaryClass = {math},
title = {{Testing predictor contributions in sufficient dimension reduction}},
volume = {32},
year = {2004}
}
@article{Barboza2014,
author = {Barboza, Luis and Li, Bo and Tingley, Martin P. and Viens, Frederi G.},
doi = {???????},
file = {:home/luis/Dropbox/Biblioteca/articulos/Barboza et al. - 2014 - Reconstructing Past Temperatures from Natural Proxies and Estimated Climate Forcings using Short- and Long-Memor.pdf:pdf},
journal = {The Annals of Applied Statistics},
number = {4},
pages = {1966--2001},
title = {{Reconstructing Past Temperatures from Natural Proxies and Estimated Climate Forcings using Short- and Long-Memory Models}},
volume = {8},
year = {2014}
}
@article{Jones2012,
abstract = {This study is an extensive revision of the Climatic Research Unit (CRU) land station temperature database that has been used to produce a grid-box data set of 5° latitude × 5° longitude temperature anomalies. The new database (CRUTEM4) comprises 5583 station records of which 4842 have enough data for the 1961-1990 period to calculate or estimate the average temperatures for this period. Many station records have had their data replaced by newly homogenized series that have been produced by a number of studies, particularly from National Meteorological Services (NMSs). Hemispheric temperature averages for land areas developed with the new CRUTEM4 data set differ slightly from their CRUTEM3 equivalent. The inclusion of much additional data from the Arctic (particularly the Russian Arctic) has led to estimates for the Northern Hemisphere (NH) being warmer by about 0.1°C for the years since 2001. The NH/Southern Hemisphere (SH) warms by 1.12°C/0.84°C over the period 1901-2010. The robustness of the hemispheric averages is assessed by producing five different analyses, each including a different subset of 20{\%} of the station time series and by omitting some large countries. CRUTEM4 is also compared with hemispheric averages produced by reanalyses undertaken by the European Centre for Medium-Range Weather Forecasts (ECMWF): ERA-40 (1958-2001) and ERA-Interim (1979-2010) data sets. For the NH, agreement is good back to 1958 and excellent from 1979 at monthly, annual, and decadal time scales. For the SH, agreement is poorer, but if the area is restricted to the SH north of 60S, the agreement is dramatically improved from the mid-1970s. Copyright 2012 by the American Geophysical Union.},
archivePrefix = {arXiv},
arxivId = {https://doi.org/10.1029/2011JD017139},
author = {Jones, P. D. and Lister, D. H. and Osborn, T. J. and Harpham, C. and Salmon, M. and Morice, C. P.},
doi = {10.1029/2011JD017139},
eprint = {/doi.org/10.1029/2011JD017139},
file = {:home/luis/Dropbox/Biblioteca/articulos/Jones et al. - 2012 - Hemispheric and large-scale land-surface air temperature variations An extensive revision and an update to 2010.pdf:pdf},
isbn = {01480227 (ISSN)},
issn = {01480227},
journal = {Journal of Geophysical Research Atmospheres},
number = {5},
pmid = {12199731},
primaryClass = {https:},
title = {{Hemispheric and large-scale land-surface air temperature variations: An extensive revision and an update to 2010}},
volume = {117},
year = {2012}
}
@article{rutherford2003climate,
abstract = {The fidelity of climate reconstructions employing covariance-based
calibration techniques is tested with varying levels of sparseness
of available data during intervals of relatively constant (stationary)
and increasing (nonstationary) forcing. These tests employ a regularized
expectation-maximization algorithm using surface temperature data
from both the instrumental record and coupled ocean–atmosphere model
integrations. The results indicate that if radiative forcing is relatively
constant over a data-rich calibration period and increases over a
data-sparse reconstruction period, the imputed temperatures in the
reconstruction period may be biased and may underestimate the true
temperature trend. However, if radiative forcing is stationary over
a data-sparse reconstruction period and increases over a data-rich
calibration period, the imputed values in the reconstruction period
are nearly unbiased. These results indicate that using the data-rich
part of the twentieth-century instrumental record (which contains
an increasing temperature trend plausibly associated with increasing
radiative forcing) for calibration does not significantly bias reconstructions
of prior climate.},
author = {Rutherford, S and Mann, M E and Delworth, T L and Stouffer, R J},
journal = {Journal of Climate},
keywords = {*file-import-12-09-21},
number = {3},
pages = {462--479},
title = {{Climate field reconstruction under stationary and nonstationary forcing}},
volume = {16},
year = {2003}
}
@article{Thomason2016,
author = {Thomason, Larry and Vernier, Jean-Paul and Bourassa, Adam and Arfeuille, Florian and Bingen, Christine and Peter, Thomas and Luo, Beiping},
journal = {To be submitted to Geoscientific Model Development Discussions},
title = {{Stratospheric Aerosol Data Set (SADS Version 2) Prospectus}},
year = {2016}
}
@misc{Chung2013,
author = {Chung, Dongjun and Chun, Hyonho and Keles, Sunduz},
title = {{spls: Sparse Partial Least Squares (SPLS) Regression and Classification}},
url = {https://cran.r-project.org/package=spls},
year = {2013}
}
@misc{Coudret2017,
author = {Coudret, Rapha{\"{e}}l and Liquet, Beno{\^{i}}t and Saracco, J{\'{e}}r{\^{o}}me},
title = {{edrGraphicalTools: Provides Tools for Dimension Reduction Methods}},
url = {https://cran.r-project.org/package=edrGraphicalTools},
year = {2017}
}
@article{Coudret2014,
abstract = {Among methods to analyze high-dimensional data, the sliced inverse regression (SIR) is of particular interest for non-linear relations between the dependent variable and some indices of the covariate. When the dimension of the covariate is greater than the number of observations, classical versions of SIR cannot be applied. Various upgrades were then proposed to tackle this issue such as regularized SIR (RSIR) and sparse ridge SIR (SR-SIR), to estimate the parameters of the underlying model and to select variables of interest. In this paper, we introduce two new estimation methods respectively based on the QZ algorithm and on the Moore-Penrose pseudo-inverse.We also describe a new selection procedure of the most relevant components of the covariate that relies on a proximity criterion between submodels and the initial one. These approaches are compared with RSIR and SR-SIR in a simulation study. Finally we applied SIR-QZ and the associated selection procedure to a genetic dataset in order to find markers that are linked to the expression of a gene. These markers are called expression quantitative trait loci (eQTL).},
author = {Coudret, R. and Liquet, B. and Saracco, J.},
file = {:home/luis/Dropbox/Biblioteca/articulos/Coudret, Liquet, Saracco - 2014 - Comparison of sliced inverse regression aproaches for undetermined cases.pdf:pdf},
journal = {Journal de la Soci{\'{e}}t{\'{e}} Fran{\c{c}}aise de Statistique},
keywords = {High-dimensional data,dimension reduction,semiparametric regression,sparsity},
number = {2},
pages = {72--96},
title = {{Comparison of sliced inverse regression aproaches for undetermined cases}},
url = {http://journal-sfds.fr/index.php/J-SFdS/article/view/278},
volume = {155},
year = {2014}
}
@article{Morice2012,
abstract = {Recent developments in observational near-surface air temperature and sea-surface temperature analyses are combined to produce HadCRUT4, a new data set of global and regional temperature evolution from 1850 to the present. This includes the addition of newly digitised measurement data, both over land and sea, new sea-surface temperature bias adjustments and a more comprehensive error model for describing uncertainties in sea-surface temperature measurements. An ensemble approach has been adopted to better describe complex temporal and spatial interdependencies of measurement and bias uncertainties and to allow these correlated uncertainties to be taken into account in studies that are based upon HadCRUT4. Climate diagnostics computed from the gridded data set broadly agree with those of other global near-surface temperature analyses. Fitted linear trends in temperature anomalies are approximately 0.07 degC/decade from 1901 to 2010 and 0.17 degC/decade from 1979 to 2010 globally. Northern/southern hemispheric trends are 0.08/0.07 degC/decade over 1901 to 2010 and 0.24/0.10 degC/decade over 1979 to 2010. Linear trends in other prominent near-surface temperature analyses agree well with the range of trends computed from the HadCRUT4 ensemble members.},
author = {Morice, Colin P. and Kennedy, John J. and Rayner, Nick A. and Jones, Phil D.},
doi = {10.1029/2011JD017187},
isbn = {2156-2202},
issn = {01480227},
journal = {Journal of Geophysical Research Atmospheres},
keywords = {climate,historical temperature,near surface temperature,observational ensemble},
number = {8},
pages = {1--22},
title = {{Quantifying uncertainties in global and regional temperature change using an ensemble of observational estimates: The HadCRUT4 data set}},
volume = {117},
year = {2012}
}
@article{Gergis2016,
abstract = {AbstractWe present multi-proxy warm season (September–February) temperature reconstructions for the combined land–ocean region of Australasia (0°S–50°S, 110°E–180°E) covering A.D. 1000-2001. Using between two (R2) and 28 (R28) proxy records we compare four 1000-member ensemble reconstructions of regional temperature using four statistical methods: Principal Component Regression (PCR), Composite Plus Scale (CPS), Bayesian Hierarchical Models (LNA) and Pairwise Comparison (PaiCo). The reconstructions are compared with a three-member ensemble of GISS-E2-R model simulations and independent palaeoclimate records.Decadal fluctuations in Australasian temperatures are remarkably similar between the four reconstruction methods. There are, however, differences in the amplitude of temperature variations between the different statistical methods and proxy networks. When the R28 network is used, the warmest 30-year periods occur after 1950 in more than 70{\%} of ensemble members for all methods. However, reconstructions ...},
author = {Gergis, Jo{\"{e}}lle and Neukom, Raphael and Gallant, Ailie J.E. and Karoly, David J.},
doi = {10.1175/JCLI-D-13-00781.1},
file = {:home/luis/Dropbox/Biblioteca/articulos/Gergis et al. - 2016 - Australasian temperature reconstructions spanning the Last Millennium.pdf:pdf},
issn = {08948755},
journal = {Journal of Climate},
keywords = {Australia,Climate change,Climate variability,Geographic location/entity,Paleoclimate,Physical Meteorology and Climatology,Variability},
number = {15},
pages = {5365--5392},
title = {{Australasian temperature reconstructions spanning the Last Millennium}},
volume = {29},
year = {2016}
}
@article{Toohey2016,
abstract = {The Easy Volcanic Aerosol (EVA) forcing generator produces stratospheric aerosol optical properties as a function of time, latitude, height and wavelength for a given input list of volcanic eruption attributes. EVA is based on a parameterized three-box model of stratospheric transport, and simple scaling relationships used to derive mid-visible (550 nm) aerosol optical depth and aerosol effective radius from stratospheric sulfate mass. Pre-calculated look up tables computed from Mie theory are used to produce wavelength dependent aerosol extinction, single scattering albedo and scattering asymmetry factor values. The structural form of EVA, and the tuning of its parameters, are chosen to produce best agreement with the satellite-based reconstruction of stratospheric aerosol properties following the 1991 Pinatubo eruption, and with prior millennial-time scale forcing reconstructions including the 1815 eruption of Tambora. EVA can be used to produce volcanic forcing for climate models which is based on recent observations and physical understanding, but internally self-consistent over any time-scale of choice. In addition, EVA is constructed so as to allow for easy modification of different aspects of aerosol properties, in order to be used in model experiments to help advance understanding of what aspects of the volcanic aerosol are important for the climate system.},
author = {Toohey, Matthew and Stevens, Bjorn and Schmidt, Hauke and Timmreck, Claudia},
doi = {10.5194/gmd-9-4049-2016},
issn = {19919603},
journal = {Geoscientific Model Development},
number = {11},
pages = {4049--4070},
title = {{Easy Volcanic Aerosol (EVA v1.0): An idealized forcing generator for climate simulations}},
volume = {9},
year = {2016}
}
@article{steig2009,
abstract = {Assessments of Antarctic temperature change have emphasized the contrast
between strong warming of the Antarctic Peninsula and slight cooling
of the Antarctic continental interior in recent decades1. This pattern
of temperature change has been attributed to the increased strength
of the circumpolar westerlies, largely in response to changes in
stratospheric ozone2. This picture, however, is substantially incomplete
owing to the sparseness and short duration of the observations. Here
we show that significant warming extends well beyond the Antarctic
Peninsula to cover most of West Antarctica, an area of warming much
larger than previously reported. West Antarctic warming exceeds 0.1 °C
per decade over the past 50 years, and is strongest in winter and
spring. Although this is partly offset by autumn cooling in East
Antarctica, the continent-wide average near-surface temperature trend
is positive. Simulations using a general circulation model reproduce
the essential features of the spatial pattern and the long-term trend,
and we suggest that neither can be attributed directly to increases
in the strength of the westerlies. Instead, regional changes in atmospheric
circulation and associated changes in sea surface temperature and
sea ice are required to explain the enhanced warming in West Antarctica.},
author = {Steig, Eric J and Schneider, David P and Rutherford, Scott D and Mann, Michael E and Comiso, Josefino C and Shindell, Drew T},
doi = {10.1038/nature07669},
issn = {0028-0836},
journal = {Nature},
number = {7228},
pages = {459--462},
publisher = {Nature Publishing Group},
title = {{Warming of the Antarctic ice-sheet surface since the 1957 International Geophysical Year}},
url = {http://dx.doi.org/10.1038/nature07669},
volume = {457},
year = {2009}
}
@article{Blangiardo2013,
abstract = {During the last three decades, Bayesian methods have developed greatly in the field of epidemiology. Their main challenge focusses around computation, but the advent of Markov Chain Monte Carlo methods (MCMC) and in particular of the WinBUGS software has opened the doors of Bayesian modelling to the wide research community. However model complexity and database dimension still remain a constraint.Recently the use of Gaussian random fields has become increasingly popular in epidemiology as very often epidemiological data are characterised by a spatial and/or temporal structure which needs to be taken into account in the inferential process. The Integrated Nested Laplace Approximation (INLA) approach has been developed as a computationally efficient alternative to MCMC and the availability of an R package (R-INLA) allows researchers to easily apply this method.In this paper we review the INLA approach and present some applications on spatial and spatio-temporal data. ?? 2012 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Blangiardo, Marta and Cameletti, Michela and Baio, Gianluca and Rue, H{\aa}vard},
doi = {10.1016/j.sste.2013.07.003},
eprint = {arXiv:1011.1669v3},
isbn = {1877-5845},
issn = {18775845},
journal = {Spatial and Spatio-temporal Epidemiology},
keywords = {Area-level data,Bayesian approach,Integrated Nested Laplace Approximation,Point-level data,Stochastic Partial Differential Equation approach,integrated nested laplace approximation,stochastic partial differential equation},
pages = {39--55},
pmid = {24377114},
publisher = {Elsevier Ltd},
title = {{Spatial and spatio-temporal models with R-INLA}},
url = {http://dx.doi.org/10.1016/j.sste.2013.07.003},
volume = {7},
year = {2013}
}
@article{Wahl2012,
author = {Wahl, Eugene R. and Smerdon, Jason E.},
doi = {10.1029/2012GL051086},
issn = {00948276},
journal = {Geophysical Research Letters},
month = {mar},
number = {6},
pages = {L06703},
title = {{Comparative performance of paleoclimate field and index reconstructions derived from climate proxies and noise-only predictors}},
url = {http://doi.wiley.com/10.1029/2012GL051086},
volume = {39},
year = {2012}
}
@article{Li2008,
abstract = {In high-dimensional data analysis, sliced inverse regression (SIR) has proven to be an effective dimension reduction tool and has enjoyed wide applications. The usual SIR, however, cannot work with problems where the number of predictors, p, exceeds the sample size, n, and can suffer when there is high collinearity among the predictors. In addition, the reduced dimensional space consists of linear combinations of all the original predictors and no variable selection is achieved. In this article, we propose a regularized SIR approach based on the least-squares formulation of SIR. The L2 regularization is introduced, and an alternating least-squares algorithm is developed, to enable SIR to work with n {\textless} p and highly correlated predictors. The L1 regularization is further introduced to achieve simultaneous reduction estimation and predictor selection. Both simulations and the analysis of a microarray expression data set demonstrate the usefulness of the proposed method.},
author = {Li, Lexin and Yin, Xiangrong},
doi = {10.1111/j.1541-0420.2007.00836.x},
issn = {0006341X},
journal = {Biometrics},
keywords = {Regularized least squares,Sliced inverse regression,Sufficient dimension reduction},
number = {1},
pages = {124--131},
pmid = {17651455},
title = {{Sliced inverse regression with regularizations}},
volume = {64},
year = {2008}
}
@book{Ramsay2005,
address = {New York, NY},
author = {Ramsay, J.O. and Silverman, B.W.},
edition = {Second},
publisher = {Springer},
series = {Springer Series in Statistics},
title = {{Functional Data Analysis}},
year = {2005}
}
@article{Martins2013,
abstract = {The INLA approach for approximate Bayesian inference for latent Gaussian models has been shown to give fast and accurate estimates of posterior marginals and also to be a valuable tool in practice via the R-package R-INLA. New developments in the R-INLA are formalized and it is shown how these features greatly extend the scope of models that can be analyzed by this interface. The current default method in R-INLA to approximate the posterior marginals of the hyperparameters using only a modest number of evaluations of the joint posterior distribution of the hyperparameters, without any need for numerical integration, is discussed. ?? 2013 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0333v1},
author = {Martins, Thiago G. and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
doi = {10.1016/j.csda.2013.04.014},
eprint = {arXiv:1210.0333v1},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximate,Bayesian inference,INLA,Latent Gaussian models},
pages = {68--83},
publisher = {Elsevier B.V.},
title = {{Bayesian computing with INLA: New features}},
url = {http://dx.doi.org/10.1016/j.csda.2013.04.014},
volume = {67},
year = {2013}
}
@article{Li1991,
abstract = {Modem advances in computing power have greatly widened scientists' scope in gathering which might have been ignored in the past. Yet to effectively many variables, information an easy task, although our ability to interact this article, we propose a novel data-analytic x without going through any parametric properties vectors. This model looks like a nonlinear unknown. For effectively space] generated regression coefficients. for e.d.r. directions. Furthermore, model-fitting inverse view of regression; that is, instead of regressing regression, the univariate and investigating with data has been much enhanced by recent innovations tool, sliced inverse regression or nonparametric (SIR), for reducing x against y. Forward regression and inverse regression are connected by a theorem that motivates of SIR are investigated by the 3k'S. This makes our goal different In fact, the fk'S themselves covariance, the inverse regression curve, E(x I y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix shows that under a suitable condition, if the distribution for the estimated inverse regression smoothing is needed. SIR can be easily implemented effectively curve can be conducted to locate its main orientation, yielding the dimension information process. This method explores the simplicity output variable y against the multivariate that the functional reducing the dimension, we need only to estimate the space [effective are not identifiable of x has been standardized from scan a large pool of variables is not in dynamic graphics. In of the input variable of the x, we regress under a model of the form, y = f(lx, ..., 8Kx, e), where the 3ks are the unknown row except for the crucial difference from the usual one in regression analysis, the estimation without a specific structural this method. The theoretical form off is completely dimension reduction (e.d.r.) of all the form onf. Our main theorem we use a simple step function to estimate the inverse regression curve. No complicated on personal computers. By simulation, we demonstrate plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin- is proposed to address the issue of whether or not a direction KEY WORDS: Dynamic graphics; Principal component analysis; Projection pursuit. 1. INTRODUCTION lationship variable x, a p-dimensional parametric model is parsimonious, as the maximum likelihood Regression analysis is a popular way of studying between a response variable y and its explanatory column vector. Quite often, a the re- model is used to guide the analysis. When the such have standard estimation proved to be successful in gathering techniques or the least squares method information data. In most applications, however, any parametric at best an approximation alternatives that offer from the model is an adequate model is not easy. When there are no persua- sive models available, nonparametric emerge as promising to the true one, and the search for regression regression only the continuity function. on the presence point of interest information. is the idea of local smoothing, or differentiability of sufficiently * Ker-Chau Li is Professor, ematics, University For one-dimensional property ibility in modeling. A common theme of nonparametric regression the needed flex- which explores The success of local smoothing many data points around each in the design space to provide adequate problems, many smooth- of California, Los Angeles, CA 90024. This research the 3's as an effective Division of Statistics, Department of Math- was supported in part by the National Science Foundation under grants DMS86-02018 and DMS89-02494. It has been a long time since I intro- duced SIR in talks given at Berkeley, Bell Labs, and Rutgers in 1985. I received many useful questions and suggestions from these audiences. I am indebted to Naihua Duan for stimulating leading eventually to Duan and Li (1991). Peter Bickel brought semi- parametric literature to my attention. edge in areas of dimension reduction and multivariate Dennis Cook, who introduced Section 6.3 would have been further regression" this article. Three referees for improving the presentation. David Brillinger, delayed. As a replacement for "slice gested to me by Don Ylvisaker, who also helped me clear hurdles in publishing suggestions whose work has inspired me so much. 316 or "slicing regression," used earlier, the name SIR was sug- nice and an associate editor offered Finally, I would like to thank Jan de Leeuw broadened my knowl- XLISP-STAT to me, the appearance of techniques found by SIR is spurious.},
author = {Li, Ker-Chau},
doi = {10.2307/2290568},
file = {:home/luis/Dropbox/Biblioteca/articulos/Li - 1991 - Sliced Inverse Regression for Dimension Reduction.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {dynamic graphics,principal component analysis,projection pursuit},
number = {414},
pages = {316--327},
title = {{Sliced Inverse Regression for Dimension Reduction}},
volume = {86},
year = {1991}
}
@article{Liquet2012,
abstract = {Sliced inverse regression (SIR) and related methods were introduced in order to reduce the dimensionality of regression problems. In general semiparametric regression framework, these methods determine linear combinations of a set of explanatory variables X related to the response variable Y, without losing information on the conditional distribution of Y given X. They are based on a "slicing step" in the population and sample versions. They are sensitive to the choice of the number H of slices, and this is particularly true for SIR-II and SAVE methods. At the moment there are no theoretical results nor practical techniques which allows the user to choose an appropriate number of slices. In this paper, we propose an approach based on the quality of the estimation of the effective dimension reduction (EDR) space: the square trace correlation between the true EDR space and its estimate can be used as goodness of estimation. We introduce a na {\textless} ve bootstrap estimation of the square trace correlation criterion to allow selection of an "optimal" number of slices. Moreover, this criterion can also simultaneously select the corresponding suitable dimension K (number of the linear combination of X). From a practical point of view, the choice of these two parameters H and K is essential. We propose a 3D-graphical tool, implemented in R, which can be useful to select the suitable couple (H, K). An R package named "edrGraphicalTools" has been developed. In this article, we focus on the SIR-I, SIR-II and SAVE methods. Moreover the proposed criterion can be use to determine which method seems to be efficient to recover the EDR space, that is the structure between Y and X. We indicate how the proposed criterion can be used in practice. A simulation study is performed to illustrate the behavior of this approach and the need for selecting properly the number H of slices and the dimension K. A short real-data example is also provided.},
author = {Liquet, Beno{\^{i}}t and Saracco, J{\'{e}}r{\^{o}}me},
doi = {10.1007/s00180-011-0241-9},
file = {:home/luis/Dropbox/Biblioteca/articulos/Liquet, Saracco - 2012 - A graphical tool for selecting the number of slices and the dimension of the model in SIR and SAVE approaches.pdf:pdf},
issn = {09434062},
journal = {Computational Statistics},
keywords = {Bootstrap,Dimension reduction,Sliced average variance estimation (SAVE),Sliced inverse regression (SIR),Square trace correlation},
number = {1},
pages = {103--125},
title = {{A graphical tool for selecting the number of slices and the dimension of the model in SIR and SAVE approaches}},
volume = {27},
year = {2012}
}
@article{mann2007robust,
abstract = {We present results from continued investigations into the fidelity
of covariance-based climate field reconstruction (CFR) approaches
used in proxy-based climate reconstruction. Our experiments employ
synthetic ” pseudoproxy” data derived from simulations of forced
climate changes over the past millennium. Using networks of these
pseudoproxy data, we investigate the sensitivity of CFR performance
to signal-to-noise ratios, the noise spectrum, the spatial sampling
of pseudoproxy locations, the statistical representation of predictors
used, and the diagnostic used to quantify reconstruction skill. Our
results reinforce previous conclusions that CFR methods, correctly
implemented and applied to suitable networks of proxy data, should
yield reliable reconstructions of past climate histories within estimated
uncertainties. Our results also demonstrate the deleterious impact
of a linear detrending procedure performed recently in certain CFR
studies and illustrate flaws in some previously proposed metrics
of reconstruction skill.},
author = {Mann, Michael E and Rutherford, Scott and Wahl, Eugene and Ammann, Caspar},
doi = {10.1029/2006JD008272},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
number = {D12},
pages = {D12109+},
publisher = {American Geophysical Union},
title = {{Robustness of proxy-based climate field reconstruction methods}},
url = {http://dx.doi.org/10.1029/2006JD008272},
volume = {112},
year = {2007}
}
@article{Bair2006,
abstract = {In regression problems where the number of predictors greatly exceeds the number of observations, conventional regression techniques may produce unsatisfactory results. We describe a technique called supervised principal components that can be applied to this type of problem. Supervised principal components is similar to conventional principal components analysis except that it uses a subset of the predictors selected based on their association with the outcome. Supervised principal components can be applied to regression and generalized regression problems, such as survival analysis. It compares favorably to other techniques for this type of problem, and can also account for the effects of other covariates and help identify which predictor variables are most important. We also provide asymptotic consistency results to help support our empirical findings. These methods could become important tools for DNA microarray data, where they may be used to more accurately diagnose and treat cancer.},
author = {Bair, Eric and Hastie, Trevor and Paul, Debashis and Tibshirani, Robert},
doi = {10.1198/016214505000000628},
file = {:home/luis/Dropbox/Biblioteca/articulos/Bair et al. - 2006 - Prediction by Supervised Principal Components(2).pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {gene expression,microarray,regression,survival analysis},
number = {473},
pages = {119--137},
title = {{Prediction by Supervised Principal Components}},
volume = {101},
year = {2006}
}
@article{Scheuerer2014,
abstract = {Statistical post-processing of dynamical forecast ensembles is an essential component of weather forecasting. In this article, we present a post-processing method which generates full predictive probability distributions for precipitation accumulations based on ensemble model output statistics (EMOS). We model precipitation amounts by a generalized extreme value distribution which is left-censored at zero. This distribution permits modelling precipitation on the original scale without prior transformation of the data. A closed form expression for its continuous ranked probability score can be derived and permits computationally efficient model fitting. We discuss an extension of our approach which incorporates further statistics characterizing the spatial variability of precipitation amounts in the vicinity of the location of interest. The proposed EMOS method is applied to daily 18 h forecasts of 6 h accumulated precipitation over Germany in 2011 using the COSMO-DE ensemble prediction system operated by the German Meteorological Service. It yields calibrated and sharp predictive distributions and compares favourably with extended logistic regression and Bayesian model averaging which are state-of-the-art approaches for precipitation post-processing. The incorporation of neighbourhood information further improves predictive performance and turns out to be a useful strategy to account for displacement errors of the dynamical forecasts in a probabilistic forecasting framework. {\textcopyright} 2013 Royal Meteorological Society.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.0893v1},
author = {Scheuerer, M.},
doi = {10.1002/qj.2183},
eprint = {arXiv:1302.0893v1},
issn = {1477870X},
journal = {Quarterly Journal of the Royal Meteorological Society},
keywords = {Ensemble prediction,Forecast calibration,Precipitation,Short-range forecasting},
number = {680},
pages = {1086--1096},
title = {{Probabilistic quantitative precipitation forecasting using Ensemble Model Output Statistics}},
volume = {140},
year = {2014}
}
@article{Weisberg2002,
abstract = {Regression is the study of the dependence of a response variable tors collected in . In dimension reduction regression, we seek to find a few linear combinations ??? ? ??? ? ??? ? ? ? on a collection ? predic- , such that all the information about the regression is contained in these linear com- binations. If is very small, perhaps one or two, then the regression problem can be summarized using simple graphics; for example, for , the plot of versus information. When , a 3D plot contains all the information. ? ? ? ? ? contains all the regression Several methods for estimating and relevant functions of "! ? ? ? ? have been suggested in the literature. In this paper, we describe an R package for three important dimension reduction methods: sliced inverse regression or sir, sliced average variance estimates, or save, and principal Hessian directions, or phd. The package is very general and flexible, and can be easily extended to include other methods of dimension reduction. It includes tests and estimates of the dimension , estimates of the relevant information including ? ? ? ? ? , and some useful graphical summaries as well},
author = {Weisberg, Sanford},
doi = {http://dx.doi.org/10.18637/jss.v007.i01},
file = {:home/luis/Dropbox/Biblioteca/articulos/Weisberg - 2002 - Dimension reduction regression in R.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1998},
pages = {1--22},
title = {{Dimension reduction regression in R}},
volume = {7},
year = {2002}
}
@article{Wu2010,
abstract = {We develop a supervised dimension reduction method that integrates the idea of localization from manifold learning with the sliced inverse regression framework. We call our method localized sliced inverse regression (LSIR) since it takes into account the local structure of the explanatory variables. The resulting projection from LSIR is a linear subspace of the explanatory variables that captures the nonlinear structure relevant to predicting the response. LSIR applies to both classification and regression problems and can be easily extended to incorporate the ancillary unlabeled data in semi-supervised learning. We illustrate the utility of LSIR on real and simulated data. Computer codes and datasets from simulations are available online.},
author = {Wu, Qiang and Liang, Feng and Mukherjee, Sayan},
doi = {10.1198/jcgs.2010.08080},
file = {:home/luis/Dropbox/Biblioteca/articulos/Wu, Liang, Mukherjee - 2010 - Localized sliced inverse regression.pdf:pdf},
isbn = {9781605609492},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Dimension reduction,Localization,Semi-supervised learning},
number = {4},
pages = {843--860},
title = {{Localized sliced inverse regression}},
volume = {19},
year = {2010}
}
@article{bradley2005proxy,
abstract = {Results are presented from a set of experiments designed to investigate
factors that may influence proxy-based reconstructions of large-scale
temperature patterns in past centuries. The factors investigated
include 1) the method used to assimilate proxy data into a climate
reconstruction, 2) the proxy data network used, 3) the target season,
and 4) the spatial domain of the reconstruction. Estimates of hemispheric-mean
temperature are formed through spatial averaging of reconstructed
temperature patterns that are based on either the local calibration
of proxy and instrumental data or a more elaborate multivariate climate
field reconstruction approach. The experiments compare results based
on the global multiproxy dataset used by Mann and coworkers, with
results obtained using the extratropical Northern Hemisphere (NH)
maximum latewood tree-ring density set used by Briffa and coworkers.
Mean temperature reconstructions are compared for the full NH (Tropics
and extratropics, land and ocean) and extratropical continents only,
withvarying target seasons (cold-season half year, warm-season half
year, and annual mean). The comparisons demonstrate dependence of
reconstructions on seasonal, spatial, and methodological considerations,
emphasizing the primary importance of the target region and seasonal
window of the reconstruction. The comparisons support the generally
robust nature of several previously published estimates of NH mean
temperature changes in past centuries and suggest that further improvements
in reconstructive skill are most likely to arise from an emphasis
on the quality, rather than quantity, of available proxy data.},
author = {Rutherford, S and Mann, M E and Osborn, T J and Briffa, K R and Jones, P D and Bradley, R S and Hughes, M K},
journal = {Journal of Climate},
keywords = {*file-import-12-09-21},
number = {13},
pages = {2308--2329},
title = {{Proxy-Based Northern Hemisphere Surface Temperature Reconstructions: Sensitivity to Method, Predictor Network, Target Season, and Target Domain}},
volume = {18},
year = {2005}
}

@article{Meinshausen_GMD2017,
	Author = {Meinshausen, M. and et al.},
	Date-Added = {2017-08-22 23:09:36 +0000},
	Date-Modified = {2017-08-22 23:10:12 +0000},
	Doi = {10.5194/gmd-10-2057-2017},
	Journal = {Geoscientific Model Development},
	Number = {5},
	Pages = {2057--2116},
	Title = {{{Historical greenhouse gas concentrations for climate modelling (CMIP6)}}},
	Url = {https://www.geosci-model-dev.net/10/2057/2017/},
	Volume = {10},
	Year = {2017}
}
@article{Rue2009,
author = {Rue, H{\^{a}}vard and Martino, Sara and Chopin, Nicolas},
journal = {Journal of the Royal Statistical Society . Series B ( Methodological )},
keywords = {laplace approxima-,latent gaussian,models using integrated nested,roximate bayesian inference for},
number = {2},
pages = {319--392},
title = {{Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations}},
volume = {71},
year = {2009}
}

@article{Chun2010,
abstract = {Partial least squares regression has been an alternative to ordinary least squares for handling multicollinearity in several areas of scientific research since the 1960s. It has recently gained much attention in the analysis of high dimensional genomic data. We show that known asymptotic consistency of the partial least squares estimator for a univariate response does not hold with the very large p and small n paradigm. We derive a similar result for a multivariate response regression with partial least squares. We then propose a sparse partial least squares formulation which aims simultaneously to achieve good predictive performance and variable selection by producing sparse linear combinations of the original predictors. We provide an efficient implementation of sparse partial least squares regression and compare it with well-known variable selection and dimension reduction approaches via simulation experiments. We illustrate the practical utility of sparse partial least squares regression in a joint analysis of gene expression and genomewide binding data.},
author = {Chun, Hyonho and Keleş, S{\"{u}}nd{\"{u}}z},
doi = {10.1111/j.1467-9868.2009.00723.x},
file = {:home/luis/Dropbox/Biblioteca/articulos/Chun, Keleş - 2010 - Sparse partial least squares regression for simultaneous dimension reduction and variable selection.pdf:pdf},
isbn = {1369-7412 (Print)$\backslash$r1369-7412 (Linking)},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Chromatin immuno-precipitation,Dimension reduction,Gene expression,Lasso,Microarrays,Partial least squares,Sparsity,Variable and feature selection},
number = {1},
pages = {3--25},
pmid = {20107611},
title = {{Sparse partial least squares regression for simultaneous dimension reduction and variable selection}},
volume = {72},
year = {2010}
}
@article{Zou2005,
abstract = {We missed an important reference in Section 3.4. In page 309 we stated that '. . . which is based on the recently proposed algorithm LARS of Efron et al. (2004). They proved that, starting from zero, the lasso solution paths grow piecewise linearly in a predictable way. They proposed a new algorithm called LARS to solve the entire lasso solution path efficiently by using the same order of computations as a single OLS fit. . . .' The following sentence should have been included. The piecewise linearity of the lasso solution path was first proved by Osborne et al. (2000), who also described an efficient algorithm for calculating the complete lasso solution path. Reference Osborne, M. R., Presnell, B. and Turlach, B. A. (2000) A new approach to variable selection in least squares problems. IMA J. Numer. Anal., 20, 389–403.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00527.x},
file = {:home/luis/Dropbox/Biblioteca/articulos/Zou, Hastie - 2005 - Regularization and variable selection via the elastic net.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {grouping effect,lars algorithm,lasso,p,penalization},
number = {5},
pages = {768--768},
title = {{Regularization and variable selection via the elastic net}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00527.x},
volume = {67},
year = {2005}
}
@article{PAGES2KConsortium2013,
abstract = {ERRATUM Continental-scale temperature variability during the past two millennia PAGES 2k Consortium Nature Geoscience 6, 339–346 (2013); published online 21 April 2013; corrected aſter print 14 May 2013. In the version of this Progress Article originally published, the references “43–54” cited in the caption of Fig. 4a should have read “43–45”. This has been corrected in the PDF and HTML versions. NATURE},
author = {{PAGES2K Consortium}},
doi = {10.1038/ngeo1849},
file = {:home/luis/Dropbox/Biblioteca/articulos/PAGES2K Consortium - 2013 - Continental-scale temperature variability during the past two millennia.pdf:pdf},
isbn = {1752-0894},
issn = {1752-0894},
journal = {Nature Geoscience},
number = {6},
pages = {503--503},
title = {{Continental-scale temperature variability during the past two millennia}},
url = {http://www.nature.com/doifinder/10.1038/ngeo1849},
volume = {6},
year = {2013}
}
@article{Fan2010,
abstract = {High dimensional statistical problems arise from diverse fields of scientific research and technological development. Variable selection plays a pivotal role in contemporary statistical learning and scientific discoveries. The traditional idea of best subset selection methods, which can be regarded as a specific form of pe-nalized likelihood, is computationally too expensive for many modern statistical applications. Other forms of penalized likelihood methods have been successfully developed over the last decade to cope with high dimensionality. They have been widely applied for simultaneously selecting important variables and estimating their effects in high dimensional statistical inference. In this article, we present a brief ac-count of the recent developments of theory, methods, and implementations for high dimensional variable selection. What limits of the dimensionality such methods can handle, what the role of penalty functions is, and what the statistical properties are rapidly drive the advances of the field. The properties of non-concave penalized likelihood and its roles in high dimensional statistical modeling are emphasized. We also review some recent advances in ultra-high dimensional variable selection, with emphasis on independence screening and two-scale methods.},
archivePrefix = {arXiv},
arxivId = {0910.1122},
author = {Fan, Jianqing and Lv, Jinchi},
doi = {10.1063/1.3520482},
eprint = {0910.1122},
file = {:home/luis/Dropbox/Biblioteca/articulos/Fan, Lv - 2010 - A Selective Overview of Variable Selection in High Dimensional Feature Space.pdf:pdf},
isbn = {2122633255},
issn = {1017-0405},
journal = {Statistica Sinica},
keywords = {Dimensionality reduction,LASSO,SCAD,and phrases,folded-concave penalty,high dimensionality,model selection,oracle property,penalized least squares,penalized likelihood,sure independence screening,sure screening,variable selection},
pages = {101--148},
pmid = {21572976},
title = {{A Selective Overview of Variable Selection in High Dimensional Feature Space}},
volume = {20},
year = {2010}
}
@article{luterbacher2004european,
abstract = {Multiproxy reconstructions of monthly and seasonal surface temperature
fields for Europe back to 1500 show that the late 20th- and early
21st-century European climate is very likely ({\textgreater}95{\%} confidence level)
warmer than that of any time during the past 500 years. This agrees
with findings for the entire Northern Hemisphere. European winter
average temperatures during the period 1500 to 1900 were reduced
by ∼0.5°C (0.25°C for annual mean temperatures) compared to the 20th
century. Summer temperatures did not experience systematic century-scale
cooling relative to present conditions. The coldest European winter
was 1708/1709; 2003 was by far the hottest summer.},
author = {Luterbacher, J and Dietrich, D and Xoplaki, E and Grosjean, M and Wanner, H},
journal = {Science},
keywords = {*file-import-12-09-20},
number = {5663},
pages = {1499--1503},
publisher = {American Association for the Advancement of Science},
title = {{European seasonal and annual temperature variability, trends, and extremes since 1500}},
volume = {303},
year = {2004}
}
@article{Tierney1986,
author = {Tierney, Luke and Kadane, Joseph B},
file = {:home/luis/Dropbox/Biblioteca/articulos/Tierney, Kadane - 1986 - Accurate Approximations for Posterior Moments and Marginal Densities.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {a user of bayesian,asymp-,bayesian inference,computation of integrals,laplace method,log posterior density,methods in practice needs,second derivative of the,the inverse of the,to be able,totic expansions},
number = {393},
pages = {82--86},
title = {{Accurate Approximations for Posterior Moments and Marginal Densities}},
volume = {81},
year = {1986}
}
@article{Friedman2010,
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
file = {:home/luis/Dropbox/Biblioteca/articulos/Friedman, Hastie, Tibshirani - 2010 - Regularization Paths for Generalized Linear Models via Coordinate Descent.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {fmri,functional connectivity,spatial model},
number = {1},
title = {{Regularization Paths for Generalized Linear Models via Coordinate Descent}},
volume = {33},
year = {2010}
}
@article{Pollack2004,
abstract = {Ground surface temperature (GST) reconstructions determined from temperature profiles measured in terrestrial boreholes, when averaged over the Northern Hemisphere, estimate a surface warming of $\backslash${\&}{\{}$\backslash${\#}{\}}8764;1 K during the interval AD 1500$\backslash${\&}{\{}$\backslash${\#}{\}}8211;2000. Other traditional proxy-based estimates suggest less warming during the same interval. Mann et al. [2003a] have raised two issues with regard to borehole-based reconstructions. The first focuses on the need for spatial gridding and area-weighting of the ensemble of borehole-based GST reconstructions to yield an average hemispheric reconstruction. The second asserts that application of optimal detection techniques show that the GST only weakly displays the spatial structure of the surface air temperature (SAT). We demonstrate the consistency of GST warming estimates by showing that over a wide range of grid element area and occupancy weighting schemes, the five-century GST change falls in the range of 0.89{\&}{\#}8211;1.05 K. We examine the subhemispheric spatial correlation of GST and SAT trends at various spatial scales. In the 5-degree grid employed for optimal detection, we find that the majority of grid element means are determined from three or fewer boreholes, a number that is insufficient to suppress site-specific noise via ensemble averaging. Significant spatial correlation between SAT and GST emerges in a 5-degree grid if low-occupancy grid elements are excluded, and also in a 30-degree grid in which grid element means are better determined through higher occupancy. Reconstructions assembled after excluding low-occupancy grid elements show a five-century GST change in the range of 1.02{\&}{\#}8211;1.06 K.},
author = {Pollack, Henry N. and Smerdon, Jason E.},
doi = {10.1029/2003JD004163},
file = {:home/luis/Dropbox/Biblioteca/articulos/Pollack, Smerdon - 2004 - Borehole climate reconstructions Spatial structure and hemispheric averages.pdf:pdf},
isbn = {0148-0227},
issn = {01480227},
journal = {Journal of Geophysical Research D: Atmospheres},
keywords = {Boreholes,Paleoclimate,Spatial analysis,Surface temperature},
number = {11},
pages = {1--9},
title = {{Borehole climate reconstructions: Spatial structure and hemispheric averages}},
volume = {109},
year = {2004}
}
@article{Hakim2016,
abstract = {An “offline” approach to DA is used, where static ensemble samples are drawn from existing CMIP climate-model simulations to serve as the prior estimate of climate variables.We use linear, univariate forward models (“proxy systemmodels (PSMs)”) that map climate variables to proxy measurements by fitting proxy data to 2mair temperature fromgridded instrumental temperature data; the linear PSMs are then used to predict proxy values from the prior estimate. Results for the LMR are compared against six gridded instrumental temperature data sets and 25{\%} of the proxy records are withheld from assimilation for independent verification. Results show broad agreement with previous reconstructions of Northern Hemisphere mean 2mair temperature, with millennial-scale cooling, a multicentennial warm period around 1000 C.E., and a cold period coincident with the Little Ice Age (circa 1450–1800 C.E.). Verification against gridded instrumental data sets during 1880–2000 C.E. reveals greatest skill in the tropics and lowest skill over Northern Hemisphere land areas. Verification against independent proxy records indicates substantial improvement relative to the model (prior) data without proxy assimilation. As an illustrative example, we present multivariate reconstructed fields for a singular event, the 1808/1809 “mystery” volcanic eruption, which reveal global cooling that is strongly enhanced locally due to the presence of the Pacific-North America wave pattern in the 500 hPa geopotential height field},
author = {Hakim, Gregory J. and Emile-Geay, Julien and Steig, Eric J. and Noone, David and Anderson, David M. and Tardif, Robert and Steiger, Nathan and Perkins, Walter A.},
doi = {10.1002/2016JD024751},
file = {:home/luis/Dropbox/Biblioteca/articulos/Hakim et al. - 2016 - The last millennium climate reanalysis project Framework and first results.pdf:pdf},
issn = {21562202},
journal = {Journal of Geophysical Research},
keywords = {10.1002/2016JD024751 and paleoclimate,data assimilation,proxies,volcanic eruption},
number = {12},
pages = {6745--6764},
title = {{The last millennium climate reanalysis project: Framework and first results}},
volume = {121},
year = {2016}
}
@article{tingley2,
abstract = {Abstract Part I presented a Bayesian algorithm for reconstructing
climate anomalies in space and time (BARCAST). This method involves
specifying simple parametric forms for the spatial covariance and
temporal evolution of the climate field as well as ?observation equations?
describing the relationships between the data types and the corresponding
true values of the climate field. As this Bayesian approach to reconstructing
climate fields is new and different, it is worthwhile to compare
it in detail to the more established regularized expectation?maximization
(RegEM) algorithm, which is based on an empirical estimate of the
joint data covariance matrix and a multivariate regression of the
instrumental time series onto the proxy time series. The differing
assumptions made by BARCAST and RegEM are detailed, and the impacts
of these differences on the analysis are discussed. Key distinctions
between BARCAST and RegEM include their treatment of spatial and
temporal covariance, the prior information that enters into each
analysis, the quantities they seek to impute, the end product of
each analysis, the temporal variance of the reconstructed field,
and the treatment of uncertainty in both the imputed values and functions
of these imputations. Differences between BARCAST and RegEM are illustrated
by applying the two approaches to various surrogate datasets. If
the assumptions inherent to BARCAST are not strongly violated, then
in scenarios comparable to practical applications BARCAST results
in reconstructions of both the field and the spatial mean that are
more skillful than those produced by RegEM, as measured by the coefficient
of efficiency. In addition, the uncertainty intervals produced by
BARCAST are narrower than those estimated using RegEM and contain
the true values with higher probability. Abstract Part I presented
a Bayesian algorithm for reconstructing climate anomalies in space
and time (BARCAST). This method involves specifying simple parametric
forms for the spatial covariance and temporal evolution of the climate
field as well as ?observation equations? describing the relationships
between the data types and the corresponding true values of the climate
field. As this Bayesian approach to reconstructing climate fields
is new and different, it is worthwhile to compare it in detail to
the more established regularized expectation?maximization (RegEM)
algorithm, which is based on an empirical estimate of the joint data
covariance matrix and a multivariate regression of the instrumental
time series onto the proxy time series. The differing assumptions
made by BARCAST and RegEM are detailed, and the impacts of these
differences on the analysis are discussed. Key distinctions between
BARCAST and RegEM include their treatment of spatial and temporal
covariance, the prior information that enters into each analysis,
the quantities they seek to impute, the end product of each analysis,
the temporal variance of the reconstructed field, and the treatment
of uncertainty in both the imputed values and functions of these
imputations. Differences between BARCAST and RegEM are illustrated
by applying the two approaches to various surrogate datasets. If
the assumptions inherent to BARCAST are not strongly violated, then
in scenarios comparable to practical applications BARCAST results
in reconstructions of both the field and the spatial mean that are
more skillful than those produced by RegEM, as measured by the coefficient
of efficiency. In addition, the uncertainty intervals produced by
BARCAST are narrower than those estimated using RegEM and contain
the true values with higher probability.},
author = {Tingley, Martin P and Huybers, Peter},
doi = {10.1175/2009JCLI3016.1},
journal = {J. Climate},
keywords = {*file-import-12-04-11},
number = {10},
pages = {2782--2800},
publisher = {American Meteorological Society},
title = {{A Bayesian Algorithm for Reconstructing Climate Anomalies in Space and Time. Part II: Comparison with the Regularized Expectation–Maximization Algorithm}},
url = {http://dx.doi.org/10.1175/2009JCLI3016.1},
volume = {23},
year = {2010}
}
@article{Jolliffe1982,
author = {Jolliffe, Ian T.},
file = {:home/luis/Dropbox/Biblioteca/articulos/Jolliffe - 1982 - A Note on the Use of Principal Components in Regression.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {3},
pages = {300--303},
title = {{A Note on the Use of Principal Components in Regression}},
volume = {31},
year = {1982}
}
@article{Vieira2011,
abstract = {Aims. We present a physically consistent reconstruction of the total solar irradiance for the Holocene. Methods. We extend the SATIRE models to estimate the evolution of the total (and partly spectral) solar irradiance over the Holocene. The basic assumption is that the variations of the solar irradiance are due to the evolution of the dark and bright magnetic features on the solar surface. The evolution of the decadally averaged magnetic flux is computed from decadal values of cosmogenic isotope concentrations recorded in natural archives employing a series of physics-based models connecting the processes from the modulation of the cosmic ray flux in the heliosphere to their record in natural archives. We then compute the total solar irradiance (TSI) as a linear combination of the jth and jth + 1 decadal values of the open magnetic flux. Results. Reconstructions of the TSI over the Holocene, each valid for a di{\_}erent paleomagnetic time series, are presented. Our analysis suggests that major sources of uncertainty in the TSI in this model are the heritage of the uncertainty of the TSI since 1610 reconstructed from sunspot data and the uncertainty of the evolution of the Earth's magnetic dipole moment. The analysis of the distribution functions of the reconstructed irradiance for the last 3000 years indicates that the estimates based on the virtual axial dipole moment are significantly lower at earlier times than the reconstructions based on the virtual dipole moment. Conclusions. We present the first physics-based reconstruction of the total solar irradiance over the Holocene, which will be of interest for studies of climate change over the last 11500 years. The reconstruction indicates that the decadally averaged total solar irradiance ranges over approximately 1.5 W/m2 from grand maxima to grand minima.},
archivePrefix = {arXiv},
arxivId = {1103.4958},
author = {Vieira, Luis Eduardo A. and Solanki, Sami K. and Krivova, Natalie A. and Usoskin, Ilya},
doi = {10.1051/0004-6361/201015843},
eprint = {1103.4958},
isbn = {doi:10.1051/0004-6361/201015843},
issn = {0004-6361},
journal = {Astronomy and Astrophysics},
keywords = {activity,faculae,plages,solar-terrestrial relations,sun,sunspots,surface magnetism,uv radiation},
number = {A6},
pages = {1--20},
title = {{Evolution of the solar irradiance during the Holocene}},
url = {http://arxiv.org/abs/1103.4958},
volume = {531},
year = {2011}
}
@article{mann2005testing,
abstract = {Two widely used statistical approaches to reconstructing past climate
histories from climate ” proxy” data such as tree rings, corals,
and ice cores are investigated using synthetic ” pseudoproxy” data
derived from a simulation of forced climate changes over the past
1200 yr. These experiments suggest that both statistical approaches
should yield reliable reconstructions of the true climate history
within estimated uncertainties, given estimates of the signal and
noise attributes of actual proxy data networks.},
author = {Mann, M E and Rutherford, S and Wahl, E and Ammann, C},
journal = {Journal of Climate},
keywords = {*file-import-12-09-20},
number = {20},
pages = {4097--4107},
title = {{Testing the fidelity of methods used in proxy-based reconstructions of past climate}},
volume = {18},
year = {2005}
}
@book{DeBoor2001,
author = {de Boor, Carl},
publisher = {Springer},
series = {Applied Mathematical Sciences},
title = {{A Practical Guide to Splines}},
year = {2001}
}
@article{Muff2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.3065v2},
author = {Muff, Stefanie and Riebler, Andrea and Held, Leonhard and Rue, H{\aa}vard and Saner, Philippe},
doi = {10.1111/rssc.12069},
eprint = {arXiv:1302.3065v2},
issn = {00359254},
journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
keywords = {approximation,bayesian analysis,berkson error,classical error,integrated nested laplace,measurement error},
number = {2},
pages = {231--252},
title = {{Bayesian analysis of measurement error models using integrated nested Laplace approximations}},
volume = {64},
year = {2015}
}
@article{Ruiz-Cardenas2012,
abstract = {Inference in state-space models usually relies on recursive forms for filtering and smoothing of the state vectors regarding the temporal structure of the observations, an assumption that is, from our view point, unnecessary if the dataset is fixed, that is, completely available before analysis. In this paper, we propose a computational framework to perform approximate full Bayesian inference in linear and generalized dynamic linear models based on the Integrated Nested Laplace Approximation (INLA) approach. The proposed framework directly approximates the posterior marginals of interest disregarding the assumption of recursive updating/estimation of the states and hyperparameters in the case of fixed datasets and, therefore, enable us to do fully Bayesian analysis of complex state-space models more easily and in a short computational time. The proposed framework overcomes some limitations of current tools in the dynamic modeling literature and is vastly illustrated with a series of simulated as well as well known real-life examples from the literature, including realistically complex models with correlated error structures and models with more than one state vector, being mutually dependent on each other. R code is available online for all the examples presented. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Ruiz-C{\'{a}}rdenas, Ramiro and Krainski, Elias T. and Rue, H{\aa}vard},
doi = {10.1016/j.csda.2011.10.024},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximate Bayesian inference,Augmented model,Laplace approximation,Spatio-temporal dynamic models,State-space models},
pages = {1808--1828},
publisher = {Elsevier B.V.},
title = {{Direct fitting of dynamic models using integrated nested Laplace approximations - INLA}},
url = {http://dx.doi.org/10.1016/j.csda.2011.10.024},
volume = {56},
year = {2012}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:home/luis/Dropbox/Biblioteca/articulos/Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society . Series B ( Methodological )},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@article{tingley2013_Ext,
author = {Tingley, M P and Huybers, P},
journal = {Nature},
number = {7444},
pages = {201--205},
publisher = {Nature Publishing Group},
title = {{Recent temperature extremes at high northern latitudes unprecedented in the past 600 years}},
volume = {496},
year = {2013}
}
@article{OlsonHunt2014,
abstract = {When data are sparse and/or predictors multicollinear, current implementation of sparse partial least squares (SPLS) does not give estimates for non-selected predictors nor provide a measure of inference. In response, an approach termed "all-possible" SPLS is proposed, which fits a SPLS model for all tuning parameter values across a set grid. Noted is the percentage of time a given predictor is chosen, as well as the average non-zero parameter estimate. Using a "large" number of multicollinear predictors, simulation confirmed variables not associated with the outcome were least likely to be chosen as sparsity increased across the grid of tuning parameters, while the opposite was true for those strongly associated. Lastly, variables with a weak association were chosen more often than those with no association, but less often than those with a strong relationship to the outcome. Similarly, predictors most strongly related to the outcome had the largest average parameter estimate magnitude, followed by those with a weak relationship, followed by those with no relationship. Across two independent studies regarding the relationship between volumetric MRI measures and a cognitive test score, this method confirmed a priori hypotheses about which brain regions would be selected most often and have the largest average parameter estimates. In conclusion, the percentage of time a predictor is chosen is a useful measure for ordering the strength of the relationship between the independent and dependent variables, serving as a form of inference. The average parameter estimates give further insight regarding the direction and strength of association. As a result, all-possible SPLS gives more information than the dichotomous output of traditional SPLS, making it useful when undertaking data exploration and hypothesis generation for a large number of potential predictors.},
author = {{Olson Hunt}, Megan J. and Weissfeld, Lisa and Boudreau, Robert M. and Aizenstein, Howard and Newman, Anne B. and Simonsick, Eleanor M. and {Van Domelen}, Dane R. and Thomas, Fridtjof and Yaffe, Kristine and Rosano, Caterina},
doi = {10.3389/fninf.2014.00018},
file = {:home/luis/Dropbox/Biblioteca/articulos/Olson Hunt et al. - 2014 - A variant of sparse partial least squares for variable selection and data exploration.pdf:pdf},
issn = {1662-5196},
journal = {Frontiers in Neuroinformatics},
keywords = {high-dimensional,high-dimensional, multicollinearity, over-fitting,,inference,mri,multicollinearity,network,over-fitting,spls,tuning parameters},
number = {March},
pmid = {24624079},
title = {{A variant of sparse partial least squares for variable selection and data exploration}},
url = {http://journal.frontiersin.org/article/10.3389/fninf.2014.00018/abstract},
volume = {8},
year = {2014}
}
@article{Kennedy2011,
abstract = {New estimates of measurement and sampling uncertainties of gridded in situ sea surface temperature anomalies are calculated for 1850 to 2006. The measurement uncertainties account for correlations between errors in observations made by the same ship or buoy due, for example, to miscalibration of the thermometer. Correlations between the errors increase the estimated uncertainties on grid box averages. In grid boxes where there are many observations from only a few ships or drifting buoys, this increase can be large. The correlations also increase uncertainties of regional, hemispheric, and global averages above and beyond the increase arising solely from the inflation of the grid box uncertainties. This is due to correlations in the errors between grid boxes visited by the same ship or drifting buoy. At times when reliable estimates can be made, the uncertainties in global average, Southern Hemisphere, and tropical sea surface temperature anomalies are between 2 and 3 times as large as when calculated assuming the errors are uncorrelated. Uncertainties of Northern Hemisphere averages are approximately double. A new estimate is also made of sampling uncertainties. They are largest in regions of high sea surface temperature variability such as the western boundary currents and along the northern boundary of the Southern Ocean. The sampling uncertainties are generally smaller in the tropics and in the ocean gyres.},
author = {Kennedy, J. J. and Rayner, N. A. and Smith, R. O. and Parker, D. E. and Saunby, M.},
doi = {10.1029/2010JD015220},
file = {:home/luis/Dropbox/Biblioteca/articulos/Kennedy et al. - 2011 - Reassessing biases and other uncertainties in sea surface temperature observations measured in situ since 1850 2.pdf:pdf},
isbn = {2156-2202},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
keywords = {http://dx.doi.org/10.1029/2010JD015220, doi:10.102},
number = {D14},
pages = {D14104},
title = {{Reassessing biases and other uncertainties in sea surface temperature observations measured in situ since 1850: 2. Biases and homogenization}},
url = {http://doi.wiley.com/10.1029/2010JD015220},
volume = {116},
year = {2011}
}
@article{tingley1,
abstract = {Abstract Reconstructing the spatial pattern of a climate field through
time from a dataset of overlapping instrumental and climate proxy
time series is a nontrivial statistical problem. The need to transform
the proxy observations into estimates of the climate field, and the
fact that the observed time series are not uniformly distributed
in space, further complicate the analysis. Current leading approaches
to this problem are based on estimating the full covariance matrix
between the proxy time series and instrumental time series over a
?calibration? interval and then using this covariance matrix in the
context of a linear regression to predict the missing instrumental
values from the proxy observations for years prior to instrumental
coverage. A fundamentally different approach to this problem is formulated
by specifying parametric forms for the spatial covariance and temporal
evolution of the climate field, as well as ?observation equations?
describing the relationship between the data types and the corresponding
true values of the climate field. A hierarchical Bayesian model is
used to assimilate both proxy and instrumental datasets and to estimate
the probability distribution of all model parameters and the climate
field through time on a regular spatial grid. The output from this
approach includes an estimate of the full covariance structure of
the climate field and model parameters as well as diagnostics that
estimate the utility of the different proxy time series. This methodology
is demonstrated using an instrumental surface temperature dataset
after corrupting a number of the time series to mimic proxy observations.
The results are compared to those achieved using the regularized
expectation?maximization algorithm, and in these experiments the
Bayesian algorithm produces reconstructions with greater skill. The
assumptions underlying these two methodologies and the results of
applying each to simple surrogate datasets are explored in greater
detail in Part II. Abstract Reconstructing the spatial pattern of
a climate field through time from a dataset of overlapping instrumental
and climate proxy time series is a nontrivial statistical problem.
The need to transform the proxy observations into estimates of the
climate field, and the fact that the observed time series are not
uniformly distributed in space, further complicate the analysis.
Current leading approaches to this problem are based on estimating
the full covariance matrix between the proxy time series and instrumental
time series over a ?calibration? interval and then using this covariance
matrix in the context of a linear regression to predict the missing
instrumental values from the proxy observations for years prior to
instrumental coverage. A fundamentally different approach to this
problem is formulated by specifying parametric forms for the spatial
covariance and temporal evolution of the climate field, as well as
?observation equations? describing the relationship between the data
types and the corresponding true values of the climate field. A hierarchical
Bayesian model is used to assimilate both proxy and instrumental
datasets and to estimate the probability distribution of all model
parameters and the climate field through time on a regular spatial
grid. The output from this approach includes an estimate of the full
covariance structure of the climate field and model parameters as
well as diagnostics that estimate the utility of the different proxy
time series. This methodology is demonstrated using an instrumental
surface temperature dataset after corrupting a number of the time
series to mimic proxy observations. The results are compared to those
achieved using the regularized expectation?maximization algorithm,
and in these experiments the Bayesian algorithm produces reconstructions
with greater skill. The assumptions underlying these two methodologies
and the results of applying each to simple surrogate datasets are
explored in greater detail in Part II.},
author = {Tingley, Martin P and Huybers, Peter},
doi = {10.1175/2009JCLI3015.1},
journal = {J. Climate},
keywords = {*file-import-12-04-11},
number = {10},
pages = {2759--2781},
publisher = {American Meteorological Society},
title = {{A Bayesian Algorithm for Reconstructing Climate Anomalies in Space and Time. Part I: Development and Applications to Paleoclimate Reconstruction Problems}},
url = {http://dx.doi.org/10.1175/2009JCLI3015.1},
volume = {23},
year = {2010}
}
@article{Schneider2001,
author = {Schneider, T},
doi = {10.1175/1520-0442(2001)014<0853:AOICDE>2.0.CO;2},
file = {:home/luis/Dropbox/Biblioteca/articulos/Schneider - 2001 - Analysis of incomplete climate data estimation of mean values and covariance matrices and imputation of missing value.pdf:pdf},
isbn = {0894-8755},
issn = {08948755},
journal = {Journal of Climate},
pages = {853--871},
title = {{Analysis of incomplete climate data: estimation of mean values and covariance matrices and imputation of missing values}},
volume = {14},
year = {2001}
}
@article{Duan1991,
author = {Duan, Naihua and Li, Ker-Chau},
file = {:home/luis/Dropbox/Biblioteca/articulos/Duan, Li - 1991 - Slicing Regression A Link-Free Regression Method.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {505--530},
title = {{Slicing Regression : A Link-Free Regression Method}},
volume = {19},
year = {1991}
}
@article{Zhong2005,
abstract = {Identification of transcription factor binding motifs ({\{}TFBMs{\}}) is$\backslash$na crucial first step towards the understanding of regulatory circuitries$\backslash$ncontrolling the expression of genes. In this paper, we propose a$\backslash$nnovel procedure called regularized sliced inverse regression ({\{}RSIR{\}})$\backslash$nfor identifying {\{}TFBMs{\}}. {\{}RSIR{\}} follows a recent trend to combine$\backslash$ninformation contained in both gene expression measurements and genes'$\backslash$npromoter sequences. Compared with existing methods, {\{}RSIR{\}} is efficient$\backslash$nin computation, very stable for data with high dimensionality and$\backslash$nhigh collinearity, and improves motif detection sensitivities and$\backslash$nspecificities by avoiding inappropriate model specification.},
author = {Zhong, Wenxuan and Zeng, Peng and Ma, Ping and Liu, Jun S. and Zhu, Yu},
doi = {10.1093/bioinformatics/bti680},
file = {:home/luis/Dropbox/Biblioteca/articulos/Zhong et al. - 2005 - RSIR Regularized sliced inverse regression for motif discovery.pdf:pdf},
isbn = {1367-4803 (Print)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {22},
pages = {4169--4175},
pmid = {16166098},
title = {{RSIR: Regularized sliced inverse regression for motif discovery}},
volume = {21},
year = {2005}
}
@article{Hanhijarvi2013,
abstract = {Existing multi-proxy climate reconstruction methods assume the suitably transformed proxy time series are linearly related to the target climate variable, which is likely a simplifying assumption for many proxy records. Furthermore, with a single exception, these methods face problems with varying temporal resolutions of the proxy data. Here we introduce a new reconstruction method that uses the ordering of all pairs of proxy observations within each record to arrive at a consensus time series that best agrees with all proxy records. The resulting unitless composite time series is subsequently calibrated to the instrumental record to provide an estimate of past climate. By considering only pairwise comparisons, this method, which we call PaiCo, facilitates the inclusion of records with differing temporal resolutions, and relaxes the assumption of linearity to the more general assumption of a monotonically increasing relationship between each proxy series and the target climate variable. We apply PaiCo to a newly assembled collection of high-quality proxy data to reconstruct the mean temperature of the Northernmost Atlantic region, which we call Arctic Atlantic, over the last 2,000 years. The Arctic Atlantic is a dynamically important region known to feature substantial temperature variability over recent millennia, and PaiCo allows for a more thorough investigation of the Arctic Atlantic regional climate as we include a diverse array of terrestrial and marine proxies with annual to multidecadal temporal resolutions. Comparisons of the PaiCo reconstruction to recent reconstructions covering larger areas indicate greater climatic variability in the Arctic Atlantic than for the Arctic as a whole. The Arctic Atlantic reconstruction features temperatures during the Roman Warm Period and Medieval Climate Anomaly that are comparable or even warmer than those of the twentieth century, and coldest temperatures in the middle of the nineteenth century, just prior to the onset of the recent warming trend.},
author = {Hanhij{\"{a}}rvi, Sami and Tingley, Martin P. and Korhola, Atte},
doi = {10.1007/s00382-013-1701-4},
file = {:home/luis/Dropbox/Biblioteca/articulos/Hanhij{\"{a}}rvi, Tingley, Korhola - 2013 - Pairwise comparisons to reconstruct mean temperature in the Arctic Atlantic Region over the last.pdf:pdf},
isbn = {0930-7575$\backslash$r1432-0894},
issn = {09307575},
journal = {Climate Dynamics},
keywords = {Multiproxy reconstruction,Non-linear method,North Atlantic,Pairwise comparisons,Temperature},
number = {7-8},
pages = {2039--2060},
title = {{Pairwise comparisons to reconstruct mean temperature in the Arctic Atlantic Region over the last 2,000 years}},
volume = {41},
year = {2013}
}
@article{Gneiting2007a,
abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the ...},
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
file = {:home/luis/Dropbox/Biblioteca/articulos/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Bregman divergence,Brier score,Coherent,Continuous ranked probability score,Cross-validation,Entropy,Kernel score,Loss function,Minimum contrast estimation,Negative definite function,Prediction interval,Predictive distribution,Quantile forecast,Scoring rule,Skill score,Strictly proper,Utility function},
language = {en},
month = {mar},
number = {477},
pages = {359--378},
publisher = {Taylor {\&} Francis},
title = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000001437{\#}.UfauueGm1As},
volume = {102},
year = {2007}
}
@article{boli1,
abstract = {Understanding the dynamics of climate change in its full richness
requires the knowledge of long temperature time series. Although
long-term, widely distributed temperature observations are not available,
there are other forms of data, known as climate proxies, that can
have a statistical relationship with temperatures and have been used
to infer temperatures in the past before direct measurements. We
propose a Bayesian hierarchical model to reconstruct past temperatures
that integrates information from different sources, such as proxies
with different temporal resolution and forcings acting as the external
drivers of large scale temperature evolution. Additionally, this
method allows us to quantify the uncertainty of the reconstruction
in a rigorous manner. The reconstruction method is assessed, using
a global climate model as the true climate system and with synthetic
proxy data derived from the simulation. The target is to reconstruct
Northern Hemisphere temperature from proxies that mimic the sampling
and errors from tree ring measurements, pollen indices, and borehole
temperatures. The forcing series used as covariates are solar irradiance,
volcanic aerosols, and greenhouse gas concentrations. The Bayesian
model was successful in integrating these different sources of information
in creating a coherent reconstruction. Within the context of this
numerical testbed, a statistical process model that includes the
external forcings can improve the quality of a hemispheric reconstruction
when long time scale proxy information is not available. This article
has supplementary material online.},
author = {Li, Bo and Nychka, Douglas W and Ammann, Caspar M},
journal = {Journal of the American Statistical Association},
keywords = {*file-import-12-04-11,bayes,climate,reconstruction},
number = {491},
pages = {883--895},
title = {{The Value of Multiproxy Reconstruction of Past Climate}},
volume = {105},
year = {2010}
}
@article{werner2012pseudoproxy,
author = {Werner, J P and Luterbacher, J and Smerdon, J E},
journal = {Journal of Climate},
number = {3},
pages = {851--867},
title = {{A Pseudoproxy Evaluation of Bayesian Hierarchical Modelling and Canonical Correlation Analysis for Climate Field Reconstructions over Europe}},
volume = {26},
year = {2013}
}
@article{Kaufman2014,
abstract = {Proxy-based reconstructions of past climate provide insights into externally forced and intrinsic variability over regional to global scales and can be used to place recent trends in a long-term context. Comparisons between these reconstructions and the output of Earth system models provide evaluation opportunities to improve our understanding of climate forcings on time scales that are not adequately represented by the instrumental record. They also provide a heuristic tool to explore mechanisms of climate variability, with implications for future climate projections [Schmidt et al., 2014].},
author = {Kaufman, Darrell},
doi = {10.1002/2014EO400001},
file = {:home/luis/Dropbox/Biblioteca/articulos/Kaufman - 2014 - A Community-Driven Framework for Climate Reconstructions.pdf:pdf},
issn = {2324-9250},
journal = {Eos, Transactions American Geophysical Union},
keywords = {1616 Climate variability,1626 Global climate models,1637 Regional climate change,Climate reconstructions},
number = {40},
pages = {361--362},
title = {{A Community-Driven Framework for Climate Reconstructions}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/2014EO400001/abstract},
volume = {95},
year = {2014}
}
@article{smerdon2010pseudoproxy,
abstract = {Canonical correlation analysis (CCA) is evaluated for paleoclimate
field reconstructions in the context of pseudoproxy experiments assembled
from the millennial integration (850–1999 c.e.) of the National Center
for Atmospheric Research Community Climate System Model, version
1.4. A parsimonious method for selecting the order of the CCA model
is presented. Results suggest that the method is capable of resolving
multiple (3–13) climatic patterns given the estimated proxy observational
network and the amount of observational uncertainty. CCA reconstructions
are compared to those derived from the regularized expectation maximization
method using ridge regression regularization (RegEM-Ridge). CCA and
RegEM-Ridge yield similar skill patterns that are characterized by
high correlation regions collocated with dense pseudoproxy sampling
areas in North America and Europe. Both methods also produce reconstructions
characterized by spatially variable warm biases and variance losses,
particularly at high pseudoproxy noise levels. RegEM-Ridge in particular
is subject to significantly larger variance losses than CCA, even
though the spatial correlation patterns of the two methods are comparable.
Results collectively indicate the importance of evaluating the field
performance of methods that target spatial climate patterns during
the last several millennia and indicate that the results of currently
available climate field reconstructions should be interpreted carefully.},
author = {Smerdon, J E and Kaplan, A and Chang, D and Evans, M N},
journal = {Journal of Climate},
keywords = {*file-import-12-09-21},
number = {18},
pages = {4856--4880},
title = {{A Pseudoproxy Evaluation of the CCA and RegEM Methods for Reconstructing Climate Fields of the Last Millennium}},
volume = {23},
year = {2010}
}
